{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "environment_name = \"SpaceInvaders-v4\"\n",
    "typical_bad_game_frame_count = 700\n",
    "reward_frame_shift = -15\n",
    "\n",
    "# environment_name = \"Pong-v4\"\n",
    "# typical_bad_game_frame_count = 1050\n",
    "# reward_frame_shift = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pytorch model.  for now, accept a 210 x 160 greyscale image and output an array of actions\n",
    "\n",
    "\n",
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_count, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8320, 512)    #64 * 14 * 14 = 12544\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, action_count)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, img_array):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        \n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #fc layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model, max_frames=5000):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name, reward_frame_shift)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    \n",
    "    while not done:\n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        atari_frame.action_array = action_array\n",
    "        current_action = np.argmax(action_array)\n",
    "        #print(\"{} - {}\".format(current_action, output.detach().cpu().numpy()[0]))\n",
    "        done = atari_frame.done_bool\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env\n",
    "\n",
    "def get_batch(atari_env, batch_size):\n",
    "    rand_arr = np.arange(len(atari_env.frame_buffer))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    frame_batch = np.zeros((batch_size, 3, 160, 210))\n",
    "    target_batch = np.zeros(batch_size)\n",
    "    reward_batch = np.zeros(batch_size)\n",
    "    \n",
    "    for ii in range(batch_size):\n",
    "        i = rand_arr[ii]\n",
    "        atari_frame = atari_env.frame_buffer[i]\n",
    "        reward = atari_frame.discounted_reward\n",
    "        reward_batch[ii] = reward\n",
    "        \n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        frame_batch[ii] = img_array\n",
    "        \n",
    "        action_array = atari_frame.action_array\n",
    "        train_action = np.argmax(action_array)\n",
    "        target_batch[ii] = train_action\n",
    "        \n",
    "    return frame_batch, target_batch, reward_batch\n",
    "    \n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(action_count)\n",
    "    train_tally = np.zeros(action_count)\n",
    "    \n",
    "    rand_arr = np.arange(len(discounted_rewards))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    reward_mean_shift = 0\n",
    "    if len(discounted_rewards) > typical_bad_game_frame_count:\n",
    "        sorted_rewards = np.sort(discounted_rewards)\n",
    "        desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        reward_mean_shift = (discounted_rewards_mean - desired_median)/4.0\n",
    "    print(\"shifting rewards by: {:.3f}\".format(reward_mean_shift))\n",
    "    \n",
    "    total_loss = 0\n",
    "    epochs = 25\n",
    "    #for ii, reward_ii in enumerate(discounted_rewards):\n",
    "    for i in range(epochs):\n",
    "    \n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        frame_batch, target_batch, reward_batch = get_batch(atari_env, 50)\n",
    "        \n",
    "        reward_batch = reward_batch + reward_mean_shift #shift rewards for long games\n",
    "        #iterate through rewards, update targets as necessary\n",
    "        for i_batch in range(len(reward_batch)):\n",
    "            if reward_batch[i_batch] < 0:\n",
    "                target_batch[i_batch] = random.randint(0,5) #update to use min action\n",
    "        \n",
    "        img_tensor = torch.from_numpy(frame_batch).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        output_actions = np.sum(output.cpu().detach().numpy(), axis=0)\n",
    "        #print(\"output_actions: {}\".format(output_actions))\n",
    "        #print(\"output: {}\".format(output))\n",
    "\n",
    "        target = torch.from_numpy(target_batch)\n",
    "        target = target.long().cuda()\n",
    "        #print(\"target: {}\".format(target))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        #print(\"loss: {}\".format(loss))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / epochs))\n",
    "#    print(\"model action_tally: {}\".format(action_tally))\n",
    "#    print(\"train_tally:        {}\".format(train_tally))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "action_count = gym.make(environment_name).action_space.n\n",
    "atari_model = AtariModel(action_count)\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 676, score: 245.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.400\n",
      "\n",
      "1) frames played: 770, score: 350.0\n",
      "shifting rewards by: 0.064\n",
      "avg loss: 1.427\n",
      "\n",
      "2) frames played: 749, score: 235.0\n",
      "shifting rewards by: 0.079\n",
      "avg loss: 1.320\n",
      "\n",
      "3) frames played: 767, score: 280.0\n",
      "shifting rewards by: 0.098\n",
      "avg loss: 1.294\n",
      "\n",
      "4) frames played: 377, score: 85.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.332\n",
      "\n",
      "5) frames played: 1710, score: 770.0\n",
      "shifting rewards by: 0.157\n",
      "avg loss: 1.396\n",
      "\n",
      "6) frames played: 690, score: 260.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.391\n",
      "\n",
      "7) frames played: 630, score: 220.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.495\n",
      "\n",
      "8) frames played: 1042, score: 480.0\n",
      "shifting rewards by: 0.123\n",
      "avg loss: 1.408\n",
      "\n",
      "9) frames played: 381, score: 85.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.250\n",
      "\n",
      "10) frames played: 631, score: 185.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.238\n",
      "\n",
      "11) frames played: 916, score: 320.0\n",
      "shifting rewards by: 0.117\n",
      "avg loss: 1.317\n",
      "\n",
      "12) frames played: 1099, score: 345.0\n",
      "shifting rewards by: 0.178\n",
      "avg loss: 1.317\n",
      "\n",
      "13) frames played: 656, score: 180.0\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 1.357\n",
      "\n",
      "14) frames played: 1032, score: 355.0\n",
      "shifting rewards by: 0.168\n",
      "avg loss: 1.247\n",
      "\n",
      "15) frames played: 924, score: 275.0\n",
      "shifting rewards by: 0.155\n",
      "avg loss: 1.333\n",
      "\n",
      "16) frames played: 861, score: 250.0\n",
      "shifting rewards by: 0.131\n",
      "avg loss: 1.269\n",
      "\n",
      "17) frames played: 768, score: 265.0\n",
      "shifting rewards by: 0.109\n",
      "avg loss: 1.214\n",
      "\n",
      "18) frames played: 736, score: 270.0\n",
      "shifting rewards by: 0.055\n",
      "avg loss: 1.339\n"
     ]
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    #play a game\n",
    "    atari_env = play_game(environment_name, atari_model)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "\n",
    "    #train the model\n",
    "    train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discounted_rewards mean: 5.4898445367778163e-17\n",
      "Shifting rewards by 0.48340174895687293\n",
      "new discounted_rewards mean: 0.48340174895687305\n"
     ]
    }
   ],
   "source": [
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "#display frame\n",
    "frame_num=600\n",
    "discounted_rewards = atari_env.get_discounted_rewards()\n",
    "discounted_rewards_mean_shifted = atari_env.get_discounted_rewards()\n",
    "\n",
    "print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "if len(discounted_rewards_mean_shifted) > typical_bad_game_frame_count:\n",
    "    sorted_rewards = np.sort(discounted_rewards_mean_shifted)\n",
    "    desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "    discounted_rewards_mean = np.mean(discounted_rewards_mean_shifted)\n",
    "    reward_shift = (discounted_rewards_mean - desired_median)/2.0\n",
    "    print(\"Shifting rewards by {}\".format(reward_shift))\n",
    "    discounted_rewards_mean_shifted = discounted_rewards_mean_shifted + reward_shift\n",
    "    print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards_mean_shifted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-d4a74d8a4b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0matari_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n\u001b[1;32m      5\u001b[0m     frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    }
   ],
   "source": [
    "frame_num += 4\n",
    "atari_frame = atari_env.frame_buffer[frame_num]\n",
    "\n",
    "print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n",
    "    frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
    "atari_frame.show_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
