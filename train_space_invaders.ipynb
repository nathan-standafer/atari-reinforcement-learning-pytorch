{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "environment_name = \"SpaceInvaders-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pytorch model.  for now, accept a 210 x 160 greyscale image and output an array of actions\n",
    "\n",
    "\n",
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8320, 512)    #64 * 14 * 14 = 12544\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 6)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, img_array):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        \n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #fc layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        atari_frame.action_array = action_array\n",
    "        current_action = np.argmax(action_array)\n",
    "        #print(\"{} - {}\".format(current_action, output.detach().cpu().numpy()[0]))\n",
    "        done = atari_frame.done_bool\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env\n",
    "\n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(6)\n",
    "    train_tally = np.zeros(6)\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, reward in enumerate(discounted_rewards):\n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #get frame from the frame buffer and run it through the model\n",
    "        atari_frame = atari_env.frame_buffer[i]\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        #print(\"train output: {}\".format(output))\n",
    "        \n",
    "        #if the reward was positive, keep the same.  if not, choose lowest option\n",
    "        action_array_from_model_in_training = output.detach().cpu().numpy()[0]\n",
    "        action_array = atari_frame.action_array\n",
    "        train_action = np.argmax(action_array)\n",
    "        action_tally[train_action] += 1\n",
    "        \n",
    "        if reward < 0:\n",
    "            train_action = np.argmin(action_array)\n",
    "            #train_action = np.argsort(action_array)[3] #fourth highest\n",
    "            \n",
    "        if np.argmax(train_tally) == train_action and np.sum(train_tally) != 0:\n",
    "            #keep things even to not introduce bias that will get it stuck on one action\n",
    "            continue\n",
    "\n",
    "        train_tally[train_action] += 1\n",
    "        \n",
    "        target = torch.empty(1, dtype=torch.int64)\n",
    "        target[0] = int(train_action)\n",
    "        target = target.cuda()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / len(train_tally)))\n",
    "    print(\"model action_tally: {}\".format(action_tally))\n",
    "    print(\"train_tally:        {}\".format(train_tally))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "atari_model = AtariModel()\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 665, score: 25.0\n",
      "avg loss: 13.238\n",
      "model action_tally: [  0.  10.  28. 616.   0.  11.]\n",
      "train_tally:        [0. 4. 1. 5. 6. 5.]\n",
      "\n",
      "1) frames played: 643, score: 35.0\n",
      "avg loss: 11.388\n",
      "model action_tally: [  0.   0.   0. 439.   0. 204.]\n",
      "train_tally:        [11.  0.  0. 11.  0. 11.]\n",
      "\n",
      "2) frames played: 920, score: 0.0\n",
      "no points granted. Setting all discounted rewards to -1\n",
      "avg loss: 15.624\n",
      "model action_tally: [920.   0.   0.   0.   0.   0.]\n",
      "train_tally:        [ 0. 13. 16.  0. 16.  0.]\n",
      "\n",
      "3) frames played: 506, score: 105.0\n",
      "avg loss: 4.467\n",
      "model action_tally: [  0. 336. 160.   0.  10.   0.]\n",
      "train_tally:        [4. 4. 4. 0. 3. 0.]\n",
      "\n",
      "4) frames played: 1076, score: 30.0\n",
      "avg loss: 29.347\n",
      "model action_tally: [   0.    8. 1060.    0.    8.    0.]\n",
      "train_tally:        [21.  6.  7. 37.  5. 37.]\n",
      "\n",
      "5) frames played: 690, score: 105.0\n",
      "avg loss: 4.796\n",
      "model action_tally: [  0.   0.   0. 398.   0. 292.]\n",
      "train_tally:        [0. 0. 5. 3. 0. 5.]\n",
      "\n",
      "6) frames played: 686, score: 105.0\n",
      "avg loss: 9.932\n",
      "model action_tally: [  0.   0.   0. 452.   0. 234.]\n",
      "train_tally:        [ 0.  0.  0. 11. 12. 12.]\n",
      "\n",
      "7) frames played: 649, score: 75.0\n",
      "avg loss: 6.621\n",
      "model action_tally: [  0.   0.   0. 367.   0. 282.]\n",
      "train_tally:        [1. 8. 0. 7. 0. 8.]\n",
      "\n",
      "8) frames played: 806, score: 155.0\n",
      "avg loss: 31.340\n",
      "model action_tally: [  0.   0.   0. 123.  11. 672.]\n",
      "train_tally:        [25. 17. 22. 24.  0. 25.]\n",
      "\n",
      "9) frames played: 922, score: 0.0\n",
      "no points granted. Setting all discounted rewards to -1\n",
      "avg loss: 12.428\n",
      "model action_tally: [  0.   0.   0. 922.   0.   0.]\n",
      "train_tally:        [ 0. 15. 15.  0. 14.  0.]\n",
      "\n",
      "10) frames played: 923, score: 15.0\n",
      "avg loss: 2.370\n",
      "model action_tally: [  0. 129. 794.   0.   0.   0.]\n",
      "train_tally:        [0. 2. 2. 3. 0. 0.]\n",
      "\n",
      "11) frames played: 715, score: 75.0\n",
      "avg loss: 16.708\n",
      "model action_tally: [  0. 136. 579.   0.   0.   0.]\n",
      "train_tally:        [18.  8.  9.  0.  0. 19.]\n",
      "\n",
      "12) frames played: 1088, score: 415.0\n",
      "avg loss: 40.933\n",
      "model action_tally: [191.  50.  36.  76.   0. 735.]\n",
      "train_tally:        [26. 22.  8. 24. 28. 28.]\n",
      "\n",
      "13) frames played: 837, score: 180.0\n",
      "avg loss: 13.323\n",
      "model action_tally: [179.  15.   0. 147.   0. 496.]\n",
      "train_tally:        [ 2.  7. 14. 13.  0. 14.]\n",
      "\n",
      "14) frames played: 874, score: 180.0\n",
      "avg loss: 17.949\n",
      "model action_tally: [ 12.  10.   0.  71.   0. 781.]\n",
      "train_tally:        [14. 10.  0. 13. 14. 14.]\n",
      "\n",
      "15) frames played: 792, score: 35.0\n",
      "avg loss: 9.260\n",
      "model action_tally: [423.   0.   0. 254.  73.  42.]\n",
      "train_tally:        [6. 0. 9. 9. 6. 9.]\n",
      "\n",
      "16) frames played: 716, score: 125.0\n",
      "avg loss: 20.427\n",
      "model action_tally: [106.   0.  31. 116. 141. 322.]\n",
      "train_tally:        [ 7. 16.  3. 16. 13. 15.]\n",
      "\n",
      "17) frames played: 690, score: 105.0\n",
      "avg loss: 6.743\n",
      "model action_tally: [  0.   0.   0. 398.   0. 292.]\n",
      "train_tally:        [6. 0. 7. 5. 0. 7.]\n",
      "\n",
      "18) frames played: 1393, score: 210.0\n",
      "avg loss: 57.272\n",
      "model action_tally: [116.   9.   0. 560.  14. 694.]\n",
      "train_tally:        [16. 35. 35. 34. 33. 35.]\n",
      "\n",
      "19) frames played: 1384, score: 315.0\n",
      "avg loss: 37.419\n",
      "model action_tally: [   0.   23.   67.   31.    0. 1263.]\n",
      "train_tally:        [30.  1. 11. 27. 24. 30.]\n",
      "\n",
      "20) frames played: 829, score: 180.0\n",
      "avg loss: 50.100\n",
      "model action_tally: [116.   0.  17. 464.  17. 215.]\n",
      "train_tally:        [36. 39. 28. 39.  6. 39.]\n",
      "\n",
      "21) frames played: 790, score: 55.0\n",
      "avg loss: 25.877\n",
      "model action_tally: [ 34.   0. 350. 317.   0.  89.]\n",
      "train_tally:        [16.  0. 18. 22. 23. 23.]\n",
      "\n",
      "22) frames played: 1652, score: 435.0\n",
      "avg loss: 52.087\n",
      "model action_tally: [1138.    0.  127.  101.  224.   62.]\n",
      "train_tally:        [38. 39. 37. 32. 39. 21.]\n",
      "\n",
      "23) frames played: 734, score: 150.0\n",
      "avg loss: 29.081\n",
      "model action_tally: [364. 137.  12.  31. 190.   0.]\n",
      "train_tally:        [18. 18. 10. 16. 13. 19.]\n",
      "\n",
      "24) frames played: 530, score: 210.0\n",
      "avg loss: 18.426\n",
      "model action_tally: [ 38.   0.   0.   0. 492.   0.]\n",
      "train_tally:        [ 6. 13. 15. 13. 15.  0.]\n",
      "\n",
      "25) frames played: 664, score: 270.0\n",
      "avg loss: 4.685\n",
      "model action_tally: [  0.  66.   0.  33. 565.   0.]\n",
      "train_tally:        [0. 0. 0. 0. 8. 9.]\n",
      "\n",
      "26) frames played: 715, score: 270.0\n",
      "avg loss: 10.694\n",
      "model action_tally: [  0.   0.   0.   0. 715.   0.]\n",
      "train_tally:        [ 0.  7.  9. 10. 10.  0.]\n",
      "\n",
      "27) frames played: 574, score: 185.0\n",
      "avg loss: 15.912\n",
      "model action_tally: [  0.  52.   0.   0. 522.   0.]\n",
      "train_tally:        [13.  5. 13.  0. 10. 12.]\n",
      "\n",
      "28) frames played: 703, score: 135.0\n",
      "avg loss: 11.571\n",
      "model action_tally: [225.  15. 189.  32. 212.  30.]\n",
      "train_tally:        [10.  0.  2.  9.  8. 10.]\n",
      "\n",
      "29) frames played: 860, score: 170.0\n",
      "avg loss: 33.664\n",
      "model action_tally: [136.   0.  98.  67. 507.  52.]\n",
      "train_tally:        [20. 23. 14. 22. 23. 22.]\n",
      "\n",
      "30) frames played: 1041, score: 370.0\n",
      "avg loss: 44.379\n",
      "model action_tally: [108.  69.   8.  20. 816.  20.]\n",
      "train_tally:        [18. 16. 31. 32. 31. 19.]\n",
      "\n",
      "31) frames played: 710, score: 270.0\n",
      "avg loss: 4.924\n",
      "model action_tally: [  0.   0.   0.   0. 710.   0.]\n",
      "train_tally:        [8. 0. 0. 0. 8. 0.]\n",
      "\n",
      "32) frames played: 537, score: 195.0\n",
      "avg loss: 25.216\n",
      "model action_tally: [193.   0.   0.   0. 344.   0.]\n",
      "train_tally:        [23. 21.  0.  0. 26. 26.]\n",
      "\n",
      "33) frames played: 826, score: 95.0\n",
      "avg loss: 17.538\n",
      "model action_tally: [240. 121.   0.  50. 405.  10.]\n",
      "train_tally:        [12. 12.  7. 13. 13.  8.]\n",
      "\n",
      "34) frames played: 941, score: 235.0\n",
      "avg loss: 39.917\n",
      "model action_tally: [151. 199.   0.  12. 550.  29.]\n",
      "train_tally:        [27. 24. 28. 19. 27. 21.]\n",
      "\n",
      "35) frames played: 621, score: 250.0\n",
      "avg loss: 13.903\n",
      "model action_tally: [ 53.   0.   0.   0. 568.   0.]\n",
      "train_tally:        [12.  0.  7. 12. 12.  5.]\n",
      "\n",
      "36) frames played: 966, score: 295.0\n",
      "avg loss: 30.840\n",
      "model action_tally: [114. 199.   0. 180. 436.  37.]\n",
      "train_tally:        [23. 23. 22. 23. 22. 21.]\n",
      "\n",
      "37) frames played: 1264, score: 175.0\n",
      "avg loss: 35.112\n",
      "model action_tally: [115.  87. 582.  62. 385.  33.]\n",
      "train_tally:        [19. 18. 25. 30. 26. 31.]\n",
      "\n",
      "38) frames played: 1324, score: 355.0\n",
      "avg loss: 60.172\n",
      "model action_tally: [ 72.  11. 638. 161. 237. 205.]\n",
      "train_tally:        [16. 43. 35. 37. 43. 37.]\n",
      "\n",
      "39) frames played: 789, score: 145.0\n",
      "avg loss: 25.051\n",
      "model action_tally: [ 68.  52.  74. 143. 351. 101.]\n",
      "train_tally:        [17. 14. 17. 13. 17. 15.]\n",
      "\n",
      "40) frames played: 662, score: 150.0\n",
      "avg loss: 32.977\n",
      "model action_tally: [ 33.  32.  68.  23. 358. 148.]\n",
      "train_tally:        [21. 23.  9. 24. 23. 22.]\n",
      "\n",
      "41) frames played: 1175, score: 315.0\n",
      "avg loss: 70.588\n",
      "model action_tally: [162.  98.  71.  18. 757.  69.]\n",
      "train_tally:        [47. 27. 34. 46. 47. 48.]\n",
      "\n",
      "42) frames played: 861, score: 340.0\n",
      "avg loss: 16.371\n",
      "model action_tally: [138.   5.   0.  85. 622.  11.]\n",
      "train_tally:        [15. 17.  0.  9. 17.  9.]\n",
      "\n",
      "43) frames played: 1276, score: 250.0\n",
      "avg loss: 40.451\n",
      "model action_tally: [325.   0.   0. 110. 809.  32.]\n",
      "train_tally:        [27. 28. 27. 28. 28. 12.]\n",
      "\n",
      "44) frames played: 366, score: 5.0\n",
      "avg loss: 4.382\n",
      "model action_tally: [114.   0.   0. 172.  80.   0.]\n",
      "train_tally:        [0. 0. 0. 3. 5. 6.]\n",
      "\n",
      "45) frames played: 479, score: 55.0\n",
      "avg loss: 10.934\n",
      "model action_tally: [ 77.   0.   0. 223. 179.   0.]\n",
      "train_tally:        [ 0.  5.  1. 11. 11. 12.]\n",
      "\n",
      "46) frames played: 821, score: 155.0\n",
      "avg loss: 19.918\n",
      "model action_tally: [  8.   0.   0. 571.  21. 221.]\n",
      "train_tally:        [18.  0. 13. 18. 12. 18.]\n",
      "\n",
      "47) frames played: 1146, score: 225.0\n",
      "avg loss: 38.899\n",
      "model action_tally: [172.  32.  26. 677.  85. 154.]\n",
      "train_tally:        [14. 29. 25. 28. 29. 27.]\n",
      "\n",
      "48) frames played: 947, score: 230.0\n",
      "avg loss: 68.720\n",
      "model action_tally: [ 57.  88.  44. 230. 457.  71.]\n",
      "train_tally:        [42. 42. 41. 35. 35. 35.]\n",
      "\n",
      "49) frames played: 503, score: 0.0\n",
      "no points granted. Setting all discounted rewards to -1\n",
      "avg loss: 0.776\n",
      "model action_tally: [ 45.   0. 458.   0.   0.   0.]\n",
      "train_tally:        [0. 0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    #play a game\n",
    "    atari_env = play_game(environment_name, atari_model)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "\n",
    "    #train the model\n",
    "    for ii in range(1):\n",
    "        train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
