{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "environment_name = \"SpaceInvaders-v4\"\n",
    "#environment_name = \"Pong-v4\"\n",
    "typical_bad_game_frame_count = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pytorch model.  for now, accept a 210 x 160 greyscale image and output an array of actions\n",
    "\n",
    "\n",
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_count, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8320, 512)    #64 * 14 * 14 = 12544\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, action_count)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, img_array):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        \n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #fc layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model, max_frames=5000):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    \n",
    "    while not done:\n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        atari_frame.action_array = action_array\n",
    "        current_action = np.argmax(action_array)\n",
    "        #print(\"{} - {}\".format(current_action, output.detach().cpu().numpy()[0]))\n",
    "        done = atari_frame.done_bool\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env\n",
    "\n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(action_count)\n",
    "    train_tally = np.zeros(action_count)\n",
    "    \n",
    "    rand_arr = np.arange(len(discounted_rewards))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    \n",
    "    print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "    if len(discounted_rewards) > typical_bad_game_frame_count:\n",
    "        sorted_rewards = np.sort(discounted_rewards)\n",
    "        desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        reward_shift = discounted_rewards_mean - desired_median\n",
    "        print(\"Shifting rewards by {}\".format(reward_shift))\n",
    "        discounted_rewards = discounted_rewards + reward_shift\n",
    "        print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "        \n",
    "    \n",
    "    total_loss = 0\n",
    "    for ii, reward_ii in enumerate(discounted_rewards):\n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "        i = rand_arr[ii]\n",
    "        \n",
    "        #get frame from the frame buffer and run it through the model\n",
    "        atari_frame = atari_env.frame_buffer[i]\n",
    "        reward = atari_frame.discounted_reward\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        #print(\"train output: {}\".format(output))\n",
    "        \n",
    "        #if the reward was positive, keep the same.  if not, choose lowest option\n",
    "        action_array_from_model_in_training = output.detach().cpu().numpy()[0]\n",
    "        action_array = atari_frame.action_array\n",
    "        train_action = np.argmax(action_array)\n",
    "        action_tally[train_action] += 1\n",
    "        \n",
    "        if reward < 0:\n",
    "            train_action = np.argmin(action_array)\n",
    "            #train_action = np.argsort(action_array)[1] #second highest\n",
    "            #train_action = random.randint(0,action_count-1)\n",
    "\n",
    "        if reward > -0.5 and reward < 0.5:\n",
    "            continue\n",
    "            \n",
    "        if np.argmax(train_tally) == train_action and np.sum(train_tally) != 0:\n",
    "            #keep things even to not introduce bias that will get it stuck on one action\n",
    "            continue\n",
    "\n",
    "        train_tally[train_action] += 1\n",
    "        \n",
    "        target = torch.empty(1, dtype=torch.int64)\n",
    "        target[0] = int(train_action)\n",
    "        target = target.cuda()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / np.sum(train_tally)))\n",
    "    print(\"model action_tally: {}\".format(action_tally))\n",
    "    print(\"train_tally:        {}\".format(train_tally))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "action_count = gym.make(environment_name).action_space.n\n",
    "atari_model = AtariModel(action_count)\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 615, score: 110.0\n",
      "discounted_rewards mean: 1.38642485026361e-16\n",
      "Shifting rewards by 0.5671103310851955\n",
      "new discounted_rewards mean: 0.5671103310851956\n",
      "avg loss: 1.033\n",
      "model action_tally: [  0. 404.   0.   0. 170.  41.]\n",
      "train_tally:        [88. 86. 88.  0.  1. 24.]\n",
      "discounted_rewards mean: 1.38642485026361e-16\n",
      "Shifting rewards by 0.5671103310851955\n",
      "new discounted_rewards mean: 0.5671103310851956\n",
      "avg loss: 0.681\n",
      "model action_tally: [  0. 404.   0.   0. 170.  41.]\n",
      "train_tally:        [89. 88. 89.  0.  1. 24.]\n",
      "discounted_rewards mean: 1.38642485026361e-16\n",
      "Shifting rewards by 0.5671103310851955\n",
      "new discounted_rewards mean: 0.5671103310851956\n",
      "avg loss: 0.531\n",
      "model action_tally: [  0. 404.   0.   0. 170.  41.]\n",
      "train_tally:        [89. 89. 89.  0.  1. 24.]\n",
      "\n",
      "1) frames played: 673, score: 105.0\n",
      "discounted_rewards mean: -1.2669409850105798e-16\n",
      "Shifting rewards by 0.4968948268678638\n",
      "new discounted_rewards mean: 0.49689482686786385\n",
      "avg loss: 0.467\n",
      "model action_tally: [273.  67. 293.   0.   0.  40.]\n",
      "train_tally:        [14. 27.  0.  0.  0. 28.]\n",
      "discounted_rewards mean: -1.2669409850105798e-16\n",
      "Shifting rewards by 0.4968948268678638\n",
      "new discounted_rewards mean: 0.49689482686786385\n",
      "avg loss: 0.362\n",
      "model action_tally: [273.  67. 293.   0.   0.  40.]\n",
      "train_tally:        [14. 25.  0.  0.  0. 25.]\n",
      "discounted_rewards mean: -1.2669409850105798e-16\n",
      "Shifting rewards by 0.4968948268678638\n",
      "new discounted_rewards mean: 0.49689482686786385\n",
      "avg loss: 0.360\n",
      "model action_tally: [273.  67. 293.   0.   0.  40.]\n",
      "train_tally:        [14. 25.  0.  0.  0. 26.]\n",
      "\n",
      "2) frames played: 1263, score: 105.0\n",
      "discounted_rewards mean: 1.1251666441173398e-16\n",
      "Shifting rewards by 0.34541092184392924\n",
      "new discounted_rewards mean: 0.3454109218439293\n",
      "avg loss: 0.234\n",
      "model action_tally: [617.  64. 528.   0.   0.  54.]\n",
      "train_tally:        [19. 27.  0.  0.  0. 27.]\n",
      "discounted_rewards mean: 1.1251666441173398e-16\n",
      "Shifting rewards by 0.34541092184392924\n",
      "new discounted_rewards mean: 0.3454109218439293\n",
      "avg loss: 0.181\n",
      "model action_tally: [617.  64. 528.   0.   0.  54.]\n",
      "train_tally:        [18. 29.  0.  0.  0. 29.]\n",
      "discounted_rewards mean: 1.1251666441173398e-16\n",
      "Shifting rewards by 0.34541092184392924\n",
      "new discounted_rewards mean: 0.3454109218439293\n",
      "avg loss: 0.127\n",
      "model action_tally: [617.  64. 528.   0.   0.  54.]\n",
      "train_tally:        [19. 34.  0.  0.  0. 34.]\n",
      "\n",
      "3) frames played: 1261, score: 105.0\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.351921140943697\n",
      "new discounted_rewards mean: 0.35192114094369703\n",
      "avg loss: 0.095\n",
      "model action_tally: [595.  90. 523.   0.   0.  53.]\n",
      "train_tally:        [22. 30.  0.  0.  0. 31.]\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.351921140943697\n",
      "new discounted_rewards mean: 0.35192114094369703\n",
      "avg loss: 0.054\n",
      "model action_tally: [595.  90. 523.   0.   0.  53.]\n",
      "train_tally:        [23. 33.  0.  0.  0. 33.]\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.351921140943697\n",
      "new discounted_rewards mean: 0.35192114094369703\n",
      "avg loss: 0.065\n",
      "model action_tally: [595.  90. 523.   0.   0.  53.]\n",
      "train_tally:        [23. 27.  0.  0.  0. 28.]\n",
      "\n",
      "4) frames played: 1245, score: 105.0\n",
      "discounted_rewards mean: -1.1414341136708436e-16\n",
      "Shifting rewards by 0.3495131871053083\n",
      "new discounted_rewards mean: 0.3495131871053084\n",
      "avg loss: 0.058\n",
      "model action_tally: [631.  62. 500.   0.   0.  52.]\n",
      "train_tally:        [25. 32.  0.  0.  0. 32.]\n",
      "discounted_rewards mean: -1.1414341136708436e-16\n",
      "Shifting rewards by 0.3495131871053083\n",
      "new discounted_rewards mean: 0.3495131871053084\n",
      "avg loss: 0.036\n",
      "model action_tally: [631.  62. 500.   0.   0.  52.]\n",
      "train_tally:        [25. 28.  0.  0.  0. 29.]\n",
      "discounted_rewards mean: -1.1414341136708436e-16\n",
      "Shifting rewards by 0.3495131871053083\n",
      "new discounted_rewards mean: 0.3495131871053084\n",
      "avg loss: 0.019\n",
      "model action_tally: [631.  62. 500.   0.   0.  52.]\n",
      "train_tally:        [23. 30.  0.  0.  0. 30.]\n",
      "\n",
      "5) frames played: 523, score: 105.0\n",
      "discounted_rewards mean: -5.4343612677636724e-17\n",
      "Shifting rewards by 0.585255139554077\n",
      "new discounted_rewards mean: 0.585255139554077\n",
      "avg loss: 0.651\n",
      "model action_tally: [281.  71. 117.   0.   0.  54.]\n",
      "train_tally:        [11. 60.  0. 66. 66. 45.]\n",
      "discounted_rewards mean: -5.4343612677636724e-17\n",
      "Shifting rewards by 0.585255139554077\n",
      "new discounted_rewards mean: 0.585255139554077\n",
      "avg loss: 0.161\n",
      "model action_tally: [281.  71. 117.   0.   0.  54.]\n",
      "train_tally:        [11. 61.  0. 72. 72. 45.]\n",
      "discounted_rewards mean: -5.4343612677636724e-17\n",
      "Shifting rewards by 0.585255139554077\n",
      "new discounted_rewards mean: 0.585255139554077\n",
      "avg loss: 0.082\n",
      "model action_tally: [281.  71. 117.   0.   0.  54.]\n",
      "train_tally:        [11. 60.  0. 70. 71. 45.]\n",
      "\n",
      "6) frames played: 664, score: 215.0\n",
      "discounted_rewards mean: 4.280377926265664e-17\n",
      "Shifting rewards by 0.860093237128951\n",
      "new discounted_rewards mean: 0.8600932371289511\n",
      "avg loss: 0.795\n",
      "model action_tally: [ 12.  64.   0. 233. 299.  56.]\n",
      "train_tally:        [95. 97. 10.  0. 96. 97.]\n",
      "discounted_rewards mean: 4.280377926265664e-17\n",
      "Shifting rewards by 0.860093237128951\n",
      "new discounted_rewards mean: 0.8600932371289511\n",
      "avg loss: 0.310\n",
      "model action_tally: [ 12.  64.   0. 233. 299.  56.]\n",
      "train_tally:        [92. 92. 10.  0. 92. 92.]\n",
      "discounted_rewards mean: 4.280377926265664e-17\n",
      "Shifting rewards by 0.860093237128951\n",
      "new discounted_rewards mean: 0.8600932371289511\n",
      "avg loss: 0.206\n",
      "model action_tally: [ 12.  64.   0. 233. 299.  56.]\n",
      "train_tally:        [94. 94. 10.  0. 94. 93.]\n",
      "\n",
      "7) frames played: 693, score: 215.0\n",
      "discounted_rewards mean: -6.151885158096106e-17\n",
      "Shifting rewards by 0.8191940793271796\n",
      "new discounted_rewards mean: 0.8191940793271795\n",
      "avg loss: 0.806\n",
      "model action_tally: [110. 239.  11.   0. 259.  74.]\n",
      "train_tally:        [20. 74. 84. 84. 82. 33.]\n",
      "discounted_rewards mean: -6.151885158096106e-17\n",
      "Shifting rewards by 0.8191940793271796\n",
      "new discounted_rewards mean: 0.8191940793271795\n",
      "avg loss: 0.366\n",
      "model action_tally: [110. 239.  11.   0. 259.  74.]\n",
      "train_tally:        [20. 75. 86. 86. 87. 33.]\n",
      "discounted_rewards mean: -6.151885158096106e-17\n",
      "Shifting rewards by 0.8191940793271796\n",
      "new discounted_rewards mean: 0.8191940793271795\n",
      "avg loss: 0.220\n",
      "model action_tally: [110. 239.  11.   0. 259.  74.]\n",
      "train_tally:        [20. 74. 89. 86. 90. 33.]\n",
      "\n",
      "8) frames played: 1449, score: 215.0\n",
      "discounted_rewards mean: 7.845882520470396e-17\n",
      "Shifting rewards by 0.5075531617440014\n",
      "new discounted_rewards mean: 0.5075531617440014\n",
      "avg loss: 0.650\n",
      "model action_tally: [ 10.  70. 770. 179. 359.  61.]\n",
      "train_tally:        [ 36.  58.  11. 217. 217. 218.]\n",
      "discounted_rewards mean: 7.845882520470396e-17\n",
      "Shifting rewards by 0.5075531617440014\n",
      "new discounted_rewards mean: 0.5075531617440014\n",
      "avg loss: 0.237\n",
      "model action_tally: [ 10.  70. 770. 179. 359.  61.]\n",
      "train_tally:        [ 36.  58.  11. 209. 210. 210.]\n",
      "discounted_rewards mean: 7.845882520470396e-17\n",
      "Shifting rewards by 0.5075531617440014\n",
      "new discounted_rewards mean: 0.5075531617440014\n",
      "avg loss: 0.161\n",
      "model action_tally: [ 10.  70. 770. 179. 359.  61.]\n",
      "train_tally:        [ 36.  58.  11. 216. 215. 217.]\n",
      "\n",
      "9) frames played: 587, score: 105.0\n",
      "discounted_rewards mean: -4.841858506031347e-17\n",
      "Shifting rewards by 0.5454710310184483\n",
      "new discounted_rewards mean: 0.5454710310184483\n",
      "avg loss: 0.686\n",
      "model action_tally: [138.  52.   0.  59. 227. 111.]\n",
      "train_tally:        [87. 88. 19. 23. 10. 53.]\n",
      "discounted_rewards mean: -4.841858506031347e-17\n",
      "Shifting rewards by 0.5454710310184483\n",
      "new discounted_rewards mean: 0.5454710310184483\n",
      "avg loss: 0.135\n",
      "model action_tally: [138.  52.   0.  59. 227. 111.]\n",
      "train_tally:        [80. 81. 20. 23. 10. 53.]\n",
      "discounted_rewards mean: -4.841858506031347e-17\n",
      "Shifting rewards by 0.5454710310184483\n",
      "new discounted_rewards mean: 0.5454710310184483\n",
      "avg loss: 0.086\n",
      "model action_tally: [138.  52.   0.  59. 227. 111.]\n",
      "train_tally:        [89. 90. 20. 23. 10. 53.]\n",
      "\n",
      "10) frames played: 1260, score: 120.0\n",
      "discounted_rewards mean: -9.022764898540955e-17\n",
      "Shifting rewards by 0.45897697429586315\n",
      "new discounted_rewards mean: 0.4589769742958631\n",
      "avg loss: 0.050\n",
      "model action_tally: [562. 263.  30.  28.  86. 291.]\n",
      "train_tally:        [ 8. 38.  0.  0.  7. 38.]\n",
      "discounted_rewards mean: -9.022764898540955e-17\n",
      "Shifting rewards by 0.45897697429586315\n",
      "new discounted_rewards mean: 0.4589769742958631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.019\n",
      "model action_tally: [562. 263.  30.  28.  86. 291.]\n",
      "train_tally:        [ 8. 37.  0.  0.  8. 38.]\n",
      "discounted_rewards mean: -9.022764898540955e-17\n",
      "Shifting rewards by 0.45897697429586315\n",
      "new discounted_rewards mean: 0.4589769742958631\n",
      "avg loss: 0.008\n",
      "model action_tally: [562. 263.  30.  28.  86. 291.]\n",
      "train_tally:        [ 8. 34.  0.  0.  8. 34.]\n",
      "\n",
      "11) frames played: 919, score: 110.0\n",
      "discounted_rewards mean: 6.185355697585203e-17\n",
      "Shifting rewards by 0.513159329645849\n",
      "new discounted_rewards mean: 0.513159329645849\n",
      "avg loss: 1.001\n",
      "model action_tally: [348. 245.  20.  21.  13. 272.]\n",
      "train_tally:        [59. 91. 98. 23. 62. 99.]\n",
      "discounted_rewards mean: 6.185355697585203e-17\n",
      "Shifting rewards by 0.513159329645849\n",
      "new discounted_rewards mean: 0.513159329645849\n",
      "avg loss: 0.260\n",
      "model action_tally: [348. 245.  20.  21.  13. 272.]\n",
      "train_tally:        [60. 91. 97. 23. 62. 97.]\n",
      "discounted_rewards mean: 6.185355697585203e-17\n",
      "Shifting rewards by 0.513159329645849\n",
      "new discounted_rewards mean: 0.513159329645849\n",
      "avg loss: 0.135\n",
      "model action_tally: [348. 245.  20.  21.  13. 272.]\n",
      "train_tally:        [60. 85. 98. 23. 62. 99.]\n",
      "\n",
      "12) frames played: 652, score: 115.0\n",
      "discounted_rewards mean: -6.538736832148162e-17\n",
      "Shifting rewards by 0.7892778525510558\n",
      "new discounted_rewards mean: 0.7892778525510558\n",
      "avg loss: 1.258\n",
      "model action_tally: [ 21. 251.  81.  61.  65. 173.]\n",
      "train_tally:        [67. 68.  9. 60. 46. 68.]\n",
      "discounted_rewards mean: -6.538736832148162e-17\n",
      "Shifting rewards by 0.7892778525510558\n",
      "new discounted_rewards mean: 0.7892778525510558\n",
      "avg loss: 0.562\n",
      "model action_tally: [ 21. 251.  81.  61.  65. 173.]\n",
      "train_tally:        [65. 66.  9. 59. 47. 67.]\n",
      "discounted_rewards mean: -6.538736832148162e-17\n",
      "Shifting rewards by 0.7892778525510558\n",
      "new discounted_rewards mean: 0.7892778525510558\n",
      "avg loss: 0.365\n",
      "model action_tally: [ 21. 251.  81.  61.  65. 173.]\n",
      "train_tally:        [68. 69.  9. 59. 47. 69.]\n",
      "\n",
      "13) frames played: 811, score: 155.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6245343618644736\n",
      "new discounted_rewards mean: 0.6245343618644738\n",
      "avg loss: 1.178\n",
      "model action_tally: [215. 106.   0.  79.  28. 383.]\n",
      "train_tally:        [ 49. 113.  21.  87. 114. 112.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6245343618644736\n",
      "new discounted_rewards mean: 0.6245343618644738\n",
      "avg loss: 0.537\n",
      "model action_tally: [215. 106.   0.  79.  28. 383.]\n",
      "train_tally:        [ 49. 102.  21.  87. 103. 103.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6245343618644736\n",
      "new discounted_rewards mean: 0.6245343618644738\n",
      "avg loss: 0.332\n",
      "model action_tally: [215. 106.   0.  79.  28. 383.]\n",
      "train_tally:        [ 49. 114.  21.  87. 115. 114.]\n",
      "\n",
      "14) frames played: 523, score: 120.0\n",
      "discounted_rewards mean: 5.4343612677636724e-17\n",
      "Shifting rewards by 0.6621269510208553\n",
      "new discounted_rewards mean: 0.6621269510208553\n",
      "avg loss: 1.029\n",
      "model action_tally: [ 87. 144.  73.  71.  84.  64.]\n",
      "train_tally:        [ 0. 66. 40. 20. 62. 67.]\n",
      "discounted_rewards mean: 5.4343612677636724e-17\n",
      "Shifting rewards by 0.6621269510208553\n",
      "new discounted_rewards mean: 0.6621269510208553\n",
      "avg loss: 0.260\n",
      "model action_tally: [ 87. 144.  73.  71.  84.  64.]\n",
      "train_tally:        [ 0. 67. 40. 20. 54. 68.]\n",
      "discounted_rewards mean: 5.4343612677636724e-17\n",
      "Shifting rewards by 0.6621269510208553\n",
      "new discounted_rewards mean: 0.6621269510208553\n",
      "avg loss: 0.172\n",
      "model action_tally: [ 87. 144.  73.  71.  84.  64.]\n",
      "train_tally:        [ 0. 66. 40. 20. 59. 66.]\n",
      "\n",
      "15) frames played: 1067, score: 235.0\n",
      "discounted_rewards mean: -6.659257129897846e-18\n",
      "Shifting rewards by 0.798445722403322\n",
      "new discounted_rewards mean: 0.798445722403322\n",
      "avg loss: 1.147\n",
      "model action_tally: [  0. 172. 182. 191. 340. 182.]\n",
      "train_tally:        [ 15.  97.  27.  98. 115. 116.]\n",
      "discounted_rewards mean: -6.659257129897846e-18\n",
      "Shifting rewards by 0.798445722403322\n",
      "new discounted_rewards mean: 0.798445722403322\n",
      "avg loss: 0.514\n",
      "model action_tally: [  0. 172. 182. 191. 340. 182.]\n",
      "train_tally:        [ 15.  97.  27.  98. 102. 103.]\n",
      "discounted_rewards mean: -6.659257129897846e-18\n",
      "Shifting rewards by 0.798445722403322\n",
      "new discounted_rewards mean: 0.798445722403322\n",
      "avg loss: 0.329\n",
      "model action_tally: [  0. 172. 182. 191. 340. 182.]\n",
      "train_tally:        [ 15.  97.  27.  98. 107. 108.]\n",
      "\n",
      "16) frames played: 960, score: 195.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8158493576886159\n",
      "new discounted_rewards mean: 0.815849357688616\n",
      "avg loss: 0.950\n",
      "model action_tally: [ 30.  73.  38. 134. 128. 557.]\n",
      "train_tally:        [ 21. 114. 116.  49. 116. 102.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8158493576886159\n",
      "new discounted_rewards mean: 0.815849357688616\n",
      "avg loss: 0.415\n",
      "model action_tally: [ 30.  73.  38. 134. 128. 557.]\n",
      "train_tally:        [ 21. 108. 114.  49. 114. 105.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8158493576886159\n",
      "new discounted_rewards mean: 0.815849357688616\n",
      "avg loss: 0.305\n",
      "model action_tally: [ 30.  73.  38. 134. 128. 557.]\n",
      "train_tally:        [ 21. 113. 111.  49. 114. 106.]\n",
      "\n",
      "17) frames played: 470, score: 110.0\n",
      "discounted_rewards mean: -9.070758328852342e-17\n",
      "Shifting rewards by 0.7748870995445948\n",
      "new discounted_rewards mean: 0.7748870995445949\n",
      "avg loss: 1.162\n",
      "model action_tally: [ 79. 178.  41.  47.  92.  33.]\n",
      "train_tally:        [22. 56. 36.  0. 33. 56.]\n",
      "discounted_rewards mean: -9.070758328852342e-17\n",
      "Shifting rewards by 0.7748870995445948\n",
      "new discounted_rewards mean: 0.7748870995445949\n",
      "avg loss: 0.307\n",
      "model action_tally: [ 79. 178.  41.  47.  92.  33.]\n",
      "train_tally:        [22. 60. 35.  0. 33. 61.]\n",
      "discounted_rewards mean: -9.070758328852342e-17\n",
      "Shifting rewards by 0.7748870995445948\n",
      "new discounted_rewards mean: 0.7748870995445949\n",
      "avg loss: 0.192\n",
      "model action_tally: [ 79. 178.  41.  47.  92.  33.]\n",
      "train_tally:        [22. 63. 35.  0. 33. 64.]\n",
      "\n",
      "18) frames played: 1781, score: 390.0\n",
      "discounted_rewards mean: 6.383314863650536e-17\n",
      "Shifting rewards by 0.7978818960599956\n",
      "new discounted_rewards mean: 0.7978818960599956\n",
      "avg loss: 1.057\n",
      "model action_tally: [  43.  175.  362.    0.  112. 1089.]\n",
      "train_tally:        [213. 192. 109.  32. 209. 214.]\n",
      "discounted_rewards mean: 6.383314863650536e-17\n",
      "Shifting rewards by 0.7978818960599956\n",
      "new discounted_rewards mean: 0.7978818960599956\n",
      "avg loss: 0.435\n",
      "model action_tally: [  43.  175.  362.    0.  112. 1089.]\n",
      "train_tally:        [209. 189. 109.  32. 209. 210.]\n",
      "discounted_rewards mean: 6.383314863650536e-17\n",
      "Shifting rewards by 0.7978818960599956\n",
      "new discounted_rewards mean: 0.7978818960599956\n",
      "avg loss: 0.252\n",
      "model action_tally: [  43.  175.  362.    0.  112. 1089.]\n",
      "train_tally:        [214. 196. 109.  32. 214. 215.]\n",
      "\n",
      "19) frames played: 1328, score: 615.0\n",
      "discounted_rewards mean: -4.280377926265664e-17\n",
      "Shifting rewards by 0.6501689816235718\n",
      "new discounted_rewards mean: 0.6501689816235718\n",
      "avg loss: 1.253\n",
      "model action_tally: [ 93. 104. 183.  38. 329. 581.]\n",
      "train_tally:        [ 60. 136.  47. 136.  50. 137.]\n",
      "discounted_rewards mean: -4.280377926265664e-17\n",
      "Shifting rewards by 0.6501689816235718\n",
      "new discounted_rewards mean: 0.6501689816235718\n",
      "avg loss: 0.525\n",
      "model action_tally: [ 93. 104. 183.  38. 329. 581.]\n",
      "train_tally:        [ 60. 135.  47. 140.  50. 140.]\n",
      "discounted_rewards mean: -4.280377926265664e-17\n",
      "Shifting rewards by 0.6501689816235718\n",
      "new discounted_rewards mean: 0.6501689816235718\n",
      "avg loss: 0.343\n",
      "model action_tally: [ 93. 104. 183.  38. 329. 581.]\n",
      "train_tally:        [ 60. 131.  47. 130.  50. 131.]\n",
      "\n",
      "20) frames played: 985, score: 260.0\n",
      "discounted_rewards mean: 2.8854527340511684e-17\n",
      "Shifting rewards by 0.7472280939259976\n",
      "new discounted_rewards mean: 0.7472280939259976\n",
      "avg loss: 1.277\n",
      "model action_tally: [136. 134.   0. 164.  79. 472.]\n",
      "train_tally:        [ 79. 137. 137.  83.  32. 138.]\n",
      "discounted_rewards mean: 2.8854527340511684e-17\n",
      "Shifting rewards by 0.7472280939259976\n",
      "new discounted_rewards mean: 0.7472280939259976\n",
      "avg loss: 0.555\n",
      "model action_tally: [136. 134.   0. 164.  79. 472.]\n",
      "train_tally:        [ 79. 134. 134.  83.  32. 134.]\n",
      "discounted_rewards mean: 2.8854527340511684e-17\n",
      "Shifting rewards by 0.7472280939259976\n",
      "new discounted_rewards mean: 0.7472280939259976\n",
      "avg loss: 0.367\n",
      "model action_tally: [136. 134.   0. 164.  79. 472.]\n",
      "train_tally:        [ 79. 138. 140.  83.  32. 140.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21) frames played: 622, score: 120.0\n",
      "discounted_rewards mean: -6.854109991254986e-17\n",
      "Shifting rewards by 0.800317779413856\n",
      "new discounted_rewards mean: 0.8003177794138558\n",
      "avg loss: 1.151\n",
      "model action_tally: [ 42. 298. 127.  41.  22.  92.]\n",
      "train_tally:        [74. 74. 29.  2. 52. 75.]\n",
      "discounted_rewards mean: -6.854109991254986e-17\n",
      "Shifting rewards by 0.800317779413856\n",
      "new discounted_rewards mean: 0.8003177794138558\n",
      "avg loss: 0.504\n",
      "model action_tally: [ 42. 298. 127.  41.  22.  92.]\n",
      "train_tally:        [77. 78. 27.  2. 52. 77.]\n",
      "discounted_rewards mean: -6.854109991254986e-17\n",
      "Shifting rewards by 0.800317779413856\n",
      "new discounted_rewards mean: 0.8003177794138558\n",
      "avg loss: 0.372\n",
      "model action_tally: [ 42. 298. 127.  41.  22.  92.]\n",
      "train_tally:        [77. 77. 29.  2. 52. 77.]\n",
      "\n",
      "22) frames played: 1179, score: 335.0\n",
      "discounted_rewards mean: -2.410662377472774e-17\n",
      "Shifting rewards by 0.9543259457643467\n",
      "new discounted_rewards mean: 0.9543259457643467\n",
      "avg loss: 0.938\n",
      "model action_tally: [105. 129. 225.   0.  57. 663.]\n",
      "train_tally:        [ 83. 102. 122. 123. 123. 124.]\n",
      "discounted_rewards mean: -2.410662377472774e-17\n",
      "Shifting rewards by 0.9543259457643467\n",
      "new discounted_rewards mean: 0.9543259457643467\n",
      "avg loss: 0.349\n",
      "model action_tally: [105. 129. 225.   0.  57. 663.]\n",
      "train_tally:        [ 83. 104. 121. 124. 123. 125.]\n",
      "discounted_rewards mean: -2.410662377472774e-17\n",
      "Shifting rewards by 0.9543259457643467\n",
      "new discounted_rewards mean: 0.9543259457643467\n",
      "avg loss: 0.248\n",
      "model action_tally: [105. 129. 225.   0.  57. 663.]\n",
      "train_tally:        [ 83. 104. 116. 120. 119. 120.]\n",
      "\n",
      "23) frames played: 778, score: 215.0\n",
      "discounted_rewards mean: -2.5572232135324944e-16\n",
      "Shifting rewards by 0.9572932483928205\n",
      "new discounted_rewards mean: 0.9572932483928203\n",
      "avg loss: 1.385\n",
      "model action_tally: [ 10. 214.  76. 100.  69. 309.]\n",
      "train_tally:        [54. 68. 81. 44. 82. 82.]\n",
      "discounted_rewards mean: -2.5572232135324944e-16\n",
      "Shifting rewards by 0.9572932483928205\n",
      "new discounted_rewards mean: 0.9572932483928203\n",
      "avg loss: 0.657\n",
      "model action_tally: [ 10. 214.  76. 100.  69. 309.]\n",
      "train_tally:        [54. 70. 81. 44. 84. 84.]\n",
      "discounted_rewards mean: -2.5572232135324944e-16\n",
      "Shifting rewards by 0.9572932483928205\n",
      "new discounted_rewards mean: 0.9572932483928203\n",
      "avg loss: 0.389\n",
      "model action_tally: [ 10. 214.  76. 100.  69. 309.]\n",
      "train_tally:        [54. 72. 83. 44. 88. 88.]\n",
      "\n",
      "24) frames played: 1081, score: 370.0\n",
      "discounted_rewards mean: -6.57301328177706e-17\n",
      "Shifting rewards by 1.0644414603395143\n",
      "new discounted_rewards mean: 1.0644414603395143\n",
      "avg loss: 1.237\n",
      "model action_tally: [ 50. 115.  56. 133. 353. 374.]\n",
      "train_tally:        [100.  93. 113.  60. 110. 114.]\n",
      "discounted_rewards mean: -6.57301328177706e-17\n",
      "Shifting rewards by 1.0644414603395143\n",
      "new discounted_rewards mean: 1.0644414603395143\n",
      "avg loss: 0.626\n",
      "model action_tally: [ 50. 115.  56. 133. 353. 374.]\n",
      "train_tally:        [ 97.  93. 112.  60. 112. 113.]\n",
      "discounted_rewards mean: -6.57301328177706e-17\n",
      "Shifting rewards by 1.0644414603395143\n",
      "new discounted_rewards mean: 1.0644414603395143\n",
      "avg loss: 0.417\n",
      "model action_tally: [ 50. 115.  56. 133. 353. 374.]\n",
      "train_tally:        [ 93.  93. 118.  60. 117. 119.]\n",
      "\n",
      "25) frames played: 1261, score: 590.0\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.6203816114547169\n",
      "new discounted_rewards mean: 0.6203816114547169\n",
      "avg loss: 1.031\n",
      "model action_tally: [176.  88. 166. 245. 368. 218.]\n",
      "train_tally:        [95. 59. 66. 95. 96. 54.]\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.6203816114547169\n",
      "new discounted_rewards mean: 0.6203816114547169\n",
      "avg loss: 0.405\n",
      "model action_tally: [176.  88. 166. 245. 368. 218.]\n",
      "train_tally:        [95. 59. 66. 96. 97. 54.]\n",
      "discounted_rewards mean: 9.015609652784776e-17\n",
      "Shifting rewards by 0.6203816114547169\n",
      "new discounted_rewards mean: 0.6203816114547169\n",
      "avg loss: 0.277\n",
      "model action_tally: [176.  88. 166. 245. 368. 218.]\n",
      "train_tally:        [94. 59. 66. 92. 94. 54.]\n",
      "\n",
      "26) frames played: 653, score: 105.0\n",
      "discounted_rewards mean: 1.0881205754366006e-16\n",
      "Shifting rewards by 0.7163610839035774\n",
      "new discounted_rewards mean: 0.7163610839035777\n",
      "avg loss: 1.104\n",
      "model action_tally: [257. 141.  75.  76.  48.  56.]\n",
      "train_tally:        [48. 74. 75. 74. 71. 37.]\n",
      "discounted_rewards mean: 1.0881205754366006e-16\n",
      "Shifting rewards by 0.7163610839035774\n",
      "new discounted_rewards mean: 0.7163610839035777\n",
      "avg loss: 0.413\n",
      "model action_tally: [257. 141.  75.  76.  48.  56.]\n",
      "train_tally:        [47. 72. 73. 71. 71. 37.]\n",
      "discounted_rewards mean: 1.0881205754366006e-16\n",
      "Shifting rewards by 0.7163610839035774\n",
      "new discounted_rewards mean: 0.7163610839035777\n",
      "avg loss: 0.284\n",
      "model action_tally: [257. 141.  75.  76.  48.  56.]\n",
      "train_tally:        [48. 74. 74. 75. 71. 36.]\n",
      "\n",
      "27) frames played: 1000, score: 295.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8875176871211097\n",
      "new discounted_rewards mean: 0.8875176871211097\n",
      "avg loss: 1.253\n",
      "model action_tally: [ 17.  90. 296.  94. 309. 194.]\n",
      "train_tally:        [ 86.  69.  13. 119. 120. 121.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8875176871211097\n",
      "new discounted_rewards mean: 0.8875176871211097\n",
      "avg loss: 0.549\n",
      "model action_tally: [ 17.  90. 296.  94. 309. 194.]\n",
      "train_tally:        [ 86.  69.  13. 121. 122. 121.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8875176871211097\n",
      "new discounted_rewards mean: 0.8875176871211097\n",
      "avg loss: 0.367\n",
      "model action_tally: [ 17.  90. 296.  94. 309. 194.]\n",
      "train_tally:        [ 84.  69.  13. 124. 119. 125.]\n",
      "\n",
      "28) frames played: 400, score: 55.0\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.8415792117801038\n",
      "new discounted_rewards mean: 0.8415792117801039\n",
      "avg loss: 0.951\n",
      "model action_tally: [ 35.  89.  84. 118.   0.  74.]\n",
      "train_tally:        [ 9. 50. 50.  3. 46. 19.]\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.8415792117801038\n",
      "new discounted_rewards mean: 0.8415792117801039\n",
      "avg loss: 0.359\n",
      "model action_tally: [ 35.  89.  84. 118.   0.  74.]\n",
      "train_tally:        [ 9. 50. 50.  3. 50. 19.]\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.8415792117801038\n",
      "new discounted_rewards mean: 0.8415792117801039\n",
      "avg loss: 0.279\n",
      "model action_tally: [ 35.  89.  84. 118.   0.  74.]\n",
      "train_tally:        [ 9. 50. 51.  3. 49. 19.]\n",
      "\n",
      "29) frames played: 1089, score: 335.0\n",
      "discounted_rewards mean: -7.829672019395044e-17\n",
      "Shifting rewards by 0.909501368191189\n",
      "new discounted_rewards mean: 0.9095013681911889\n",
      "avg loss: 1.209\n",
      "model action_tally: [132. 213. 278. 141. 180. 145.]\n",
      "train_tally:        [ 97. 122.  53. 117. 122. 116.]\n",
      "discounted_rewards mean: -7.829672019395044e-17\n",
      "Shifting rewards by 0.909501368191189\n",
      "new discounted_rewards mean: 0.9095013681911889\n",
      "avg loss: 0.593\n",
      "model action_tally: [132. 213. 278. 141. 180. 145.]\n",
      "train_tally:        [ 93. 122.  53. 122. 124. 124.]\n",
      "discounted_rewards mean: -7.829672019395044e-17\n",
      "Shifting rewards by 0.909501368191189\n",
      "new discounted_rewards mean: 0.9095013681911889\n",
      "avg loss: 0.475\n",
      "model action_tally: [132. 213. 278. 141. 180. 145.]\n",
      "train_tally:        [ 95. 119.  53. 116. 120. 118.]\n",
      "\n",
      "30) frames played: 691, score: 155.0\n",
      "discounted_rewards mean: 8.22625453846715e-17\n",
      "Shifting rewards by 0.7802874351607637\n",
      "new discounted_rewards mean: 0.7802874351607637\n",
      "avg loss: 1.253\n",
      "model action_tally: [153. 150.  40.  76. 157. 115.]\n",
      "train_tally:        [24. 71. 87. 46. 25. 88.]\n",
      "discounted_rewards mean: 8.22625453846715e-17\n",
      "Shifting rewards by 0.7802874351607637\n",
      "new discounted_rewards mean: 0.7802874351607637\n",
      "avg loss: 0.483\n",
      "model action_tally: [153. 150.  40.  76. 157. 115.]\n",
      "train_tally:        [24. 69. 96. 46. 25. 97.]\n",
      "discounted_rewards mean: 8.22625453846715e-17\n",
      "Shifting rewards by 0.7802874351607637\n",
      "new discounted_rewards mean: 0.7802874351607637\n",
      "avg loss: 0.356\n",
      "model action_tally: [153. 150.  40.  76. 157. 115.]\n",
      "train_tally:        [24. 71. 91. 46. 25. 91.]\n",
      "\n",
      "31) frames played: 701, score: 235.0\n",
      "discounted_rewards mean: 1.4190582454552642e-16\n",
      "Shifting rewards by 1.0983102338750461\n",
      "new discounted_rewards mean: 1.0983102338750463\n",
      "avg loss: 1.070\n",
      "model action_tally: [ 11.  91. 153.  56. 113. 277.]\n",
      "train_tally:        [71. 80. 60. 27. 28. 81.]\n",
      "discounted_rewards mean: 1.4190582454552642e-16\n",
      "Shifting rewards by 1.0983102338750461\n",
      "new discounted_rewards mean: 1.0983102338750463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.509\n",
      "model action_tally: [ 11.  91. 153.  56. 113. 277.]\n",
      "train_tally:        [75. 77. 58. 27. 28. 77.]\n",
      "discounted_rewards mean: 1.4190582454552642e-16\n",
      "Shifting rewards by 1.0983102338750461\n",
      "new discounted_rewards mean: 1.0983102338750463\n",
      "avg loss: 0.419\n",
      "model action_tally: [ 11.  91. 153.  56. 113. 277.]\n",
      "train_tally:        [73. 80. 60. 27. 28. 80.]\n",
      "\n",
      "32) frames played: 498, score: 70.0\n",
      "discounted_rewards mean: -5.707170568354218e-17\n",
      "Shifting rewards by 0.7305356853030974\n",
      "new discounted_rewards mean: 0.7305356853030973\n",
      "avg loss: 0.929\n",
      "model action_tally: [ 37. 175.  99.  39.  25. 123.]\n",
      "train_tally:        [47. 47. 38. 43. 47. 36.]\n",
      "discounted_rewards mean: -5.707170568354218e-17\n",
      "Shifting rewards by 0.7305356853030974\n",
      "new discounted_rewards mean: 0.7305356853030973\n",
      "avg loss: 0.300\n",
      "model action_tally: [ 37. 175.  99.  39.  25. 123.]\n",
      "train_tally:        [46. 47. 42. 43. 48. 36.]\n",
      "discounted_rewards mean: -5.707170568354218e-17\n",
      "Shifting rewards by 0.7305356853030974\n",
      "new discounted_rewards mean: 0.7305356853030973\n",
      "avg loss: 0.181\n",
      "model action_tally: [ 37. 175.  99.  39.  25. 123.]\n",
      "train_tally:        [48. 46. 44. 45. 48. 36.]\n",
      "\n",
      "33) frames played: 406, score: 80.0\n",
      "discounted_rewards mean: -5.2503157814785726e-17\n",
      "Shifting rewards by 0.7589605905877844\n",
      "new discounted_rewards mean: 0.7589605905877842\n",
      "avg loss: 1.576\n",
      "model action_tally: [ 67. 102.  92.  44.  41.  60.]\n",
      "train_tally:        [10. 43. 47. 11. 38. 47.]\n",
      "discounted_rewards mean: -5.2503157814785726e-17\n",
      "Shifting rewards by 0.7589605905877844\n",
      "new discounted_rewards mean: 0.7589605905877842\n",
      "avg loss: 0.607\n",
      "model action_tally: [ 67. 102.  92.  44.  41.  60.]\n",
      "train_tally:        [10. 51. 50. 11. 41. 52.]\n",
      "discounted_rewards mean: -5.2503157814785726e-17\n",
      "Shifting rewards by 0.7589605905877844\n",
      "new discounted_rewards mean: 0.7589605905877842\n",
      "avg loss: 0.396\n",
      "model action_tally: [ 67. 102.  92.  44.  41.  60.]\n",
      "train_tally:        [ 9. 47. 45. 11. 39. 48.]\n",
      "\n",
      "34) frames played: 741, score: 275.0\n",
      "discounted_rewards mean: -5.753382475790285e-17\n",
      "Shifting rewards by 0.9928032644687593\n",
      "new discounted_rewards mean: 0.9928032644687591\n",
      "avg loss: 1.065\n",
      "model action_tally: [ 31. 178.  95.   1. 205. 231.]\n",
      "train_tally:        [71. 71. 74. 73. 71. 74.]\n",
      "discounted_rewards mean: -5.753382475790285e-17\n",
      "Shifting rewards by 0.9928032644687593\n",
      "new discounted_rewards mean: 0.9928032644687591\n",
      "avg loss: 0.541\n",
      "model action_tally: [ 31. 178.  95.   1. 205. 231.]\n",
      "train_tally:        [70. 71. 75. 75. 70. 74.]\n",
      "discounted_rewards mean: -5.753382475790285e-17\n",
      "Shifting rewards by 0.9928032644687593\n",
      "new discounted_rewards mean: 0.9928032644687591\n",
      "avg loss: 0.444\n",
      "model action_tally: [ 31. 178.  95.   1. 205. 231.]\n",
      "train_tally:        [75. 77. 74. 74. 75. 77.]\n",
      "\n",
      "35) frames played: 395, score: 60.0\n",
      "discounted_rewards mean: -1.798842369012912e-17\n",
      "Shifting rewards by 0.8724585115367595\n",
      "new discounted_rewards mean: 0.8724585115367596\n",
      "avg loss: 0.890\n",
      "model action_tally: [ 73. 178.  35.  22.   0.  87.]\n",
      "train_tally:        [19. 30. 33. 46. 43. 47.]\n",
      "discounted_rewards mean: -1.798842369012912e-17\n",
      "Shifting rewards by 0.8724585115367595\n",
      "new discounted_rewards mean: 0.8724585115367596\n",
      "avg loss: 0.468\n",
      "model action_tally: [ 73. 178.  35.  22.   0.  87.]\n",
      "train_tally:        [19. 30. 30. 46. 45. 46.]\n",
      "discounted_rewards mean: -1.798842369012912e-17\n",
      "Shifting rewards by 0.8724585115367595\n",
      "new discounted_rewards mean: 0.8724585115367596\n",
      "avg loss: 0.300\n",
      "model action_tally: [ 73. 178.  35.  22.   0.  87.]\n",
      "train_tally:        [19. 29. 33. 44. 44. 45.]\n",
      "\n",
      "36) frames played: 927, score: 225.0\n",
      "discounted_rewards mean: 1.5329940361598708e-17\n",
      "Shifting rewards by 0.9615212811416748\n",
      "new discounted_rewards mean: 0.9615212811416747\n",
      "avg loss: 1.167\n",
      "model action_tally: [ 27.  89. 168. 129. 317. 197.]\n",
      "train_tally:        [86. 95. 42. 78. 90. 96.]\n",
      "discounted_rewards mean: 1.5329940361598708e-17\n",
      "Shifting rewards by 0.9615212811416748\n",
      "new discounted_rewards mean: 0.9615212811416747\n",
      "avg loss: 0.491\n",
      "model action_tally: [ 27.  89. 168. 129. 317. 197.]\n",
      "train_tally:        [ 86. 100.  42.  75. 101.  98.]\n",
      "discounted_rewards mean: 1.5329940361598708e-17\n",
      "Shifting rewards by 0.9615212811416748\n",
      "new discounted_rewards mean: 0.9615212811416747\n",
      "avg loss: 0.343\n",
      "model action_tally: [ 27.  89. 168. 129. 317. 197.]\n",
      "train_tally:        [84. 95. 42. 79. 95. 95.]\n",
      "\n",
      "37) frames played: 405, score: 105.0\n",
      "discounted_rewards mean: 2.631639762074445e-16\n",
      "Shifting rewards by 0.8936168986337154\n",
      "new discounted_rewards mean: 0.8936168986337154\n",
      "avg loss: 1.320\n",
      "model action_tally: [53. 95. 75. 58. 54. 70.]\n",
      "train_tally:        [48. 50. 51.  6. 30. 50.]\n",
      "discounted_rewards mean: 2.631639762074445e-16\n",
      "Shifting rewards by 0.8936168986337154\n",
      "new discounted_rewards mean: 0.8936168986337154\n",
      "avg loss: 0.574\n",
      "model action_tally: [53. 95. 75. 58. 54. 70.]\n",
      "train_tally:        [46. 44. 46.  6. 30. 47.]\n",
      "discounted_rewards mean: 2.631639762074445e-16\n",
      "Shifting rewards by 0.8936168986337154\n",
      "new discounted_rewards mean: 0.8936168986337154\n",
      "avg loss: 0.442\n",
      "model action_tally: [53. 95. 75. 58. 54. 70.]\n",
      "train_tally:        [48. 46. 49.  6. 30. 46.]\n",
      "\n",
      "38) frames played: 346, score: 105.0\n",
      "discounted_rewards mean: 2.0535917218500005e-16\n",
      "Shifting rewards by 0.8823795882576794\n",
      "new discounted_rewards mean: 0.8823795882576795\n",
      "avg loss: 0.751\n",
      "model action_tally: [ 21.  43. 113.  92.  30.  47.]\n",
      "train_tally:        [42. 39. 14. 42.  0. 22.]\n",
      "discounted_rewards mean: 2.0535917218500005e-16\n",
      "Shifting rewards by 0.8823795882576794\n",
      "new discounted_rewards mean: 0.8823795882576795\n",
      "avg loss: 0.234\n",
      "model action_tally: [ 21.  43. 113.  92.  30.  47.]\n",
      "train_tally:        [43. 37. 14. 43.  0. 22.]\n",
      "discounted_rewards mean: 2.0535917218500005e-16\n",
      "Shifting rewards by 0.8823795882576794\n",
      "new discounted_rewards mean: 0.8823795882576795\n",
      "avg loss: 0.279\n",
      "model action_tally: [ 21.  43. 113.  92.  30.  47.]\n",
      "train_tally:        [47. 41. 14. 47.  0. 22.]\n",
      "\n",
      "39) frames played: 672, score: 105.0\n",
      "discounted_rewards mean: -8.458842092382145e-17\n",
      "Shifting rewards by 0.6026163733112831\n",
      "new discounted_rewards mean: 0.6026163733112831\n",
      "avg loss: 1.187\n",
      "model action_tally: [217.  95.  78. 175.  66.  41.]\n",
      "train_tally:        [19. 92. 91. 22. 92. 92.]\n",
      "discounted_rewards mean: -8.458842092382145e-17\n",
      "Shifting rewards by 0.6026163733112831\n",
      "new discounted_rewards mean: 0.6026163733112831\n",
      "avg loss: 0.400\n",
      "model action_tally: [217.  95.  78. 175.  66.  41.]\n",
      "train_tally:        [19. 91. 92. 22. 91. 93.]\n",
      "discounted_rewards mean: -8.458842092382145e-17\n",
      "Shifting rewards by 0.6026163733112831\n",
      "new discounted_rewards mean: 0.6026163733112831\n",
      "avg loss: 0.243\n",
      "model action_tally: [217.  95.  78. 175.  66.  41.]\n",
      "train_tally:        [19. 98. 95. 22. 96. 98.]\n",
      "\n",
      "40) frames played: 1312, score: 470.0\n",
      "discounted_rewards mean: -2.166288828536891e-17\n",
      "Shifting rewards by 1.0797184056094795\n",
      "new discounted_rewards mean: 1.0797184056094793\n",
      "avg loss: 1.078\n",
      "model action_tally: [ 42. 220. 103.  33. 588. 326.]\n",
      "train_tally:        [ 66. 118.  90. 148. 149. 149.]\n",
      "discounted_rewards mean: -2.166288828536891e-17\n",
      "Shifting rewards by 1.0797184056094795\n",
      "new discounted_rewards mean: 1.0797184056094793\n",
      "avg loss: 0.464\n",
      "model action_tally: [ 42. 220. 103.  33. 588. 326.]\n",
      "train_tally:        [ 66. 120.  89. 147. 148. 147.]\n",
      "discounted_rewards mean: -2.166288828536891e-17\n",
      "Shifting rewards by 1.0797184056094795\n",
      "new discounted_rewards mean: 1.0797184056094793\n",
      "avg loss: 0.333\n",
      "model action_tally: [ 42. 220. 103.  33. 588. 326.]\n",
      "train_tally:        [ 66. 117.  90. 144. 146. 147.]\n",
      "\n",
      "41) frames played: 629, score: 110.0\n",
      "discounted_rewards mean: 2.259277379205406e-17\n",
      "Shifting rewards by 0.8877021859694466\n",
      "new discounted_rewards mean: 0.8877021859694467\n",
      "avg loss: 0.941\n",
      "model action_tally: [147. 120. 116.  46.  55. 145.]\n",
      "train_tally:        [64. 36. 69. 45. 69. 32.]\n",
      "discounted_rewards mean: 2.259277379205406e-17\n",
      "Shifting rewards by 0.8877021859694466\n",
      "new discounted_rewards mean: 0.8877021859694467\n",
      "avg loss: 0.340\n",
      "model action_tally: [147. 120. 116.  46.  55. 145.]\n",
      "train_tally:        [64. 36. 67. 44. 67. 32.]\n",
      "discounted_rewards mean: 2.259277379205406e-17\n",
      "Shifting rewards by 0.8877021859694466\n",
      "new discounted_rewards mean: 0.8877021859694467\n",
      "avg loss: 0.251\n",
      "model action_tally: [147. 120. 116.  46.  55. 145.]\n",
      "train_tally:        [64. 36. 66. 44. 66. 32.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "42) frames played: 552, score: 80.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7028491477283041\n",
      "new discounted_rewards mean: 0.7028491477283041\n",
      "avg loss: 1.088\n",
      "model action_tally: [ 85.  33. 251. 117.  35.  31.]\n",
      "train_tally:        [64. 65. 35. 22. 25. 66.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7028491477283041\n",
      "new discounted_rewards mean: 0.7028491477283041\n",
      "avg loss: 0.407\n",
      "model action_tally: [ 85.  33. 251. 117.  35.  31.]\n",
      "train_tally:        [64. 69. 34. 22. 25. 70.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7028491477283041\n",
      "new discounted_rewards mean: 0.7028491477283041\n",
      "avg loss: 0.295\n",
      "model action_tally: [ 85.  33. 251. 117.  35.  31.]\n",
      "train_tally:        [70. 71. 36. 22. 24. 70.]\n",
      "\n",
      "43) frames played: 860, score: 210.0\n",
      "discounted_rewards mean: -5.783487384093839e-17\n",
      "Shifting rewards by 0.7482173514686549\n",
      "new discounted_rewards mean: 0.7482173514686549\n",
      "avg loss: 1.302\n",
      "model action_tally: [144. 101.  74.  34.  60. 447.]\n",
      "train_tally:        [ 33. 101. 101.  86.  58. 102.]\n",
      "discounted_rewards mean: -5.783487384093839e-17\n",
      "Shifting rewards by 0.7482173514686549\n",
      "new discounted_rewards mean: 0.7482173514686549\n",
      "avg loss: 0.562\n",
      "model action_tally: [144. 101.  74.  34.  60. 447.]\n",
      "train_tally:        [ 33. 107. 105.  85.  58. 108.]\n",
      "discounted_rewards mean: -5.783487384093839e-17\n",
      "Shifting rewards by 0.7482173514686549\n",
      "new discounted_rewards mean: 0.7482173514686549\n",
      "avg loss: 0.361\n",
      "model action_tally: [144. 101.  74.  34.  60. 447.]\n",
      "train_tally:        [ 33. 103. 103.  86.  58. 102.]\n",
      "\n",
      "44) frames played: 1040, score: 275.0\n",
      "discounted_rewards mean: 6.832141690000964e-17\n",
      "Shifting rewards by 0.885391501277817\n",
      "new discounted_rewards mean: 0.885391501277817\n",
      "avg loss: 1.348\n",
      "model action_tally: [112. 127. 205. 253. 139. 204.]\n",
      "train_tally:        [106. 106.  55.  70.  86. 107.]\n",
      "discounted_rewards mean: 6.832141690000964e-17\n",
      "Shifting rewards by 0.885391501277817\n",
      "new discounted_rewards mean: 0.885391501277817\n",
      "avg loss: 0.611\n",
      "model action_tally: [112. 127. 205. 253. 139. 204.]\n",
      "train_tally:        [105. 108.  55.  70.  89. 109.]\n",
      "discounted_rewards mean: 6.832141690000964e-17\n",
      "Shifting rewards by 0.885391501277817\n",
      "new discounted_rewards mean: 0.885391501277817\n",
      "avg loss: 0.459\n",
      "model action_tally: [112. 127. 205. 253. 139. 204.]\n",
      "train_tally:        [105. 107.  55.  70.  84. 108.]\n",
      "\n",
      "45) frames played: 1520, score: 620.0\n",
      "discounted_rewards mean: 3.739698609263685e-17\n",
      "Shifting rewards by 0.6095653227684512\n",
      "new discounted_rewards mean: 0.6095653227684512\n",
      "avg loss: 1.134\n",
      "model action_tally: [226. 237. 104. 298. 142. 513.]\n",
      "train_tally:        [117.  72. 117.  86. 116. 118.]\n",
      "discounted_rewards mean: 3.739698609263685e-17\n",
      "Shifting rewards by 0.6095653227684512\n",
      "new discounted_rewards mean: 0.6095653227684512\n",
      "avg loss: 0.543\n",
      "model action_tally: [226. 237. 104. 298. 142. 513.]\n",
      "train_tally:        [115.  72. 114.  91. 118. 119.]\n",
      "discounted_rewards mean: 3.739698609263685e-17\n",
      "Shifting rewards by 0.6095653227684512\n",
      "new discounted_rewards mean: 0.6095653227684512\n",
      "avg loss: 0.419\n",
      "model action_tally: [226. 237. 104. 298. 142. 513.]\n",
      "train_tally:        [114.  72. 104.  91. 113. 114.]\n",
      "\n",
      "46) frames played: 831, score: 225.0\n",
      "discounted_rewards mean: -5.1302724603617344e-17\n",
      "Shifting rewards by 0.8474616652879126\n",
      "new discounted_rewards mean: 0.8474616652879124\n",
      "avg loss: 1.119\n",
      "model action_tally: [191.  33. 183. 187.  66. 171.]\n",
      "train_tally:        [93. 98. 34. 85. 23. 99.]\n",
      "discounted_rewards mean: -5.1302724603617344e-17\n",
      "Shifting rewards by 0.8474616652879126\n",
      "new discounted_rewards mean: 0.8474616652879124\n",
      "avg loss: 0.493\n",
      "model action_tally: [191.  33. 183. 187.  66. 171.]\n",
      "train_tally:        [85. 90. 34. 84. 23. 90.]\n",
      "discounted_rewards mean: -5.1302724603617344e-17\n",
      "Shifting rewards by 0.8474616652879126\n",
      "new discounted_rewards mean: 0.8474616652879124\n",
      "avg loss: 0.331\n",
      "model action_tally: [191.  33. 183. 187.  66. 171.]\n",
      "train_tally:        [92. 95. 34. 82. 23. 96.]\n",
      "\n",
      "47) frames played: 643, score: 115.0\n",
      "discounted_rewards mean: -8.840345079441371e-17\n",
      "Shifting rewards by 0.6709041921960195\n",
      "new discounted_rewards mean: 0.6709041921960195\n",
      "avg loss: 1.193\n",
      "model action_tally: [ 45. 237.  31. 259.  11.  60.]\n",
      "train_tally:        [58. 19. 36. 61. 72. 73.]\n",
      "discounted_rewards mean: -8.840345079441371e-17\n",
      "Shifting rewards by 0.6709041921960195\n",
      "new discounted_rewards mean: 0.6709041921960195\n",
      "avg loss: 0.514\n",
      "model action_tally: [ 45. 237.  31. 259.  11.  60.]\n",
      "train_tally:        [58. 19. 36. 60. 69. 70.]\n",
      "discounted_rewards mean: -8.840345079441371e-17\n",
      "Shifting rewards by 0.6709041921960195\n",
      "new discounted_rewards mean: 0.6709041921960195\n",
      "avg loss: 0.348\n",
      "model action_tally: [ 45. 237.  31. 259.  11.  60.]\n",
      "train_tally:        [57. 19. 36. 63. 73. 74.]\n",
      "\n",
      "48) frames played: 706, score: 120.0\n",
      "discounted_rewards mean: -7.045041289406092e-17\n",
      "Shifting rewards by 0.8264526423433968\n",
      "new discounted_rewards mean: 0.8264526423433968\n",
      "avg loss: 1.331\n",
      "model action_tally: [ 75.  18. 107. 313.  80. 113.]\n",
      "train_tally:        [37. 60. 43. 69. 64. 69.]\n",
      "discounted_rewards mean: -7.045041289406092e-17\n",
      "Shifting rewards by 0.8264526423433968\n",
      "new discounted_rewards mean: 0.8264526423433968\n",
      "avg loss: 0.610\n",
      "model action_tally: [ 75.  18. 107. 313.  80. 113.]\n",
      "train_tally:        [37. 58. 44. 70. 62. 70.]\n",
      "discounted_rewards mean: -7.045041289406092e-17\n",
      "Shifting rewards by 0.8264526423433968\n",
      "new discounted_rewards mean: 0.8264526423433968\n",
      "avg loss: 0.522\n",
      "model action_tally: [ 75.  18. 107. 313.  80. 113.]\n",
      "train_tally:        [36. 60. 44. 63. 64. 64.]\n",
      "\n",
      "49) frames played: 1212, score: 385.0\n",
      "discounted_rewards mean: 8.207589356964853e-17\n",
      "Shifting rewards by 1.0187298036236845\n",
      "new discounted_rewards mean: 1.0187298036236845\n",
      "avg loss: 1.167\n",
      "model action_tally: [ 20. 184.  57. 197. 206. 548.]\n",
      "train_tally:        [ 55.  97.  58. 142. 143. 143.]\n",
      "discounted_rewards mean: 8.207589356964853e-17\n",
      "Shifting rewards by 1.0187298036236845\n",
      "new discounted_rewards mean: 1.0187298036236845\n",
      "avg loss: 0.609\n",
      "model action_tally: [ 20. 184.  57. 197. 206. 548.]\n",
      "train_tally:        [ 55.  94.  58. 138. 139. 138.]\n",
      "discounted_rewards mean: 8.207589356964853e-17\n",
      "Shifting rewards by 1.0187298036236845\n",
      "new discounted_rewards mean: 1.0187298036236845\n",
      "avg loss: 0.428\n",
      "model action_tally: [ 20. 184.  57. 197. 206. 548.]\n",
      "train_tally:        [ 55.  97.  58. 138. 140. 140.]\n",
      "\n",
      "50) frames played: 405, score: 75.0\n",
      "discounted_rewards mean: -1.75442650804963e-16\n",
      "Shifting rewards by 0.7677528722348498\n",
      "new discounted_rewards mean: 0.7677528722348497\n",
      "avg loss: 1.080\n",
      "model action_tally: [ 36.  10.  30.  94.  21. 214.]\n",
      "train_tally:        [19. 46. 46. 26.  3. 41.]\n",
      "discounted_rewards mean: -1.75442650804963e-16\n",
      "Shifting rewards by 0.7677528722348498\n",
      "new discounted_rewards mean: 0.7677528722348497\n",
      "avg loss: 0.504\n",
      "model action_tally: [ 36.  10.  30.  94.  21. 214.]\n",
      "train_tally:        [19. 46. 45. 26.  3. 46.]\n",
      "discounted_rewards mean: -1.75442650804963e-16\n",
      "Shifting rewards by 0.7677528722348498\n",
      "new discounted_rewards mean: 0.7677528722348497\n",
      "avg loss: 0.435\n",
      "model action_tally: [ 36.  10.  30.  94.  21. 214.]\n",
      "train_tally:        [19. 43. 43. 26.  3. 41.]\n",
      "\n",
      "51) frames played: 887, score: 370.0\n",
      "discounted_rewards mean: 9.61275403508591e-17\n",
      "Shifting rewards by 1.1298604852908738\n",
      "new discounted_rewards mean: 1.129860485290874\n",
      "avg loss: 1.338\n",
      "model action_tally: [134. 193.  69. 103. 192. 196.]\n",
      "train_tally:        [ 64.  82.  25. 103. 100. 103.]\n",
      "discounted_rewards mean: 9.61275403508591e-17\n",
      "Shifting rewards by 1.1298604852908738\n",
      "new discounted_rewards mean: 1.129860485290874\n",
      "avg loss: 0.745\n",
      "model action_tally: [134. 193.  69. 103. 192. 196.]\n",
      "train_tally:        [ 65.  82.  25. 113. 112. 113.]\n",
      "discounted_rewards mean: 9.61275403508591e-17\n",
      "Shifting rewards by 1.1298604852908738\n",
      "new discounted_rewards mean: 1.129860485290874\n",
      "avg loss: 0.537\n",
      "model action_tally: [134. 193.  69. 103. 192. 196.]\n",
      "train_tally:        [ 65.  82.  25. 110. 109. 111.]\n",
      "\n",
      "52) frames played: 974, score: 380.0\n",
      "discounted_rewards mean: 5.836079965175361e-17\n",
      "Shifting rewards by 1.0290934893636208\n",
      "new discounted_rewards mean: 1.029093489363621\n",
      "avg loss: 1.098\n",
      "model action_tally: [ 68. 202. 107. 165. 208. 224.]\n",
      "train_tally:        [114. 115. 116.  26.  60. 111.]\n",
      "discounted_rewards mean: 5.836079965175361e-17\n",
      "Shifting rewards by 1.0290934893636208\n",
      "new discounted_rewards mean: 1.029093489363621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.507\n",
      "model action_tally: [ 68. 202. 107. 165. 208. 224.]\n",
      "train_tally:        [118. 118. 112.  26.  60. 119.]\n",
      "discounted_rewards mean: 5.836079965175361e-17\n",
      "Shifting rewards by 1.0290934893636208\n",
      "new discounted_rewards mean: 1.029093489363621\n",
      "avg loss: 0.332\n",
      "model action_tally: [ 68. 202. 107. 165. 208. 224.]\n",
      "train_tally:        [113. 111. 114.  26.  60. 115.]\n",
      "\n",
      "53) frames played: 527, score: 155.0\n",
      "discounted_rewards mean: 2.6965568719548395e-17\n",
      "Shifting rewards by 1.062374574938902\n",
      "new discounted_rewards mean: 1.062374574938902\n",
      "avg loss: 0.995\n",
      "model action_tally: [ 34.  28. 209.  60.  43. 153.]\n",
      "train_tally:        [36. 37. 32. 53. 54. 54.]\n",
      "discounted_rewards mean: 2.6965568719548395e-17\n",
      "Shifting rewards by 1.062374574938902\n",
      "new discounted_rewards mean: 1.062374574938902\n",
      "avg loss: 0.528\n",
      "model action_tally: [ 34.  28. 209.  60.  43. 153.]\n",
      "train_tally:        [38. 36. 32. 55. 56. 54.]\n",
      "discounted_rewards mean: 2.6965568719548395e-17\n",
      "Shifting rewards by 1.062374574938902\n",
      "new discounted_rewards mean: 1.062374574938902\n",
      "avg loss: 0.444\n",
      "model action_tally: [ 34.  28. 209.  60.  43. 153.]\n",
      "train_tally:        [34. 37. 32. 53. 53. 54.]\n",
      "\n",
      "54) frames played: 691, score: 185.0\n",
      "discounted_rewards mean: 1.2339381807700728e-16\n",
      "Shifting rewards by 0.8344600266524461\n",
      "new discounted_rewards mean: 0.8344600266524463\n",
      "avg loss: 1.096\n",
      "model action_tally: [ 57. 157.  61.  50. 194. 172.]\n",
      "train_tally:        [89. 89. 39. 90. 26. 67.]\n",
      "discounted_rewards mean: 1.2339381807700728e-16\n",
      "Shifting rewards by 0.8344600266524461\n",
      "new discounted_rewards mean: 0.8344600266524463\n",
      "avg loss: 0.447\n",
      "model action_tally: [ 57. 157.  61.  50. 194. 172.]\n",
      "train_tally:        [92. 93. 39. 93. 26. 69.]\n",
      "discounted_rewards mean: 1.2339381807700728e-16\n",
      "Shifting rewards by 0.8344600266524461\n",
      "new discounted_rewards mean: 0.8344600266524463\n",
      "avg loss: 0.305\n",
      "model action_tally: [ 57. 157.  61.  50. 194. 172.]\n",
      "train_tally:        [88. 89. 39. 87. 26. 69.]\n",
      "\n",
      "55) frames played: 1583, score: 450.0\n",
      "discounted_rewards mean: 1.9749766502491729e-16\n",
      "Shifting rewards by 0.9117484863004949\n",
      "new discounted_rewards mean: 0.911748486300495\n",
      "avg loss: 1.283\n",
      "model action_tally: [ 41. 786.  10. 280.  59. 407.]\n",
      "train_tally:        [162. 146. 165. 110. 161. 166.]\n",
      "discounted_rewards mean: 1.9749766502491729e-16\n",
      "Shifting rewards by 0.9117484863004949\n",
      "new discounted_rewards mean: 0.911748486300495\n",
      "avg loss: 0.709\n",
      "model action_tally: [ 41. 786.  10. 280.  59. 407.]\n",
      "train_tally:        [163. 145. 165. 110. 166. 166.]\n",
      "discounted_rewards mean: 1.9749766502491729e-16\n",
      "Shifting rewards by 0.9117484863004949\n",
      "new discounted_rewards mean: 0.911748486300495\n",
      "avg loss: 0.438\n",
      "model action_tally: [ 41. 786.  10. 280.  59. 407.]\n",
      "train_tally:        [167. 146. 170. 110. 171. 171.]\n",
      "\n",
      "56) frames played: 675, score: 310.0\n",
      "discounted_rewards mean: 8.421247238638224e-17\n",
      "Shifting rewards by 0.5112668540571266\n",
      "new discounted_rewards mean: 0.5112668540571266\n",
      "avg loss: 1.326\n",
      "model action_tally: [190.  74.  96.  95. 106. 114.]\n",
      "train_tally:        [13. 27. 10. 63. 40. 64.]\n",
      "discounted_rewards mean: 8.421247238638224e-17\n",
      "Shifting rewards by 0.5112668540571266\n",
      "new discounted_rewards mean: 0.5112668540571266\n",
      "avg loss: 0.451\n",
      "model action_tally: [190.  74.  96.  95. 106. 114.]\n",
      "train_tally:        [13. 27. 10. 62. 39. 62.]\n",
      "discounted_rewards mean: 8.421247238638224e-17\n",
      "Shifting rewards by 0.5112668540571266\n",
      "new discounted_rewards mean: 0.5112668540571266\n",
      "avg loss: 0.263\n",
      "model action_tally: [190.  74.  96.  95. 106. 114.]\n",
      "train_tally:        [13. 27. 10. 64. 40. 65.]\n",
      "\n",
      "57) frames played: 527, score: 80.0\n",
      "discounted_rewards mean: -5.393113743909679e-17\n",
      "Shifting rewards by 0.6817674319252156\n",
      "new discounted_rewards mean: 0.6817674319252156\n",
      "avg loss: 1.184\n",
      "model action_tally: [ 33.  51.  56. 189.  68. 130.]\n",
      "train_tally:        [59. 57. 43. 39. 42. 59.]\n",
      "discounted_rewards mean: -5.393113743909679e-17\n",
      "Shifting rewards by 0.6817674319252156\n",
      "new discounted_rewards mean: 0.6817674319252156\n",
      "avg loss: 0.342\n",
      "model action_tally: [ 33.  51.  56. 189.  68. 130.]\n",
      "train_tally:        [63. 63. 43. 39. 42. 63.]\n",
      "discounted_rewards mean: -5.393113743909679e-17\n",
      "Shifting rewards by 0.6817674319252156\n",
      "new discounted_rewards mean: 0.6817674319252156\n",
      "avg loss: 0.220\n",
      "model action_tally: [ 33.  51.  56. 189.  68. 130.]\n",
      "train_tally:        [60. 59. 43. 39. 42. 60.]\n",
      "\n",
      "58) frames played: 1340, score: 445.0\n",
      "discounted_rewards mean: 4.242046183642389e-17\n",
      "Shifting rewards by 1.0117781240893442\n",
      "new discounted_rewards mean: 1.0117781240893442\n",
      "avg loss: 1.010\n",
      "model action_tally: [120. 253.  75.  56. 174. 662.]\n",
      "train_tally:        [ 38. 156. 158. 131.  81. 159.]\n",
      "discounted_rewards mean: 4.242046183642389e-17\n",
      "Shifting rewards by 1.0117781240893442\n",
      "new discounted_rewards mean: 1.0117781240893442\n",
      "avg loss: 0.475\n",
      "model action_tally: [120. 253.  75.  56. 174. 662.]\n",
      "train_tally:        [ 38. 165. 165. 132.  81. 166.]\n",
      "discounted_rewards mean: 4.242046183642389e-17\n",
      "Shifting rewards by 1.0117781240893442\n",
      "new discounted_rewards mean: 1.0117781240893442\n",
      "avg loss: 0.393\n",
      "model action_tally: [120. 253.  75.  56. 174. 662.]\n",
      "train_tally:        [ 38. 161. 160. 129.  81. 161.]\n",
      "\n",
      "59) frames played: 425, score: 75.0\n",
      "discounted_rewards mean: -3.3437305212240006e-17\n",
      "Shifting rewards by 0.7536302702958853\n",
      "new discounted_rewards mean: 0.7536302702958851\n",
      "avg loss: 1.035\n",
      "model action_tally: [ 11.  62.  16. 185.  43. 108.]\n",
      "train_tally:        [44. 10.  8. 24. 49. 50.]\n",
      "discounted_rewards mean: -3.3437305212240006e-17\n",
      "Shifting rewards by 0.7536302702958853\n",
      "new discounted_rewards mean: 0.7536302702958851\n",
      "avg loss: 0.393\n",
      "model action_tally: [ 11.  62.  16. 185.  43. 108.]\n",
      "train_tally:        [44. 10.  8. 24. 49. 49.]\n",
      "discounted_rewards mean: -3.3437305212240006e-17\n",
      "Shifting rewards by 0.7536302702958853\n",
      "new discounted_rewards mean: 0.7536302702958851\n",
      "avg loss: 0.240\n",
      "model action_tally: [ 11.  62.  16. 185.  43. 108.]\n",
      "train_tally:        [43. 10.  8. 24. 46. 46.]\n",
      "\n",
      "60) frames played: 800, score: 585.0\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.7770453894296767\n",
      "new discounted_rewards mean: 0.7770453894296767\n",
      "avg loss: 1.563\n",
      "model action_tally: [ 67.  32.  43.  43. 436. 179.]\n",
      "train_tally:        [27. 55. 62. 63. 63. 44.]\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.7770453894296767\n",
      "new discounted_rewards mean: 0.7770453894296767\n",
      "avg loss: 0.664\n",
      "model action_tally: [ 67.  32.  43.  43. 436. 179.]\n",
      "train_tally:        [27. 52. 59. 58. 60. 44.]\n",
      "discounted_rewards mean: 1.0658141036401502e-16\n",
      "Shifting rewards by 0.7770453894296767\n",
      "new discounted_rewards mean: 0.7770453894296767\n",
      "avg loss: 0.410\n",
      "model action_tally: [ 67.  32.  43.  43. 436. 179.]\n",
      "train_tally:        [27. 54. 57. 56. 57. 44.]\n",
      "\n",
      "61) frames played: 504, score: 75.0\n",
      "discounted_rewards mean: -5.639228061588096e-17\n",
      "Shifting rewards by 0.6591007373269797\n",
      "new discounted_rewards mean: 0.6591007373269797\n",
      "avg loss: 0.969\n",
      "model action_tally: [ 11. 182.  45. 102.  60. 104.]\n",
      "train_tally:        [46. 14. 37. 65. 64. 65.]\n",
      "discounted_rewards mean: -5.639228061588096e-17\n",
      "Shifting rewards by 0.6591007373269797\n",
      "new discounted_rewards mean: 0.6591007373269797\n",
      "avg loss: 0.360\n",
      "model action_tally: [ 11. 182.  45. 102.  60. 104.]\n",
      "train_tally:        [46. 14. 37. 66. 66. 67.]\n",
      "discounted_rewards mean: -5.639228061588096e-17\n",
      "Shifting rewards by 0.6591007373269797\n",
      "new discounted_rewards mean: 0.6591007373269797\n",
      "avg loss: 0.182\n",
      "model action_tally: [ 11. 182.  45. 102.  60. 104.]\n",
      "train_tally:        [46. 14. 37. 60. 60. 61.]\n",
      "\n",
      "62) frames played: 522, score: 110.0\n",
      "discounted_rewards mean: -2.7223859607666672e-17\n",
      "Shifting rewards by 0.8826597834417781\n",
      "new discounted_rewards mean: 0.8826597834417781\n",
      "avg loss: 0.938\n",
      "model action_tally: [ 41.  21.  80.  76.  83. 221.]\n",
      "train_tally:        [58. 57. 11. 56. 42. 59.]\n",
      "discounted_rewards mean: -2.7223859607666672e-17\n",
      "Shifting rewards by 0.8826597834417781\n",
      "new discounted_rewards mean: 0.8826597834417781\n",
      "avg loss: 0.329\n",
      "model action_tally: [ 41.  21.  80.  76.  83. 221.]\n",
      "train_tally:        [57. 58. 11. 57. 42. 59.]\n",
      "discounted_rewards mean: -2.7223859607666672e-17\n",
      "Shifting rewards by 0.8826597834417781\n",
      "new discounted_rewards mean: 0.8826597834417781\n",
      "avg loss: 0.239\n",
      "model action_tally: [ 41.  21.  80.  76.  83. 221.]\n",
      "train_tally:        [59. 58. 11. 57. 42. 59.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "63) frames played: 1275, score: 375.0\n",
      "discounted_rewards mean: 3.3437305212240006e-17\n",
      "Shifting rewards by 0.9380866958276117\n",
      "new discounted_rewards mean: 0.9380866958276116\n",
      "avg loss: 1.082\n",
      "model action_tally: [140.  31.  11. 251. 418. 424.]\n",
      "train_tally:        [ 32. 149. 147. 149. 150. 146.]\n",
      "discounted_rewards mean: 3.3437305212240006e-17\n",
      "Shifting rewards by 0.9380866958276117\n",
      "new discounted_rewards mean: 0.9380866958276116\n",
      "avg loss: 0.484\n",
      "model action_tally: [140.  31.  11. 251. 418. 424.]\n",
      "train_tally:        [ 32. 149. 150. 149. 150. 139.]\n",
      "discounted_rewards mean: 3.3437305212240006e-17\n",
      "Shifting rewards by 0.9380866958276117\n",
      "new discounted_rewards mean: 0.9380866958276116\n",
      "avg loss: 0.345\n",
      "model action_tally: [140.  31.  11. 251. 418. 424.]\n",
      "train_tally:        [ 32. 151. 149. 148. 151. 144.]\n",
      "\n",
      "64) frames played: 684, score: 140.0\n",
      "discounted_rewards mean: -9.349246523159212e-17\n",
      "Shifting rewards by 0.8460866998194763\n",
      "new discounted_rewards mean: 0.8460866998194762\n",
      "avg loss: 1.408\n",
      "model action_tally: [ 39. 130. 102. 134. 190.  89.]\n",
      "train_tally:        [74. 37. 67. 47. 73. 75.]\n",
      "discounted_rewards mean: -9.349246523159212e-17\n",
      "Shifting rewards by 0.8460866998194763\n",
      "new discounted_rewards mean: 0.8460866998194762\n",
      "avg loss: 0.739\n",
      "model action_tally: [ 39. 130. 102. 134. 190.  89.]\n",
      "train_tally:        [72. 38. 67. 47. 73. 74.]\n",
      "discounted_rewards mean: -9.349246523159212e-17\n",
      "Shifting rewards by 0.8460866998194763\n",
      "new discounted_rewards mean: 0.8460866998194762\n",
      "avg loss: 0.510\n",
      "model action_tally: [ 39. 130. 102. 134. 190.  89.]\n",
      "train_tally:        [70. 38. 66. 47. 71. 72.]\n",
      "\n",
      "65) frames played: 630, score: 120.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6614070196012749\n",
      "new discounted_rewards mean: 0.6614070196012749\n",
      "avg loss: 1.185\n",
      "model action_tally: [100.  81.  26.  44. 156. 223.]\n",
      "train_tally:        [68. 70. 53. 66. 26. 70.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6614070196012749\n",
      "new discounted_rewards mean: 0.6614070196012749\n",
      "avg loss: 0.549\n",
      "model action_tally: [100.  81.  26.  44. 156. 223.]\n",
      "train_tally:        [63. 66. 58. 67. 26. 66.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6614070196012749\n",
      "new discounted_rewards mean: 0.6614070196012749\n",
      "avg loss: 0.380\n",
      "model action_tally: [100.  81.  26.  44. 156. 223.]\n",
      "train_tally:        [70. 71. 58. 70. 26. 67.]\n",
      "\n",
      "66) frames played: 858, score: 290.0\n",
      "discounted_rewards mean: -9.109522253334617e-17\n",
      "Shifting rewards by 1.187953484982541\n",
      "new discounted_rewards mean: 1.187953484982541\n",
      "avg loss: 1.068\n",
      "model action_tally: [214. 280.  65. 110.  83. 106.]\n",
      "train_tally:        [41. 84. 73. 84. 81. 84.]\n",
      "discounted_rewards mean: -9.109522253334617e-17\n",
      "Shifting rewards by 1.187953484982541\n",
      "new discounted_rewards mean: 1.187953484982541\n",
      "avg loss: 0.413\n",
      "model action_tally: [214. 280.  65. 110.  83. 106.]\n",
      "train_tally:        [41. 88. 73. 86. 85. 88.]\n",
      "discounted_rewards mean: -9.109522253334617e-17\n",
      "Shifting rewards by 1.187953484982541\n",
      "new discounted_rewards mean: 1.187953484982541\n",
      "avg loss: 0.305\n",
      "model action_tally: [214. 280.  65. 110.  83. 106.]\n",
      "train_tally:        [41. 89. 73. 83. 83. 89.]\n",
      "\n",
      "67) frames played: 652, score: 135.0\n",
      "discounted_rewards mean: 4.359157888098774e-17\n",
      "Shifting rewards by 0.8833543106383075\n",
      "new discounted_rewards mean: 0.8833543106383076\n",
      "avg loss: 1.517\n",
      "model action_tally: [ 51. 130.  89. 120.  56. 206.]\n",
      "train_tally:        [31. 50. 46. 70. 46. 70.]\n",
      "discounted_rewards mean: 4.359157888098774e-17\n",
      "Shifting rewards by 0.8833543106383075\n",
      "new discounted_rewards mean: 0.8833543106383076\n",
      "avg loss: 0.682\n",
      "model action_tally: [ 51. 130.  89. 120.  56. 206.]\n",
      "train_tally:        [31. 50. 46. 67. 46. 68.]\n",
      "discounted_rewards mean: 4.359157888098774e-17\n",
      "Shifting rewards by 0.8833543106383075\n",
      "new discounted_rewards mean: 0.8833543106383076\n",
      "avg loss: 0.492\n",
      "model action_tally: [ 51. 130.  89. 120.  56. 206.]\n",
      "train_tally:        [31. 48. 46. 72. 46. 72.]\n",
      "\n",
      "68) frames played: 810, score: 415.0\n",
      "discounted_rewards mean: -3.50885301609926e-17\n",
      "Shifting rewards by 0.5644375377680575\n",
      "new discounted_rewards mean: 0.5644375377680575\n",
      "avg loss: 1.360\n",
      "model action_tally: [ 41.  49. 168. 257.  81. 214.]\n",
      "train_tally:        [30. 50. 47. 31. 51. 52.]\n",
      "discounted_rewards mean: -3.50885301609926e-17\n",
      "Shifting rewards by 0.5644375377680575\n",
      "new discounted_rewards mean: 0.5644375377680575\n",
      "avg loss: 0.711\n",
      "model action_tally: [ 41.  49. 168. 257.  81. 214.]\n",
      "train_tally:        [30. 51. 47. 31. 51. 51.]\n",
      "discounted_rewards mean: -3.50885301609926e-17\n",
      "Shifting rewards by 0.5644375377680575\n",
      "new discounted_rewards mean: 0.5644375377680575\n",
      "avg loss: 0.468\n",
      "model action_tally: [ 41.  49. 168. 257.  81. 214.]\n",
      "train_tally:        [29. 54. 48. 31. 56. 56.]\n",
      "\n",
      "69) frames played: 505, score: 155.0\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "Shifting rewards by 0.8624307187641862\n",
      "new discounted_rewards mean: 0.8624307187641862\n",
      "avg loss: 0.867\n",
      "model action_tally: [102.  63.  44.  53.  48. 195.]\n",
      "train_tally:        [39. 54. 13. 55. 55. 36.]\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "Shifting rewards by 0.8624307187641862\n",
      "new discounted_rewards mean: 0.8624307187641862\n",
      "avg loss: 0.420\n",
      "model action_tally: [102.  63.  44.  53.  48. 195.]\n",
      "train_tally:        [39. 53. 13. 54. 52. 36.]\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "Shifting rewards by 0.8624307187641862\n",
      "new discounted_rewards mean: 0.8624307187641862\n",
      "avg loss: 0.368\n",
      "model action_tally: [102.  63.  44.  53.  48. 195.]\n",
      "train_tally:        [40. 56. 13. 56. 54. 36.]\n",
      "\n",
      "70) frames played: 651, score: 140.0\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "Shifting rewards by 0.8495283079499425\n",
      "new discounted_rewards mean: 0.8495283079499424\n",
      "avg loss: 1.190\n",
      "model action_tally: [ 33.  93.  27. 312.  78. 108.]\n",
      "train_tally:        [63. 70. 70. 63. 40. 69.]\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "Shifting rewards by 0.8495283079499425\n",
      "new discounted_rewards mean: 0.8495283079499424\n",
      "avg loss: 0.554\n",
      "model action_tally: [ 33.  93.  27. 312.  78. 108.]\n",
      "train_tally:        [61. 65. 67. 65. 40. 67.]\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "Shifting rewards by 0.8495283079499425\n",
      "new discounted_rewards mean: 0.8495283079499424\n",
      "avg loss: 0.432\n",
      "model action_tally: [ 33.  93.  27. 312.  78. 108.]\n",
      "train_tally:        [64. 70. 70. 65. 40. 70.]\n",
      "\n",
      "71) frames played: 962, score: 470.0\n",
      "discounted_rewards mean: 2.9544396497301465e-17\n",
      "Shifting rewards by 0.5762568608262738\n",
      "new discounted_rewards mean: 0.5762568608262738\n",
      "avg loss: 1.364\n",
      "model action_tally: [107. 114. 162. 208. 154. 217.]\n",
      "train_tally:        [ 8. 32. 54. 44. 55. 56.]\n",
      "discounted_rewards mean: 2.9544396497301465e-17\n",
      "Shifting rewards by 0.5762568608262738\n",
      "new discounted_rewards mean: 0.5762568608262738\n",
      "avg loss: 0.798\n",
      "model action_tally: [107. 114. 162. 208. 154. 217.]\n",
      "train_tally:        [ 8. 32. 62. 44. 63. 57.]\n",
      "discounted_rewards mean: 2.9544396497301465e-17\n",
      "Shifting rewards by 0.5762568608262738\n",
      "new discounted_rewards mean: 0.5762568608262738\n",
      "avg loss: 0.610\n",
      "model action_tally: [107. 114. 162. 208. 154. 217.]\n",
      "train_tally:        [ 8. 32. 57. 46. 58. 58.]\n",
      "\n",
      "72) frames played: 730, score: 160.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.9325484816950635\n",
      "new discounted_rewards mean: 0.9325484816950634\n",
      "avg loss: 1.173\n",
      "model action_tally: [ 58.  36. 115. 224. 215.  82.]\n",
      "train_tally:        [56. 77. 20. 35. 22. 78.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.9325484816950635\n",
      "new discounted_rewards mean: 0.9325484816950634\n",
      "avg loss: 0.537\n",
      "model action_tally: [ 58.  36. 115. 224. 215.  82.]\n",
      "train_tally:        [56. 89. 20. 35. 22. 89.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.9325484816950635\n",
      "new discounted_rewards mean: 0.9325484816950634\n",
      "avg loss: 0.395\n",
      "model action_tally: [ 58.  36. 115. 224. 215.  82.]\n",
      "train_tally:        [56. 85. 20. 35. 22. 85.]\n",
      "\n",
      "73) frames played: 826, score: 210.0\n",
      "discounted_rewards mean: 8.602212297337774e-17\n",
      "Shifting rewards by 0.7785078601331759\n",
      "new discounted_rewards mean: 0.778507860133176\n",
      "avg loss: 1.493\n",
      "model action_tally: [112.  97.  44.  63.  92. 418.]\n",
      "train_tally:        [ 63.  80. 102.  92.  31. 103.]\n",
      "discounted_rewards mean: 8.602212297337774e-17\n",
      "Shifting rewards by 0.7785078601331759\n",
      "new discounted_rewards mean: 0.778507860133176\n",
      "avg loss: 0.815\n",
      "model action_tally: [112.  97.  44.  63.  92. 418.]\n",
      "train_tally:        [63. 80. 96. 89. 31. 97.]\n",
      "discounted_rewards mean: 8.602212297337774e-17\n",
      "Shifting rewards by 0.7785078601331759\n",
      "new discounted_rewards mean: 0.778507860133176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.568\n",
      "model action_tally: [112.  97.  44.  63.  92. 418.]\n",
      "train_tally:        [63. 79. 98. 95. 31. 98.]\n",
      "\n",
      "74) frames played: 605, score: 105.0\n",
      "discounted_rewards mean: 9.395606423274052e-17\n",
      "Shifting rewards by 0.5780300536650621\n",
      "new discounted_rewards mean: 0.5780300536650622\n",
      "avg loss: 1.425\n",
      "model action_tally: [ 68. 116. 161. 140.  23.  97.]\n",
      "train_tally:        [75. 77. 21. 41. 77. 78.]\n",
      "discounted_rewards mean: 9.395606423274052e-17\n",
      "Shifting rewards by 0.5780300536650621\n",
      "new discounted_rewards mean: 0.5780300536650622\n",
      "avg loss: 0.703\n",
      "model action_tally: [ 68. 116. 161. 140.  23.  97.]\n",
      "train_tally:        [73. 77. 21. 41. 77. 78.]\n",
      "discounted_rewards mean: 9.395606423274052e-17\n",
      "Shifting rewards by 0.5780300536650621\n",
      "new discounted_rewards mean: 0.5780300536650622\n",
      "avg loss: 0.492\n",
      "model action_tally: [ 68. 116. 161. 140.  23.  97.]\n",
      "train_tally:        [75. 82. 21. 40. 81. 82.]\n",
      "\n",
      "75) frames played: 823, score: 225.0\n",
      "discounted_rewards mean: -7.770212177206442e-17\n",
      "Shifting rewards by 0.8677186017159785\n",
      "new discounted_rewards mean: 0.8677186017159784\n",
      "avg loss: 0.964\n",
      "model action_tally: [105. 228.  43.  97.  90. 260.]\n",
      "train_tally:        [ 51.  43.  99. 110. 109. 110.]\n",
      "discounted_rewards mean: -7.770212177206442e-17\n",
      "Shifting rewards by 0.8677186017159785\n",
      "new discounted_rewards mean: 0.8677186017159784\n",
      "avg loss: 0.455\n",
      "model action_tally: [105. 228.  43.  97.  90. 260.]\n",
      "train_tally:        [ 51.  43.  96. 108. 110. 111.]\n",
      "discounted_rewards mean: -7.770212177206442e-17\n",
      "Shifting rewards by 0.8677186017159785\n",
      "new discounted_rewards mean: 0.8677186017159784\n",
      "avg loss: 0.328\n",
      "model action_tally: [105. 228.  43.  97.  90. 260.]\n",
      "train_tally:        [ 50.  42.  96. 109. 109. 109.]\n",
      "\n",
      "76) frames played: 435, score: 105.0\n",
      "discounted_rewards mean: -1.7967747341060004e-16\n",
      "Shifting rewards by 0.8559154190071675\n",
      "new discounted_rewards mean: 0.8559154190071674\n",
      "avg loss: 0.991\n",
      "model action_tally: [ 34.  20.  92.  88.  20. 181.]\n",
      "train_tally:        [43. 62.  8. 16. 60. 63.]\n",
      "discounted_rewards mean: -1.7967747341060004e-16\n",
      "Shifting rewards by 0.8559154190071675\n",
      "new discounted_rewards mean: 0.8559154190071674\n",
      "avg loss: 0.533\n",
      "model action_tally: [ 34.  20.  92.  88.  20. 181.]\n",
      "train_tally:        [44. 59.  8. 16. 55. 59.]\n",
      "discounted_rewards mean: -1.7967747341060004e-16\n",
      "Shifting rewards by 0.8559154190071675\n",
      "new discounted_rewards mean: 0.8559154190071674\n",
      "avg loss: 0.399\n",
      "model action_tally: [ 34.  20.  92.  88.  20. 181.]\n",
      "train_tally:        [43. 58.  8. 16. 58. 59.]\n",
      "\n",
      "77) frames played: 1270, score: 460.0\n",
      "discounted_rewards mean: 2.2379298764097643e-17\n",
      "Shifting rewards by 1.1404864920882367\n",
      "new discounted_rewards mean: 1.1404864920882365\n",
      "avg loss: 0.984\n",
      "model action_tally: [ 54. 229.  16.  31. 493. 447.]\n",
      "train_tally:        [129.  72. 124. 130. 122. 129.]\n",
      "discounted_rewards mean: 2.2379298764097643e-17\n",
      "Shifting rewards by 1.1404864920882367\n",
      "new discounted_rewards mean: 1.1404864920882365\n",
      "avg loss: 0.467\n",
      "model action_tally: [ 54. 229.  16.  31. 493. 447.]\n",
      "train_tally:        [127.  72. 121. 130. 129. 130.]\n",
      "discounted_rewards mean: 2.2379298764097643e-17\n",
      "Shifting rewards by 1.1404864920882367\n",
      "new discounted_rewards mean: 1.1404864920882365\n",
      "avg loss: 0.345\n",
      "model action_tally: [ 54. 229.  16.  31. 493. 447.]\n",
      "train_tally:        [131.  72. 120. 130. 131. 130.]\n",
      "\n",
      "78) frames played: 647, score: 130.0\n",
      "discounted_rewards mean: 1.0982113381145289e-17\n",
      "Shifting rewards by 0.8513015949697078\n",
      "new discounted_rewards mean: 0.8513015949697079\n",
      "avg loss: 1.151\n",
      "model action_tally: [123.  37. 126. 204. 101.  56.]\n",
      "train_tally:        [44. 60.  3. 56. 82. 83.]\n",
      "discounted_rewards mean: 1.0982113381145289e-17\n",
      "Shifting rewards by 0.8513015949697078\n",
      "new discounted_rewards mean: 0.8513015949697079\n",
      "avg loss: 0.521\n",
      "model action_tally: [123.  37. 126. 204. 101.  56.]\n",
      "train_tally:        [44. 62.  3. 56. 88. 88.]\n",
      "discounted_rewards mean: 1.0982113381145289e-17\n",
      "Shifting rewards by 0.8513015949697078\n",
      "new discounted_rewards mean: 0.8513015949697079\n",
      "avg loss: 0.377\n",
      "model action_tally: [123.  37. 126. 204. 101.  56.]\n",
      "train_tally:        [43. 59.  3. 55. 83. 84.]\n",
      "\n",
      "79) frames played: 409, score: 80.0\n",
      "discounted_rewards mean: 8.68634151295966e-17\n",
      "Shifting rewards by 0.8426800566611612\n",
      "new discounted_rewards mean: 0.8426800566611612\n",
      "avg loss: 1.246\n",
      "model action_tally: [ 21.  80.   0. 106.  55. 147.]\n",
      "train_tally:        [47. 24. 17. 48. 10. 47.]\n",
      "discounted_rewards mean: 8.68634151295966e-17\n",
      "Shifting rewards by 0.8426800566611612\n",
      "new discounted_rewards mean: 0.8426800566611612\n",
      "avg loss: 0.380\n",
      "model action_tally: [ 21.  80.   0. 106.  55. 147.]\n",
      "train_tally:        [47. 25. 17. 48. 10. 48.]\n",
      "discounted_rewards mean: 8.68634151295966e-17\n",
      "Shifting rewards by 0.8426800566611612\n",
      "new discounted_rewards mean: 0.8426800566611612\n",
      "avg loss: 0.253\n",
      "model action_tally: [ 21.  80.   0. 106.  55. 147.]\n",
      "train_tally:        [49. 25. 17. 50. 10. 49.]\n",
      "\n",
      "80) frames played: 494, score: 120.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7395935082695155\n",
      "new discounted_rewards mean: 0.7395935082695155\n",
      "avg loss: 1.198\n",
      "model action_tally: [166.  23.  85.  41. 136.  43.]\n",
      "train_tally:        [23. 58. 48. 19. 53. 58.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7395935082695155\n",
      "new discounted_rewards mean: 0.7395935082695155\n",
      "avg loss: 0.434\n",
      "model action_tally: [166.  23.  85.  41. 136.  43.]\n",
      "train_tally:        [23. 63. 51. 19. 53. 64.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7395935082695155\n",
      "new discounted_rewards mean: 0.7395935082695155\n",
      "avg loss: 0.256\n",
      "model action_tally: [166.  23.  85.  41. 136.  43.]\n",
      "train_tally:        [23. 56. 50. 19. 53. 56.]\n",
      "\n",
      "81) frames played: 1506, score: 545.0\n",
      "discounted_rewards mean: 7.548926807544225e-17\n",
      "Shifting rewards by 1.0491434642178674\n",
      "new discounted_rewards mean: 1.0491434642178674\n",
      "avg loss: 1.270\n",
      "model action_tally: [ 21. 545.  66.  71. 430. 373.]\n",
      "train_tally:        [ 86. 140. 158.  65. 159. 159.]\n",
      "discounted_rewards mean: 7.548926807544225e-17\n",
      "Shifting rewards by 1.0491434642178674\n",
      "new discounted_rewards mean: 1.0491434642178674\n",
      "avg loss: 0.674\n",
      "model action_tally: [ 21. 545.  66.  71. 430. 373.]\n",
      "train_tally:        [ 86. 140. 155.  65. 156. 156.]\n",
      "discounted_rewards mean: 7.548926807544225e-17\n",
      "Shifting rewards by 1.0491434642178674\n",
      "new discounted_rewards mean: 1.0491434642178674\n",
      "avg loss: 0.456\n",
      "model action_tally: [ 21. 545.  66.  71. 430. 373.]\n",
      "train_tally:        [ 86. 140. 166.  65. 167. 166.]\n",
      "\n",
      "82) frames played: 880, score: 460.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6001188931637679\n",
      "new discounted_rewards mean: 0.6001188931637679\n",
      "avg loss: 0.974\n",
      "model action_tally: [ 96. 129. 123.  56. 280. 196.]\n",
      "train_tally:        [60. 49. 61. 60. 53. 61.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6001188931637679\n",
      "new discounted_rewards mean: 0.6001188931637679\n",
      "avg loss: 0.365\n",
      "model action_tally: [ 96. 129. 123.  56. 280. 196.]\n",
      "train_tally:        [62. 49. 64. 63. 53. 64.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6001188931637679\n",
      "new discounted_rewards mean: 0.6001188931637679\n",
      "avg loss: 0.271\n",
      "model action_tally: [ 96. 129. 123.  56. 280. 196.]\n",
      "train_tally:        [65. 49. 64. 65. 53. 64.]\n",
      "\n",
      "83) frames played: 499, score: 105.0\n",
      "discounted_rewards mean: 1.1391466705572748e-16\n",
      "Shifting rewards by 0.7335970684059591\n",
      "new discounted_rewards mean: 0.733597068405959\n",
      "avg loss: 0.675\n",
      "model action_tally: [121.  85.  53.  23. 152.  65.]\n",
      "train_tally:        [24. 18. 71. 66. 71. 71.]\n",
      "discounted_rewards mean: 1.1391466705572748e-16\n",
      "Shifting rewards by 0.7335970684059591\n",
      "new discounted_rewards mean: 0.733597068405959\n",
      "avg loss: 0.220\n",
      "model action_tally: [121.  85.  53.  23. 152.  65.]\n",
      "train_tally:        [23. 18. 68. 68. 67. 68.]\n",
      "discounted_rewards mean: 1.1391466705572748e-16\n",
      "Shifting rewards by 0.7335970684059591\n",
      "new discounted_rewards mean: 0.733597068405959\n",
      "avg loss: 0.188\n",
      "model action_tally: [121.  85.  53.  23. 152.  65.]\n",
      "train_tally:        [24. 18. 67. 65. 66. 67.]\n",
      "\n",
      "84) frames played: 494, score: 105.0\n",
      "discounted_rewards mean: -1.150676495158057e-16\n",
      "Shifting rewards by 0.6357052480918024\n",
      "new discounted_rewards mean: 0.6357052480918025\n",
      "avg loss: 1.214\n",
      "model action_tally: [ 20.  59. 102. 159.  98.  56.]\n",
      "train_tally:        [55. 47. 53. 41. 56. 57.]\n",
      "discounted_rewards mean: -1.150676495158057e-16\n",
      "Shifting rewards by 0.6357052480918024\n",
      "new discounted_rewards mean: 0.6357052480918025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.567\n",
      "model action_tally: [ 20.  59. 102. 159.  98.  56.]\n",
      "train_tally:        [55. 45. 52. 41. 56. 56.]\n",
      "discounted_rewards mean: -1.150676495158057e-16\n",
      "Shifting rewards by 0.6357052480918024\n",
      "new discounted_rewards mean: 0.6357052480918025\n",
      "avg loss: 0.446\n",
      "model action_tally: [ 20.  59. 102. 159.  98.  56.]\n",
      "train_tally:        [57. 47. 52. 41. 57. 58.]\n",
      "\n",
      "85) frames played: 643, score: 115.0\n",
      "discounted_rewards mean: -5.525215674650857e-17\n",
      "Shifting rewards by 0.7673262164584065\n",
      "new discounted_rewards mean: 0.7673262164584066\n",
      "avg loss: 1.381\n",
      "model action_tally: [155.  41. 111.  91. 130. 115.]\n",
      "train_tally:        [71. 59. 69. 65. 26. 72.]\n",
      "discounted_rewards mean: -5.525215674650857e-17\n",
      "Shifting rewards by 0.7673262164584065\n",
      "new discounted_rewards mean: 0.7673262164584066\n",
      "avg loss: 0.802\n",
      "model action_tally: [155.  41. 111.  91. 130. 115.]\n",
      "train_tally:        [68. 59. 66. 66. 26. 69.]\n",
      "discounted_rewards mean: -5.525215674650857e-17\n",
      "Shifting rewards by 0.7673262164584065\n",
      "new discounted_rewards mean: 0.7673262164584066\n",
      "avg loss: 0.649\n",
      "model action_tally: [155.  41. 111.  91. 130. 115.]\n",
      "train_tally:        [74. 59. 69. 63. 26. 74.]\n",
      "\n",
      "86) frames played: 519, score: 110.0\n",
      "discounted_rewards mean: -5.4762445916000014e-17\n",
      "Shifting rewards by 0.7618265522768273\n",
      "new discounted_rewards mean: 0.7618265522768272\n",
      "avg loss: 1.018\n",
      "model action_tally: [ 84.  32. 164.  62.  84.  93.]\n",
      "train_tally:        [63. 60. 27. 62. 64. 53.]\n",
      "discounted_rewards mean: -5.4762445916000014e-17\n",
      "Shifting rewards by 0.7618265522768273\n",
      "new discounted_rewards mean: 0.7618265522768272\n",
      "avg loss: 0.394\n",
      "model action_tally: [ 84.  32. 164.  62.  84.  93.]\n",
      "train_tally:        [55. 58. 27. 60. 60. 54.]\n",
      "discounted_rewards mean: -5.4762445916000014e-17\n",
      "Shifting rewards by 0.7618265522768273\n",
      "new discounted_rewards mean: 0.7618265522768272\n",
      "avg loss: 0.247\n",
      "model action_tally: [ 84.  32. 164.  62.  84.  93.]\n",
      "train_tally:        [58. 63. 27. 63. 63. 54.]\n",
      "\n",
      "87) frames played: 511, score: 105.0\n",
      "discounted_rewards mean: -5.561978362114287e-17\n",
      "Shifting rewards by 0.724105955347274\n",
      "new discounted_rewards mean: 0.7241059553472741\n",
      "avg loss: 1.186\n",
      "model action_tally: [ 93.  42.  52.  96. 167.  61.]\n",
      "train_tally:        [64. 48. 15. 56. 61. 65.]\n",
      "discounted_rewards mean: -5.561978362114287e-17\n",
      "Shifting rewards by 0.724105955347274\n",
      "new discounted_rewards mean: 0.7241059553472741\n",
      "avg loss: 0.557\n",
      "model action_tally: [ 93.  42.  52.  96. 167.  61.]\n",
      "train_tally:        [57. 48. 15. 57. 57. 57.]\n",
      "discounted_rewards mean: -5.561978362114287e-17\n",
      "Shifting rewards by 0.724105955347274\n",
      "new discounted_rewards mean: 0.7241059553472741\n",
      "avg loss: 0.448\n",
      "model action_tally: [ 93.  42.  52.  96. 167.  61.]\n",
      "train_tally:        [59. 48. 15. 58. 61. 61.]\n",
      "\n",
      "88) frames played: 649, score: 150.0\n",
      "discounted_rewards mean: -2.1896540393223427e-17\n",
      "Shifting rewards by 0.8621258407736655\n",
      "new discounted_rewards mean: 0.8621258407736654\n",
      "avg loss: 1.263\n",
      "model action_tally: [ 69.  63.  26. 163. 170. 158.]\n",
      "train_tally:        [63. 63. 47. 59. 60. 59.]\n",
      "discounted_rewards mean: -2.1896540393223427e-17\n",
      "Shifting rewards by 0.8621258407736655\n",
      "new discounted_rewards mean: 0.8621258407736654\n",
      "avg loss: 0.606\n",
      "model action_tally: [ 69.  63.  26. 163. 170. 158.]\n",
      "train_tally:        [61. 58. 49. 58. 59. 62.]\n",
      "discounted_rewards mean: -2.1896540393223427e-17\n",
      "Shifting rewards by 0.8621258407736655\n",
      "new discounted_rewards mean: 0.8621258407736654\n",
      "avg loss: 0.444\n",
      "model action_tally: [ 69.  63.  26. 163. 170. 158.]\n",
      "train_tally:        [62. 59. 49. 62. 60. 56.]\n",
      "\n",
      "89) frames played: 1229, score: 400.0\n",
      "discounted_rewards mean: -9.250352947242964e-17\n",
      "Shifting rewards by 0.960998166369839\n",
      "new discounted_rewards mean: 0.9609981663698389\n",
      "avg loss: 1.361\n",
      "model action_tally: [273. 133. 192. 210. 149. 272.]\n",
      "train_tally:        [121. 126.  48. 101. 127. 125.]\n",
      "discounted_rewards mean: -9.250352947242964e-17\n",
      "Shifting rewards by 0.960998166369839\n",
      "new discounted_rewards mean: 0.9609981663698389\n",
      "avg loss: 0.824\n",
      "model action_tally: [273. 133. 192. 210. 149. 272.]\n",
      "train_tally:        [121. 129.  48. 102. 130. 126.]\n",
      "discounted_rewards mean: -9.250352947242964e-17\n",
      "Shifting rewards by 0.960998166369839\n",
      "new discounted_rewards mean: 0.9609981663698389\n",
      "avg loss: 0.608\n",
      "model action_tally: [273. 133. 192. 210. 149. 272.]\n",
      "train_tally:        [117. 125.  48. 102. 125. 123.]\n",
      "\n",
      "90) frames played: 446, score: 110.0\n",
      "discounted_rewards mean: 1.2745161179553366e-16\n",
      "Shifting rewards by 0.8560163899413484\n",
      "new discounted_rewards mean: 0.8560163899413485\n",
      "avg loss: 0.987\n",
      "model action_tally: [134.  20.   9.  53. 183.  47.]\n",
      "train_tally:        [35. 10. 18. 55. 49. 55.]\n",
      "discounted_rewards mean: 1.2745161179553366e-16\n",
      "Shifting rewards by 0.8560163899413484\n",
      "new discounted_rewards mean: 0.8560163899413485\n",
      "avg loss: 0.474\n",
      "model action_tally: [134.  20.   9.  53. 183.  47.]\n",
      "train_tally:        [35. 10. 18. 56. 51. 57.]\n",
      "discounted_rewards mean: 1.2745161179553366e-16\n",
      "Shifting rewards by 0.8560163899413484\n",
      "new discounted_rewards mean: 0.8560163899413485\n",
      "avg loss: 0.307\n",
      "model action_tally: [134.  20.   9.  53. 183.  47.]\n",
      "train_tally:        [35. 10. 18. 53. 50. 53.]\n",
      "\n",
      "91) frames played: 412, score: 80.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7752473184917176\n",
      "new discounted_rewards mean: 0.7752473184917176\n",
      "avg loss: 1.056\n",
      "model action_tally: [ 91.  10.  76.  48. 114.  73.]\n",
      "train_tally:        [44. 46. 13. 20. 47. 36.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7752473184917176\n",
      "new discounted_rewards mean: 0.7752473184917176\n",
      "avg loss: 0.502\n",
      "model action_tally: [ 91.  10.  76.  48. 114.  73.]\n",
      "train_tally:        [47. 48. 13. 20. 48. 36.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7752473184917176\n",
      "new discounted_rewards mean: 0.7752473184917176\n",
      "avg loss: 0.508\n",
      "model action_tally: [ 91.  10.  76.  48. 114.  73.]\n",
      "train_tally:        [43. 45. 13. 20. 45. 36.]\n",
      "\n",
      "92) frames played: 722, score: 140.0\n",
      "discounted_rewards mean: 7.873049703713021e-17\n",
      "Shifting rewards by 0.7525502760920788\n",
      "new discounted_rewards mean: 0.7525502760920788\n",
      "avg loss: 1.223\n",
      "model action_tally: [176. 103.  19.  30. 224. 170.]\n",
      "train_tally:        [46. 73. 33. 93. 93. 93.]\n",
      "discounted_rewards mean: 7.873049703713021e-17\n",
      "Shifting rewards by 0.7525502760920788\n",
      "new discounted_rewards mean: 0.7525502760920788\n",
      "avg loss: 0.656\n",
      "model action_tally: [176. 103.  19.  30. 224. 170.]\n",
      "train_tally:        [45. 69. 33. 89. 91. 91.]\n",
      "discounted_rewards mean: 7.873049703713021e-17\n",
      "Shifting rewards by 0.7525502760920788\n",
      "new discounted_rewards mean: 0.7525502760920788\n",
      "avg loss: 0.504\n",
      "model action_tally: [176. 103.  19.  30. 224. 170.]\n",
      "train_tally:        [46. 74. 33. 87. 86. 87.]\n",
      "\n",
      "93) frames played: 497, score: 105.0\n",
      "discounted_rewards mean: 5.718653808934408e-17\n",
      "Shifting rewards by 0.7382335485819415\n",
      "new discounted_rewards mean: 0.7382335485819416\n",
      "avg loss: 1.036\n",
      "model action_tally: [ 31.  53.  18. 176. 122.  97.]\n",
      "train_tally:        [58. 60. 61. 22. 60. 44.]\n",
      "discounted_rewards mean: 5.718653808934408e-17\n",
      "Shifting rewards by 0.7382335485819415\n",
      "new discounted_rewards mean: 0.7382335485819416\n",
      "avg loss: 0.450\n",
      "model action_tally: [ 31.  53.  18. 176. 122.  97.]\n",
      "train_tally:        [64. 66. 66. 22. 66. 44.]\n",
      "discounted_rewards mean: 5.718653808934408e-17\n",
      "Shifting rewards by 0.7382335485819415\n",
      "new discounted_rewards mean: 0.7382335485819416\n",
      "avg loss: 0.336\n",
      "model action_tally: [ 31.  53.  18. 176. 122.  97.]\n",
      "train_tally:        [60. 60. 60. 22. 60. 44.]\n",
      "\n",
      "94) frames played: 677, score: 170.0\n",
      "discounted_rewards mean: 6.297276830961006e-17\n",
      "Shifting rewards by 0.8653576896120924\n",
      "new discounted_rewards mean: 0.8653576896120925\n",
      "avg loss: 1.177\n",
      "model action_tally: [117.  84. 166.  22. 127. 161.]\n",
      "train_tally:        [77. 42. 28. 78. 77. 70.]\n",
      "discounted_rewards mean: 6.297276830961006e-17\n",
      "Shifting rewards by 0.8653576896120924\n",
      "new discounted_rewards mean: 0.8653576896120925\n",
      "avg loss: 0.650\n",
      "model action_tally: [117.  84. 166.  22. 127. 161.]\n",
      "train_tally:        [73. 42. 28. 74. 71. 70.]\n",
      "discounted_rewards mean: 6.297276830961006e-17\n",
      "Shifting rewards by 0.8653576896120924\n",
      "new discounted_rewards mean: 0.8653576896120925\n",
      "avg loss: 0.504\n",
      "model action_tally: [117.  84. 166.  22. 127. 161.]\n",
      "train_tally:        [72. 42. 28. 74. 74. 70.]\n",
      "\n",
      "95) frames played: 1099, score: 350.0\n",
      "discounted_rewards mean: -1.2930714026571432e-17\n",
      "Shifting rewards by 1.0796403207409146\n",
      "new discounted_rewards mean: 1.0796403207409146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.258\n",
      "model action_tally: [142. 145.  51. 208. 297. 256.]\n",
      "train_tally:        [ 41. 137.  99.  48. 139. 139.]\n",
      "discounted_rewards mean: -1.2930714026571432e-17\n",
      "Shifting rewards by 1.0796403207409146\n",
      "new discounted_rewards mean: 1.0796403207409146\n",
      "avg loss: 0.707\n",
      "model action_tally: [142. 145.  51. 208. 297. 256.]\n",
      "train_tally:        [ 41. 144.  99.  48. 146. 146.]\n",
      "discounted_rewards mean: -1.2930714026571432e-17\n",
      "Shifting rewards by 1.0796403207409146\n",
      "new discounted_rewards mean: 1.0796403207409146\n",
      "avg loss: 0.499\n",
      "model action_tally: [142. 145.  51. 208. 297. 256.]\n",
      "train_tally:        [ 41. 137.  99.  48. 138. 137.]\n",
      "\n",
      "96) frames played: 531, score: 160.0\n",
      "discounted_rewards mean: 1.0704975303353675e-16\n",
      "Shifting rewards by 0.8518270777275052\n",
      "new discounted_rewards mean: 0.8518270777275053\n",
      "avg loss: 1.100\n",
      "model action_tally: [ 98. 111. 110.  32.  80. 100.]\n",
      "train_tally:        [56. 57. 27. 37. 51. 58.]\n",
      "discounted_rewards mean: 1.0704975303353675e-16\n",
      "Shifting rewards by 0.8518270777275052\n",
      "new discounted_rewards mean: 0.8518270777275053\n",
      "avg loss: 0.571\n",
      "model action_tally: [ 98. 111. 110.  32.  80. 100.]\n",
      "train_tally:        [56. 59. 27. 37. 51. 60.]\n",
      "discounted_rewards mean: 1.0704975303353675e-16\n",
      "Shifting rewards by 0.8518270777275052\n",
      "new discounted_rewards mean: 0.8518270777275053\n",
      "avg loss: 0.417\n",
      "model action_tally: [ 98. 111. 110.  32.  80. 100.]\n",
      "train_tally:        [52. 48. 27. 37. 51. 52.]\n",
      "\n",
      "97) frames played: 597, score: 150.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8760087949866713\n",
      "new discounted_rewards mean: 0.8760087949866712\n",
      "avg loss: 1.038\n",
      "model action_tally: [ 65.  97. 134.  72. 116. 113.]\n",
      "train_tally:        [49. 51. 61. 51. 60. 61.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8760087949866713\n",
      "new discounted_rewards mean: 0.8760087949866712\n",
      "avg loss: 0.484\n",
      "model action_tally: [ 65.  97. 134.  72. 116. 113.]\n",
      "train_tally:        [51. 51. 63. 49. 62. 63.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8760087949866713\n",
      "new discounted_rewards mean: 0.8760087949866712\n",
      "avg loss: 0.314\n",
      "model action_tally: [ 65.  97. 134.  72. 116. 113.]\n",
      "train_tally:        [51. 51. 63. 45. 62. 64.]\n",
      "\n",
      "98) frames played: 819, score: 225.0\n",
      "discounted_rewards mean: 1.5616323862859346e-16\n",
      "Shifting rewards by 0.9316673991240239\n",
      "new discounted_rewards mean: 0.9316673991240241\n",
      "avg loss: 0.795\n",
      "model action_tally: [ 67.  71. 184. 193.  86. 218.]\n",
      "train_tally:        [ 58. 101.  42.  97. 103. 103.]\n",
      "discounted_rewards mean: 1.5616323862859346e-16\n",
      "Shifting rewards by 0.9316673991240239\n",
      "new discounted_rewards mean: 0.9316673991240241\n",
      "avg loss: 0.323\n",
      "model action_tally: [ 67.  71. 184. 193.  86. 218.]\n",
      "train_tally:        [58. 97. 42. 95. 98. 97.]\n",
      "discounted_rewards mean: 1.5616323862859346e-16\n",
      "Shifting rewards by 0.9316673991240239\n",
      "new discounted_rewards mean: 0.9316673991240241\n",
      "avg loss: 0.281\n",
      "model action_tally: [ 67.  71. 184. 193.  86. 218.]\n",
      "train_tally:        [ 56. 101.  42. 101. 102. 102.]\n",
      "\n",
      "99) frames played: 880, score: 270.0\n",
      "discounted_rewards mean: -1.291895883200182e-16\n",
      "Shifting rewards by 0.9616560802527796\n",
      "new discounted_rewards mean: 0.9616560802527795\n",
      "avg loss: 1.369\n",
      "model action_tally: [225.  72. 182.  84. 120. 197.]\n",
      "train_tally:        [60. 39. 65. 67. 52. 68.]\n",
      "discounted_rewards mean: -1.291895883200182e-16\n",
      "Shifting rewards by 0.9616560802527796\n",
      "new discounted_rewards mean: 0.9616560802527795\n",
      "avg loss: 0.692\n",
      "model action_tally: [225.  72. 182.  84. 120. 197.]\n",
      "train_tally:        [63. 37. 66. 69. 52. 70.]\n",
      "discounted_rewards mean: -1.291895883200182e-16\n",
      "Shifting rewards by 0.9616560802527796\n",
      "new discounted_rewards mean: 0.9616560802527795\n",
      "avg loss: 0.501\n",
      "model action_tally: [225.  72. 182.  84. 120. 197.]\n",
      "train_tally:        [58. 39. 67. 67. 52. 67.]\n",
      "\n",
      "100) frames played: 692, score: 120.0\n",
      "discounted_rewards mean: 6.160775165550001e-17\n",
      "Shifting rewards by 0.7622972070906724\n",
      "new discounted_rewards mean: 0.7622972070906724\n",
      "avg loss: 1.536\n",
      "model action_tally: [145.  46. 100. 195.  97. 109.]\n",
      "train_tally:        [77. 44. 27. 54. 72. 77.]\n",
      "discounted_rewards mean: 6.160775165550001e-17\n",
      "Shifting rewards by 0.7622972070906724\n",
      "new discounted_rewards mean: 0.7622972070906724\n",
      "avg loss: 0.796\n",
      "model action_tally: [145.  46. 100. 195.  97. 109.]\n",
      "train_tally:        [80. 44. 27. 54. 79. 80.]\n",
      "discounted_rewards mean: 6.160775165550001e-17\n",
      "Shifting rewards by 0.7622972070906724\n",
      "new discounted_rewards mean: 0.7622972070906724\n",
      "avg loss: 0.552\n",
      "model action_tally: [145.  46. 100. 195.  97. 109.]\n",
      "train_tally:        [79. 43. 27. 54. 76. 80.]\n",
      "\n",
      "101) frames played: 466, score: 105.0\n",
      "discounted_rewards mean: -1.2198158553821462e-16\n",
      "Shifting rewards by 0.7658534485717426\n",
      "new discounted_rewards mean: 0.7658534485717424\n",
      "avg loss: 1.038\n",
      "model action_tally: [197.  11.  19.  43.  80. 116.]\n",
      "train_tally:        [42. 49. 54. 53. 54. 48.]\n",
      "discounted_rewards mean: -1.2198158553821462e-16\n",
      "Shifting rewards by 0.7658534485717426\n",
      "new discounted_rewards mean: 0.7658534485717424\n",
      "avg loss: 0.573\n",
      "model action_tally: [197.  11.  19.  43.  80. 116.]\n",
      "train_tally:        [41. 47. 52. 51. 52. 48.]\n",
      "discounted_rewards mean: -1.2198158553821462e-16\n",
      "Shifting rewards by 0.7658534485717426\n",
      "new discounted_rewards mean: 0.7658534485717424\n",
      "avg loss: 0.499\n",
      "model action_tally: [197.  11.  19.  43.  80. 116.]\n",
      "train_tally:        [42. 46. 54. 54. 52. 48.]\n",
      "\n",
      "102) frames played: 678, score: 170.0\n",
      "discounted_rewards mean: -6.287988812036285e-17\n",
      "Shifting rewards by 0.8479767200499781\n",
      "new discounted_rewards mean: 0.8479767200499781\n",
      "avg loss: 1.167\n",
      "model action_tally: [ 37.  91. 109. 183. 162.  96.]\n",
      "train_tally:        [79. 80. 50. 23. 78. 70.]\n",
      "discounted_rewards mean: -6.287988812036285e-17\n",
      "Shifting rewards by 0.8479767200499781\n",
      "new discounted_rewards mean: 0.8479767200499781\n",
      "avg loss: 0.522\n",
      "model action_tally: [ 37.  91. 109. 183. 162.  96.]\n",
      "train_tally:        [80. 80. 51. 23. 77. 71.]\n",
      "discounted_rewards mean: -6.287988812036285e-17\n",
      "Shifting rewards by 0.8479767200499781\n",
      "new discounted_rewards mean: 0.8479767200499781\n",
      "avg loss: 0.431\n",
      "model action_tally: [ 37.  91. 109. 183. 162.  96.]\n",
      "train_tally:        [71. 71. 51. 23. 70. 70.]\n",
      "\n",
      "103) frames played: 539, score: 120.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7998024744125574\n",
      "new discounted_rewards mean: 0.7998024744125575\n",
      "avg loss: 1.176\n",
      "model action_tally: [130. 148.  88.  32.  74.  67.]\n",
      "train_tally:        [64. 20. 11. 64. 65. 45.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7998024744125574\n",
      "new discounted_rewards mean: 0.7998024744125575\n",
      "avg loss: 0.503\n",
      "model action_tally: [130. 148.  88.  32.  74.  67.]\n",
      "train_tally:        [61. 20. 11. 63. 63. 45.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7998024744125574\n",
      "new discounted_rewards mean: 0.7998024744125575\n",
      "avg loss: 0.339\n",
      "model action_tally: [130. 148.  88.  32.  74.  67.]\n",
      "train_tally:        [57. 20. 11. 67. 68. 45.]\n",
      "\n",
      "104) frames played: 567, score: 105.0\n",
      "discounted_rewards mean: -5.0126471658560857e-17\n",
      "Shifting rewards by 0.6802714449263946\n",
      "new discounted_rewards mean: 0.6802714449263946\n",
      "avg loss: 1.308\n",
      "model action_tally: [128.  78.  11. 145. 154.  51.]\n",
      "train_tally:        [61. 51. 68. 41. 65. 68.]\n",
      "discounted_rewards mean: -5.0126471658560857e-17\n",
      "Shifting rewards by 0.6802714449263946\n",
      "new discounted_rewards mean: 0.6802714449263946\n",
      "avg loss: 0.677\n",
      "model action_tally: [128.  78.  11. 145. 154.  51.]\n",
      "train_tally:        [65. 50. 65. 41. 66. 66.]\n",
      "discounted_rewards mean: -5.0126471658560857e-17\n",
      "Shifting rewards by 0.6802714449263946\n",
      "new discounted_rewards mean: 0.6802714449263946\n",
      "avg loss: 0.499\n",
      "model action_tally: [128.  78.  11. 145. 154.  51.]\n",
      "train_tally:        [73. 51. 72. 41. 68. 73.]\n",
      "\n",
      "105) frames played: 628, score: 140.0\n",
      "discounted_rewards mean: 1.3577249727900003e-16\n",
      "Shifting rewards by 0.8415198741807504\n",
      "new discounted_rewards mean: 0.8415198741807505\n",
      "avg loss: 1.229\n",
      "model action_tally: [ 66. 108. 105.  78. 175.  96.]\n",
      "train_tally:        [59. 36. 56. 56. 60. 55.]\n",
      "discounted_rewards mean: 1.3577249727900003e-16\n",
      "Shifting rewards by 0.8415198741807504\n",
      "new discounted_rewards mean: 0.8415198741807505\n",
      "avg loss: 0.730\n",
      "model action_tally: [ 66. 108. 105.  78. 175.  96.]\n",
      "train_tally:        [54. 36. 58. 57. 59. 56.]\n",
      "discounted_rewards mean: 1.3577249727900003e-16\n",
      "Shifting rewards by 0.8415198741807504\n",
      "new discounted_rewards mean: 0.8415198741807505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.550\n",
      "model action_tally: [ 66. 108. 105.  78. 175.  96.]\n",
      "train_tally:        [52. 36. 60. 57. 60. 60.]\n",
      "\n",
      "106) frames played: 1529, score: 915.0\n",
      "discounted_rewards mean: 4.647107493525835e-17\n",
      "Shifting rewards by 0.698867867125117\n",
      "new discounted_rewards mean: 0.6988678671251168\n",
      "avg loss: 1.398\n",
      "model action_tally: [ 68. 389. 329. 261. 341. 141.]\n",
      "train_tally:        [114. 115.  54.  53. 114.  90.]\n",
      "discounted_rewards mean: 4.647107493525835e-17\n",
      "Shifting rewards by 0.698867867125117\n",
      "new discounted_rewards mean: 0.6988678671251168\n",
      "avg loss: 0.869\n",
      "model action_tally: [ 68. 389. 329. 261. 341. 141.]\n",
      "train_tally:        [113. 115.  54.  53. 115.  93.]\n",
      "discounted_rewards mean: 4.647107493525835e-17\n",
      "Shifting rewards by 0.698867867125117\n",
      "new discounted_rewards mean: 0.6988678671251168\n",
      "avg loss: 0.727\n",
      "model action_tally: [ 68. 389. 329. 261. 341. 141.]\n",
      "train_tally:        [102. 103.  54.  53. 102.  94.]\n",
      "\n",
      "107) frames played: 590, score: 120.0\n",
      "discounted_rewards mean: 4.8172388865091537e-17\n",
      "Shifting rewards by 0.7604536732085443\n",
      "new discounted_rewards mean: 0.7604536732085443\n",
      "avg loss: 1.300\n",
      "model action_tally: [ 66.  87. 198.  84. 108.  47.]\n",
      "train_tally:        [65. 16. 35. 66. 66. 55.]\n",
      "discounted_rewards mean: 4.8172388865091537e-17\n",
      "Shifting rewards by 0.7604536732085443\n",
      "new discounted_rewards mean: 0.7604536732085443\n",
      "avg loss: 0.783\n",
      "model action_tally: [ 66.  87. 198.  84. 108.  47.]\n",
      "train_tally:        [68. 16. 35. 67. 69. 55.]\n",
      "discounted_rewards mean: 4.8172388865091537e-17\n",
      "Shifting rewards by 0.7604536732085443\n",
      "new discounted_rewards mean: 0.7604536732085443\n",
      "avg loss: 0.541\n",
      "model action_tally: [ 66.  87. 198.  84. 108.  47.]\n",
      "train_tally:        [71. 16. 35. 67. 71. 54.]\n",
      "\n",
      "108) frames played: 1792, score: 690.0\n",
      "discounted_rewards mean: 7.930164461608261e-17\n",
      "Shifting rewards by 0.5950729770433433\n",
      "new discounted_rewards mean: 0.5950729770433433\n",
      "avg loss: 1.258\n",
      "model action_tally: [938.  55.  66. 107. 329. 297.]\n",
      "train_tally:        [115. 136. 136. 137.  82. 107.]\n",
      "discounted_rewards mean: 7.930164461608261e-17\n",
      "Shifting rewards by 0.5950729770433433\n",
      "new discounted_rewards mean: 0.5950729770433433\n",
      "avg loss: 0.718\n",
      "model action_tally: [938.  55.  66. 107. 329. 297.]\n",
      "train_tally:        [117. 134. 134. 134.  82. 107.]\n",
      "discounted_rewards mean: 7.930164461608261e-17\n",
      "Shifting rewards by 0.5950729770433433\n",
      "new discounted_rewards mean: 0.5950729770433433\n",
      "avg loss: 0.543\n",
      "model action_tally: [938.  55.  66. 107. 329. 297.]\n",
      "train_tally:        [116. 142. 141. 142.  82. 107.]\n",
      "\n",
      "109) frames played: 606, score: 110.0\n",
      "discounted_rewards mean: -4.6900510611227736e-17\n",
      "Shifting rewards by 0.6624253807479453\n",
      "new discounted_rewards mean: 0.6624253807479452\n",
      "avg loss: 1.246\n",
      "model action_tally: [198.  17. 102. 109.  76. 104.]\n",
      "train_tally:        [71. 72. 30. 69. 66. 44.]\n",
      "discounted_rewards mean: -4.6900510611227736e-17\n",
      "Shifting rewards by 0.6624253807479453\n",
      "new discounted_rewards mean: 0.6624253807479452\n",
      "avg loss: 0.620\n",
      "model action_tally: [198.  17. 102. 109.  76. 104.]\n",
      "train_tally:        [71. 72. 30. 71. 71. 44.]\n",
      "discounted_rewards mean: -4.6900510611227736e-17\n",
      "Shifting rewards by 0.6624253807479453\n",
      "new discounted_rewards mean: 0.6624253807479452\n",
      "avg loss: 0.466\n",
      "model action_tally: [198.  17. 102. 109.  76. 104.]\n",
      "train_tally:        [73. 73. 30. 71. 65. 44.]\n",
      "\n",
      "110) frames played: 426, score: 135.0\n",
      "discounted_rewards mean: 3.3358813885450714e-17\n",
      "Shifting rewards by 0.9587725113346758\n",
      "new discounted_rewards mean: 0.9587725113346757\n",
      "avg loss: 1.046\n",
      "model action_tally: [ 55. 139.   6.  82. 112.  32.]\n",
      "train_tally:        [45. 33. 52.  9. 24. 53.]\n",
      "discounted_rewards mean: 3.3358813885450714e-17\n",
      "Shifting rewards by 0.9587725113346758\n",
      "new discounted_rewards mean: 0.9587725113346757\n",
      "avg loss: 0.452\n",
      "model action_tally: [ 55. 139.   6.  82. 112.  32.]\n",
      "train_tally:        [48. 33. 49.  9. 24. 49.]\n",
      "discounted_rewards mean: 3.3358813885450714e-17\n",
      "Shifting rewards by 0.9587725113346758\n",
      "new discounted_rewards mean: 0.9587725113346757\n",
      "avg loss: 0.321\n",
      "model action_tally: [ 55. 139.   6.  82. 112.  32.]\n",
      "train_tally:        [45. 33. 47.  9. 24. 48.]\n",
      "\n",
      "111) frames played: 823, score: 180.0\n",
      "discounted_rewards mean: -8.633569085784935e-18\n",
      "Shifting rewards by 0.8336587443525\n",
      "new discounted_rewards mean: 0.8336587443525\n",
      "avg loss: 1.206\n",
      "model action_tally: [141. 124. 143.  28.  99. 288.]\n",
      "train_tally:        [81. 83. 78. 54. 84. 82.]\n",
      "discounted_rewards mean: -8.633569085784935e-18\n",
      "Shifting rewards by 0.8336587443525\n",
      "new discounted_rewards mean: 0.8336587443525\n",
      "avg loss: 0.538\n",
      "model action_tally: [141. 124. 143.  28.  99. 288.]\n",
      "train_tally:        [79. 81. 74. 54. 81. 80.]\n",
      "discounted_rewards mean: -8.633569085784935e-18\n",
      "Shifting rewards by 0.8336587443525\n",
      "new discounted_rewards mean: 0.8336587443525\n",
      "avg loss: 0.420\n",
      "model action_tally: [141. 124. 143.  28.  99. 288.]\n",
      "train_tally:        [76. 80. 74. 54. 81. 80.]\n",
      "\n",
      "112) frames played: 703, score: 175.0\n",
      "discounted_rewards mean: 1.2128752246260602e-16\n",
      "Shifting rewards by 0.903288829634023\n",
      "new discounted_rewards mean: 0.9032888296340231\n",
      "avg loss: 1.195\n",
      "model action_tally: [ 81. 198.  11. 142. 176.  95.]\n",
      "train_tally:        [76. 36. 69. 76. 76. 76.]\n",
      "discounted_rewards mean: 1.2128752246260602e-16\n",
      "Shifting rewards by 0.903288829634023\n",
      "new discounted_rewards mean: 0.9032888296340231\n",
      "avg loss: 0.663\n",
      "model action_tally: [ 81. 198.  11. 142. 176.  95.]\n",
      "train_tally:        [77. 37. 77. 75. 76. 77.]\n",
      "discounted_rewards mean: 1.2128752246260602e-16\n",
      "Shifting rewards by 0.903288829634023\n",
      "new discounted_rewards mean: 0.9032888296340231\n",
      "avg loss: 0.489\n",
      "model action_tally: [ 81. 198.  11. 142. 176.  95.]\n",
      "train_tally:        [75. 37. 76. 76. 76. 76.]\n",
      "\n",
      "113) frames played: 1100, score: 345.0\n",
      "discounted_rewards mean: -7.751375299201094e-17\n",
      "Shifting rewards by 1.0566844889074267\n",
      "new discounted_rewards mean: 1.0566844889074267\n",
      "avg loss: 1.444\n",
      "model action_tally: [ 98.  31. 159. 280. 357. 175.]\n",
      "train_tally:        [ 85.  99. 104.  48. 105. 102.]\n",
      "discounted_rewards mean: -7.751375299201094e-17\n",
      "Shifting rewards by 1.0566844889074267\n",
      "new discounted_rewards mean: 1.0566844889074267\n",
      "avg loss: 0.836\n",
      "model action_tally: [ 98.  31. 159. 280. 357. 175.]\n",
      "train_tally:        [ 85. 108. 106.  48. 111. 111.]\n",
      "discounted_rewards mean: -7.751375299201094e-17\n",
      "Shifting rewards by 1.0566844889074267\n",
      "new discounted_rewards mean: 1.0566844889074267\n",
      "avg loss: 0.538\n",
      "model action_tally: [ 98.  31. 159. 280. 357. 175.]\n",
      "train_tally:        [ 85. 107. 110.  48. 111. 111.]\n",
      "\n",
      "114) frames played: 352, score: 105.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8642298977357081\n",
      "new discounted_rewards mean: 0.864229897735708\n",
      "avg loss: 1.647\n",
      "model action_tally: [ 21.  37.  47.  76.  71. 100.]\n",
      "train_tally:        [46. 44.  6. 22. 41. 46.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8642298977357081\n",
      "new discounted_rewards mean: 0.864229897735708\n",
      "avg loss: 0.852\n",
      "model action_tally: [ 21.  37.  47.  76.  71. 100.]\n",
      "train_tally:        [46. 44.  6. 22. 42. 46.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.8642298977357081\n",
      "new discounted_rewards mean: 0.864229897735708\n",
      "avg loss: 0.584\n",
      "model action_tally: [ 21.  37.  47.  76.  71. 100.]\n",
      "train_tally:        [41. 42.  6. 22. 42. 40.]\n",
      "\n",
      "115) frames played: 922, score: 770.0\n",
      "discounted_rewards mean: -8.477190990630262e-17\n",
      "Shifting rewards by 0.7422177806623336\n",
      "new discounted_rewards mean: 0.7422177806623335\n",
      "avg loss: 1.315\n",
      "model action_tally: [152. 183.  12.   2. 335. 238.]\n",
      "train_tally:        [62. 22. 67. 68. 66. 51.]\n",
      "discounted_rewards mean: -8.477190990630262e-17\n",
      "Shifting rewards by 0.7422177806623336\n",
      "new discounted_rewards mean: 0.7422177806623335\n",
      "avg loss: 0.796\n",
      "model action_tally: [152. 183.  12.   2. 335. 238.]\n",
      "train_tally:        [61. 22. 67. 67. 62. 51.]\n",
      "discounted_rewards mean: -8.477190990630262e-17\n",
      "Shifting rewards by 0.7422177806623336\n",
      "new discounted_rewards mean: 0.7422177806623335\n",
      "avg loss: 0.586\n",
      "model action_tally: [152. 183.  12.   2. 335. 238.]\n",
      "train_tally:        [62. 22. 64. 65. 63. 50.]\n",
      "\n",
      "116) frames played: 1262, score: 610.0\n",
      "discounted_rewards mean: -6.193320200761571e-17\n",
      "Shifting rewards by 0.6254066781209174\n",
      "new discounted_rewards mean: 0.6254066781209174\n",
      "avg loss: 1.422\n",
      "model action_tally: [141.  68. 244. 196. 228. 385.]\n",
      "train_tally:        [87. 67. 53. 44. 88. 87.]\n",
      "discounted_rewards mean: -6.193320200761571e-17\n",
      "Shifting rewards by 0.6254066781209174\n",
      "new discounted_rewards mean: 0.6254066781209174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.691\n",
      "model action_tally: [141.  68. 244. 196. 228. 385.]\n",
      "train_tally:        [85. 67. 53. 44. 88. 89.]\n",
      "discounted_rewards mean: -6.193320200761571e-17\n",
      "Shifting rewards by 0.6254066781209174\n",
      "new discounted_rewards mean: 0.6254066781209174\n",
      "avg loss: 0.500\n",
      "model action_tally: [141.  68. 244. 196. 228. 385.]\n",
      "train_tally:        [87. 61. 53. 44. 93. 94.]\n",
      "\n",
      "117) frames played: 896, score: 275.0\n",
      "discounted_rewards mean: -1.2688263138573217e-16\n",
      "Shifting rewards by 0.9475619861011093\n",
      "new discounted_rewards mean: 0.9475619861011092\n",
      "avg loss: 1.415\n",
      "model action_tally: [109.  46.  17. 105. 151. 468.]\n",
      "train_tally:        [90. 92. 90. 41. 43. 92.]\n",
      "discounted_rewards mean: -1.2688263138573217e-16\n",
      "Shifting rewards by 0.9475619861011093\n",
      "new discounted_rewards mean: 0.9475619861011092\n",
      "avg loss: 0.864\n",
      "model action_tally: [109.  46.  17. 105. 151. 468.]\n",
      "train_tally:        [99. 94. 94. 41. 43. 99.]\n",
      "discounted_rewards mean: -1.2688263138573217e-16\n",
      "Shifting rewards by 0.9475619861011093\n",
      "new discounted_rewards mean: 0.9475619861011092\n",
      "avg loss: 0.658\n",
      "model action_tally: [109.  46.  17. 105. 151. 468.]\n",
      "train_tally:        [95. 93. 90. 41. 43. 96.]\n",
      "\n",
      "118) frames played: 1280, score: 440.0\n",
      "discounted_rewards mean: 4.4408920985006264e-17\n",
      "Shifting rewards by 1.063429682657759\n",
      "new discounted_rewards mean: 1.063429682657759\n",
      "avg loss: 1.280\n",
      "model action_tally: [267. 233. 205. 169. 102. 304.]\n",
      "train_tally:        [141. 100. 110.  44. 142. 143.]\n",
      "discounted_rewards mean: 4.4408920985006264e-17\n",
      "Shifting rewards by 1.063429682657759\n",
      "new discounted_rewards mean: 1.063429682657759\n",
      "avg loss: 0.742\n",
      "model action_tally: [267. 233. 205. 169. 102. 304.]\n",
      "train_tally:        [133. 103. 108.  44. 141. 142.]\n",
      "discounted_rewards mean: 4.4408920985006264e-17\n",
      "Shifting rewards by 1.063429682657759\n",
      "new discounted_rewards mean: 1.063429682657759\n",
      "avg loss: 0.553\n",
      "model action_tally: [267. 233. 205. 169. 102. 304.]\n",
      "train_tally:        [137. 103. 111.  44. 139. 139.]\n",
      "\n",
      "119) frames played: 949, score: 545.0\n",
      "discounted_rewards mean: 5.989822851507693e-17\n",
      "Shifting rewards by 0.6189370371848503\n",
      "new discounted_rewards mean: 0.6189370371848504\n",
      "avg loss: 1.528\n",
      "model action_tally: [375.  71.  36.  36. 205. 226.]\n",
      "train_tally:        [23. 45. 66. 66. 58. 54.]\n",
      "discounted_rewards mean: 5.989822851507693e-17\n",
      "Shifting rewards by 0.6189370371848503\n",
      "new discounted_rewards mean: 0.6189370371848504\n",
      "avg loss: 0.866\n",
      "model action_tally: [375.  71.  36.  36. 205. 226.]\n",
      "train_tally:        [23. 45. 69. 69. 58. 58.]\n",
      "discounted_rewards mean: 5.989822851507693e-17\n",
      "Shifting rewards by 0.6189370371848503\n",
      "new discounted_rewards mean: 0.6189370371848504\n",
      "avg loss: 0.678\n",
      "model action_tally: [375.  71.  36.  36. 205. 226.]\n",
      "train_tally:        [23. 44. 60. 61. 54. 58.]\n",
      "\n",
      "120) frames played: 807, score: 225.0\n",
      "discounted_rewards mean: -1.937043393645874e-16\n",
      "Shifting rewards by 0.9487890217036385\n",
      "new discounted_rewards mean: 0.9487890217036383\n",
      "avg loss: 1.389\n",
      "model action_tally: [ 38.  74.  44. 233. 235. 183.]\n",
      "train_tally:        [62. 73. 39. 51. 72. 73.]\n",
      "discounted_rewards mean: -1.937043393645874e-16\n",
      "Shifting rewards by 0.9487890217036385\n",
      "new discounted_rewards mean: 0.9487890217036383\n",
      "avg loss: 0.821\n",
      "model action_tally: [ 38.  74.  44. 233. 235. 183.]\n",
      "train_tally:        [71. 71. 39. 51. 69. 72.]\n",
      "discounted_rewards mean: -1.937043393645874e-16\n",
      "Shifting rewards by 0.9487890217036385\n",
      "new discounted_rewards mean: 0.9487890217036383\n",
      "avg loss: 0.611\n",
      "model action_tally: [ 38.  74.  44. 233. 235. 183.]\n",
      "train_tally:        [69. 72. 39. 51. 70. 72.]\n",
      "\n",
      "121) frames played: 732, score: 110.0\n",
      "discounted_rewards mean: 3.8827471899459027e-17\n",
      "Shifting rewards by 0.6178770392480192\n",
      "new discounted_rewards mean: 0.6178770392480192\n",
      "avg loss: 1.526\n",
      "model action_tally: [124. 107. 132.  71.  76. 222.]\n",
      "train_tally:        [83. 83. 84. 62. 78. 77.]\n",
      "discounted_rewards mean: 3.8827471899459027e-17\n",
      "Shifting rewards by 0.6178770392480192\n",
      "new discounted_rewards mean: 0.6178770392480192\n",
      "avg loss: 0.867\n",
      "model action_tally: [124. 107. 132.  71.  76. 222.]\n",
      "train_tally:        [80. 80. 77. 61. 76. 77.]\n",
      "discounted_rewards mean: 3.8827471899459027e-17\n",
      "Shifting rewards by 0.6178770392480192\n",
      "new discounted_rewards mean: 0.6178770392480192\n",
      "avg loss: 0.655\n",
      "model action_tally: [124. 107. 132.  71.  76. 222.]\n",
      "train_tally:        [81. 79. 81. 62. 80. 82.]\n",
      "\n",
      "122) frames played: 991, score: 315.0\n",
      "discounted_rewards mean: -5.735965576267206e-17\n",
      "Shifting rewards by 1.114060394343692\n",
      "new discounted_rewards mean: 1.114060394343692\n",
      "avg loss: 1.124\n",
      "model action_tally: [241. 311. 163.  31. 102. 143.]\n",
      "train_tally:        [ 62. 130.  34. 132.  88. 133.]\n",
      "discounted_rewards mean: -5.735965576267206e-17\n",
      "Shifting rewards by 1.114060394343692\n",
      "new discounted_rewards mean: 1.114060394343692\n",
      "avg loss: 0.567\n",
      "model action_tally: [241. 311. 163.  31. 102. 143.]\n",
      "train_tally:        [ 62. 132.  34. 133.  88. 132.]\n",
      "discounted_rewards mean: -5.735965576267206e-17\n",
      "Shifting rewards by 1.114060394343692\n",
      "new discounted_rewards mean: 1.114060394343692\n",
      "avg loss: 0.474\n",
      "model action_tally: [241. 311. 163.  31. 102. 143.]\n",
      "train_tally:        [ 62. 137.  34. 136.  88. 137.]\n",
      "\n",
      "123) frames played: 485, score: 135.0\n",
      "discounted_rewards mean: 1.172029254862021e-16\n",
      "Shifting rewards by 0.8636574148514784\n",
      "new discounted_rewards mean: 0.8636574148514787\n",
      "avg loss: 1.127\n",
      "model action_tally: [ 84.  67.   0. 150.  57. 127.]\n",
      "train_tally:        [19. 48. 46. 45. 49. 37.]\n",
      "discounted_rewards mean: 1.172029254862021e-16\n",
      "Shifting rewards by 0.8636574148514784\n",
      "new discounted_rewards mean: 0.8636574148514787\n",
      "avg loss: 0.599\n",
      "model action_tally: [ 84.  67.   0. 150.  57. 127.]\n",
      "train_tally:        [19. 47. 50. 46. 50. 37.]\n",
      "discounted_rewards mean: 1.172029254862021e-16\n",
      "Shifting rewards by 0.8636574148514784\n",
      "new discounted_rewards mean: 0.8636574148514787\n",
      "avg loss: 0.435\n",
      "model action_tally: [ 84.  67.   0. 150.  57. 127.]\n",
      "train_tally:        [19. 50. 50. 49. 48. 37.]\n",
      "\n",
      "124) frames played: 819, score: 170.0\n",
      "discounted_rewards mean: 6.940588383493042e-17\n",
      "Shifting rewards by 0.8327892791149039\n",
      "new discounted_rewards mean: 0.832789279114904\n",
      "avg loss: 1.352\n",
      "model action_tally: [ 12. 104. 178. 170. 147. 208.]\n",
      "train_tally:        [85. 88. 66. 58. 64. 89.]\n",
      "discounted_rewards mean: 6.940588383493042e-17\n",
      "Shifting rewards by 0.8327892791149039\n",
      "new discounted_rewards mean: 0.832789279114904\n",
      "avg loss: 0.810\n",
      "model action_tally: [ 12. 104. 178. 170. 147. 208.]\n",
      "train_tally:        [85. 84. 66. 58. 62. 85.]\n",
      "discounted_rewards mean: 6.940588383493042e-17\n",
      "Shifting rewards by 0.8327892791149039\n",
      "new discounted_rewards mean: 0.832789279114904\n",
      "avg loss: 0.616\n",
      "model action_tally: [ 12. 104. 178. 170. 147. 208.]\n",
      "train_tally:        [88. 83. 66. 58. 64. 88.]\n",
      "\n",
      "125) frames played: 598, score: 140.0\n",
      "discounted_rewards mean: 3.564595664348329e-17\n",
      "Shifting rewards by 0.8540063207235934\n",
      "new discounted_rewards mean: 0.8540063207235934\n",
      "avg loss: 1.364\n",
      "model action_tally: [145. 161.  71.  35.  71. 115.]\n",
      "train_tally:        [47. 55. 59. 67. 41. 68.]\n",
      "discounted_rewards mean: 3.564595664348329e-17\n",
      "Shifting rewards by 0.8540063207235934\n",
      "new discounted_rewards mean: 0.8540063207235934\n",
      "avg loss: 0.756\n",
      "model action_tally: [145. 161.  71.  35.  71. 115.]\n",
      "train_tally:        [47. 55. 59. 64. 41. 65.]\n",
      "discounted_rewards mean: 3.564595664348329e-17\n",
      "Shifting rewards by 0.8540063207235934\n",
      "new discounted_rewards mean: 0.8540063207235934\n",
      "avg loss: 0.551\n",
      "model action_tally: [145. 161.  71.  35.  71. 115.]\n",
      "train_tally:        [47. 53. 54. 66. 40. 67.]\n",
      "\n",
      "126) frames played: 849, score: 210.0\n",
      "discounted_rewards mean: -5.02150343293357e-17\n",
      "Shifting rewards by 0.8425846441565613\n",
      "new discounted_rewards mean: 0.8425846441565614\n",
      "avg loss: 1.273\n",
      "model action_tally: [204. 105.  30.  95.  84. 331.]\n",
      "train_tally:        [74. 91. 65. 90. 76. 91.]\n",
      "discounted_rewards mean: -5.02150343293357e-17\n",
      "Shifting rewards by 0.8425846441565613\n",
      "new discounted_rewards mean: 0.8425846441565614\n",
      "avg loss: 0.723\n",
      "model action_tally: [204. 105.  30.  95.  84. 331.]\n",
      "train_tally:        [74. 88. 65. 88. 76. 89.]\n",
      "discounted_rewards mean: -5.02150343293357e-17\n",
      "Shifting rewards by 0.8425846441565613\n",
      "new discounted_rewards mean: 0.8425846441565614\n",
      "avg loss: 0.515\n",
      "model action_tally: [204. 105.  30.  95.  84. 331.]\n",
      "train_tally:        [74. 91. 65. 93. 75. 93.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "127) frames played: 811, score: 200.0\n",
      "discounted_rewards mean: 7.885184490555982e-17\n",
      "Shifting rewards by 0.8614179049868024\n",
      "new discounted_rewards mean: 0.8614179049868023\n",
      "avg loss: 1.251\n",
      "model action_tally: [106. 238. 122.  77.  87. 181.]\n",
      "train_tally:        [55. 96. 21. 71. 96. 97.]\n",
      "discounted_rewards mean: 7.885184490555982e-17\n",
      "Shifting rewards by 0.8614179049868024\n",
      "new discounted_rewards mean: 0.8614179049868023\n",
      "avg loss: 0.696\n",
      "model action_tally: [106. 238. 122.  77.  87. 181.]\n",
      "train_tally:        [ 55. 102.  21.  71. 106. 107.]\n",
      "discounted_rewards mean: 7.885184490555982e-17\n",
      "Shifting rewards by 0.8614179049868024\n",
      "new discounted_rewards mean: 0.8614179049868023\n",
      "avg loss: 0.471\n",
      "model action_tally: [106. 238. 122.  77.  87. 181.]\n",
      "train_tally:        [ 55. 100.  21.  71. 109. 109.]\n",
      "\n",
      "128) frames played: 567, score: 105.0\n",
      "discounted_rewards mean: -1.2531617914640214e-16\n",
      "Shifting rewards by 0.8017951439456878\n",
      "new discounted_rewards mean: 0.8017951439456877\n",
      "avg loss: 1.243\n",
      "model action_tally: [ 76. 100.  76. 114. 133.  68.]\n",
      "train_tally:        [64. 55. 39. 64. 30. 60.]\n",
      "discounted_rewards mean: -1.2531617914640214e-16\n",
      "Shifting rewards by 0.8017951439456878\n",
      "new discounted_rewards mean: 0.8017951439456877\n",
      "avg loss: 0.531\n",
      "model action_tally: [ 76. 100.  76. 114. 133.  68.]\n",
      "train_tally:        [63. 52. 39. 63. 30. 64.]\n",
      "discounted_rewards mean: -1.2531617914640214e-16\n",
      "Shifting rewards by 0.8017951439456878\n",
      "new discounted_rewards mean: 0.8017951439456877\n",
      "avg loss: 0.361\n",
      "model action_tally: [ 76. 100.  76. 114. 133.  68.]\n",
      "train_tally:        [64. 55. 39. 65. 30. 64.]\n",
      "\n",
      "129) frames played: 705, score: 105.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.49314756546466487\n",
      "new discounted_rewards mean: 0.49314756546466487\n",
      "avg loss: 0.096\n",
      "model action_tally: [154.  85.  51. 233.  64. 118.]\n",
      "train_tally:        [23. 21.  5. 23.  0. 22.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.49314756546466487\n",
      "new discounted_rewards mean: 0.49314756546466487\n",
      "avg loss: 0.080\n",
      "model action_tally: [154.  85.  51. 233.  64. 118.]\n",
      "train_tally:        [24. 18.  5. 23.  0. 24.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.49314756546466487\n",
      "new discounted_rewards mean: 0.49314756546466487\n",
      "avg loss: 0.049\n",
      "model action_tally: [154.  85.  51. 233.  64. 118.]\n",
      "train_tally:        [22. 21.  5. 22.  0. 23.]\n",
      "\n",
      "130) frames played: 963, score: 225.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6611889554016713\n",
      "new discounted_rewards mean: 0.6611889554016712\n",
      "avg loss: 1.494\n",
      "model action_tally: [213.  40.  89. 302.  62. 257.]\n",
      "train_tally:        [103. 104. 107.  41. 107. 108.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6611889554016713\n",
      "new discounted_rewards mean: 0.6611889554016712\n",
      "avg loss: 0.967\n",
      "model action_tally: [213.  40.  89. 302.  62. 257.]\n",
      "train_tally:        [107. 105. 109.  41. 110. 111.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.6611889554016713\n",
      "new discounted_rewards mean: 0.6611889554016712\n",
      "avg loss: 0.737\n",
      "model action_tally: [213.  40.  89. 302.  62. 257.]\n",
      "train_tally:        [103.  99. 108.  41. 106. 109.]\n",
      "\n",
      "131) frames played: 626, score: 175.0\n",
      "discounted_rewards mean: -7.945366054825401e-17\n",
      "Shifting rewards by 0.901993333451744\n",
      "new discounted_rewards mean: 0.9019933334517438\n",
      "avg loss: 1.215\n",
      "model action_tally: [116.  48. 129.  93. 103. 137.]\n",
      "train_tally:        [68. 75.  9. 40. 73. 75.]\n",
      "discounted_rewards mean: -7.945366054825401e-17\n",
      "Shifting rewards by 0.901993333451744\n",
      "new discounted_rewards mean: 0.9019933334517438\n",
      "avg loss: 0.607\n",
      "model action_tally: [116.  48. 129.  93. 103. 137.]\n",
      "train_tally:        [69. 70.  9. 35. 70. 70.]\n",
      "discounted_rewards mean: -7.945366054825401e-17\n",
      "Shifting rewards by 0.901993333451744\n",
      "new discounted_rewards mean: 0.9019933334517438\n",
      "avg loss: 0.476\n",
      "model action_tally: [116.  48. 129.  93. 103. 137.]\n",
      "train_tally:        [71. 76.  9. 40. 68. 76.]\n",
      "\n",
      "132) frames played: 862, score: 260.0\n",
      "discounted_rewards mean: -1.8134501376707893e-16\n",
      "Shifting rewards by 1.0979865494902927\n",
      "new discounted_rewards mean: 1.0979865494902925\n",
      "avg loss: 1.260\n",
      "model action_tally: [ 83. 389.   8.  70. 172. 140.]\n",
      "train_tally:        [92. 92. 43. 92. 91. 93.]\n",
      "discounted_rewards mean: -1.8134501376707893e-16\n",
      "Shifting rewards by 1.0979865494902927\n",
      "new discounted_rewards mean: 1.0979865494902925\n",
      "avg loss: 0.652\n",
      "model action_tally: [ 83. 389.   8.  70. 172. 140.]\n",
      "train_tally:        [90. 90. 43. 89. 90. 91.]\n",
      "discounted_rewards mean: -1.8134501376707893e-16\n",
      "Shifting rewards by 1.0979865494902927\n",
      "new discounted_rewards mean: 1.0979865494902925\n",
      "avg loss: 0.499\n",
      "model action_tally: [ 83. 389.   8.  70. 172. 140.]\n",
      "train_tally:        [91. 92. 43. 90. 86. 90.]\n",
      "\n",
      "133) frames played: 735, score: 280.0\n",
      "discounted_rewards mean: -6.283711268626737e-17\n",
      "Shifting rewards by 1.0639581425378593\n",
      "new discounted_rewards mean: 1.0639581425378593\n",
      "avg loss: 1.099\n",
      "model action_tally: [268.  91.  62.  89.  93. 132.]\n",
      "train_tally:        [55. 71. 67. 49. 71. 69.]\n",
      "discounted_rewards mean: -6.283711268626737e-17\n",
      "Shifting rewards by 1.0639581425378593\n",
      "new discounted_rewards mean: 1.0639581425378593\n",
      "avg loss: 0.547\n",
      "model action_tally: [268.  91.  62.  89.  93. 132.]\n",
      "train_tally:        [56. 72. 73. 49. 72. 72.]\n",
      "discounted_rewards mean: -6.283711268626737e-17\n",
      "Shifting rewards by 1.0639581425378593\n",
      "new discounted_rewards mean: 1.0639581425378593\n",
      "avg loss: 0.431\n",
      "model action_tally: [268.  91.  62.  89.  93. 132.]\n",
      "train_tally:        [56. 74. 75. 48. 75. 68.]\n",
      "\n",
      "134) frames played: 435, score: 110.0\n",
      "discounted_rewards mean: -9.800589458760002e-17\n",
      "Shifting rewards by 0.8418088144602504\n",
      "new discounted_rewards mean: 0.8418088144602502\n",
      "avg loss: 1.024\n",
      "model action_tally: [ 85.  11.  54.  83. 105.  97.]\n",
      "train_tally:        [58. 51.  3. 59.  1. 60.]\n",
      "discounted_rewards mean: -9.800589458760002e-17\n",
      "Shifting rewards by 0.8418088144602504\n",
      "new discounted_rewards mean: 0.8418088144602502\n",
      "avg loss: 0.398\n",
      "model action_tally: [ 85.  11.  54.  83. 105.  97.]\n",
      "train_tally:        [58. 58.  3. 56.  1. 58.]\n",
      "discounted_rewards mean: -9.800589458760002e-17\n",
      "Shifting rewards by 0.8418088144602504\n",
      "new discounted_rewards mean: 0.8418088144602502\n",
      "avg loss: 0.342\n",
      "model action_tally: [ 85.  11.  54.  83. 105.  97.]\n",
      "train_tally:        [60. 51.  3. 60.  1. 60.]\n",
      "\n",
      "135) frames played: 1462, score: 320.0\n",
      "discounted_rewards mean: 2.9160440592069775e-17\n",
      "Shifting rewards by 0.6689424605965791\n",
      "new discounted_rewards mean: 0.6689424605965792\n",
      "avg loss: 1.402\n",
      "model action_tally: [309. 398.   9. 337.  38. 371.]\n",
      "train_tally:        [133.  95. 167. 166. 165. 168.]\n",
      "discounted_rewards mean: 2.9160440592069775e-17\n",
      "Shifting rewards by 0.6689424605965791\n",
      "new discounted_rewards mean: 0.6689424605965792\n",
      "avg loss: 0.764\n",
      "model action_tally: [309. 398.   9. 337.  38. 371.]\n",
      "train_tally:        [135.  96. 177. 169. 174. 178.]\n",
      "discounted_rewards mean: 2.9160440592069775e-17\n",
      "Shifting rewards by 0.6689424605965791\n",
      "new discounted_rewards mean: 0.6689424605965792\n",
      "avg loss: 0.572\n",
      "model action_tally: [309. 398.   9. 337.  38. 371.]\n",
      "train_tally:        [140.  96. 172. 159. 173. 173.]\n",
      "\n",
      "136) frames played: 736, score: 170.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7802854327697449\n",
      "new discounted_rewards mean: 0.7802854327697449\n",
      "avg loss: 1.274\n",
      "model action_tally: [119.  91. 179. 177.  76.  94.]\n",
      "train_tally:        [79. 71. 37. 74. 30. 80.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7802854327697449\n",
      "new discounted_rewards mean: 0.7802854327697449\n",
      "avg loss: 0.738\n",
      "model action_tally: [119.  91. 179. 177.  76.  94.]\n",
      "train_tally:        [81. 68. 37. 74. 30. 81.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.7802854327697449\n",
      "new discounted_rewards mean: 0.7802854327697449\n",
      "avg loss: 0.544\n",
      "model action_tally: [119.  91. 179. 177.  76.  94.]\n",
      "train_tally:        [76. 66. 37. 75. 30. 77.]\n",
      "\n",
      "137) frames played: 618, score: 105.0\n",
      "discounted_rewards mean: -1.3796946325438837e-16\n",
      "Shifting rewards by 0.536056953117359\n",
      "new discounted_rewards mean: 0.536056953117359\n",
      "avg loss: 1.334\n",
      "model action_tally: [ 54. 101. 157.  81.  51. 174.]\n",
      "train_tally:        [76. 78. 78. 53. 66. 79.]\n",
      "discounted_rewards mean: -1.3796946325438837e-16\n",
      "Shifting rewards by 0.536056953117359\n",
      "new discounted_rewards mean: 0.536056953117359\n",
      "avg loss: 0.700\n",
      "model action_tally: [ 54. 101. 157.  81.  51. 174.]\n",
      "train_tally:        [80. 77. 80. 53. 68. 80.]\n",
      "discounted_rewards mean: -1.3796946325438837e-16\n",
      "Shifting rewards by 0.536056953117359\n",
      "new discounted_rewards mean: 0.536056953117359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.526\n",
      "model action_tally: [ 54. 101. 157.  81.  51. 174.]\n",
      "train_tally:        [78. 77. 74. 53. 66. 78.]\n",
      "\n",
      "138) frames played: 971, score: 285.0\n",
      "discounted_rewards mean: -2.9270555541095784e-17\n",
      "Shifting rewards by 0.829015025958447\n",
      "new discounted_rewards mean: 0.829015025958447\n",
      "avg loss: 1.193\n",
      "model action_tally: [193. 311.  36.  92.  90. 249.]\n",
      "train_tally:        [103.  69.  98. 103. 103. 104.]\n",
      "discounted_rewards mean: -2.9270555541095784e-17\n",
      "Shifting rewards by 0.829015025958447\n",
      "new discounted_rewards mean: 0.829015025958447\n",
      "avg loss: 0.588\n",
      "model action_tally: [193. 311.  36.  92.  90. 249.]\n",
      "train_tally:        [109.  69. 103. 104. 109. 110.]\n",
      "discounted_rewards mean: -2.9270555541095784e-17\n",
      "Shifting rewards by 0.829015025958447\n",
      "new discounted_rewards mean: 0.829015025958447\n",
      "avg loss: 0.447\n",
      "model action_tally: [193. 311.  36.  92.  90. 249.]\n",
      "train_tally:        [102.  69. 101. 102. 102. 103.]\n",
      "\n",
      "139) frames played: 802, score: 180.0\n",
      "discounted_rewards mean: 8.859635109228182e-17\n",
      "Shifting rewards by 0.8800013151053199\n",
      "new discounted_rewards mean: 0.88000131510532\n",
      "avg loss: 1.307\n",
      "model action_tally: [177.  51. 147.  96. 111. 220.]\n",
      "train_tally:        [69. 57. 37. 73. 64. 74.]\n",
      "discounted_rewards mean: 8.859635109228182e-17\n",
      "Shifting rewards by 0.8800013151053199\n",
      "new discounted_rewards mean: 0.88000131510532\n",
      "avg loss: 0.806\n",
      "model action_tally: [177.  51. 147.  96. 111. 220.]\n",
      "train_tally:        [66. 58. 37. 77. 67. 78.]\n",
      "discounted_rewards mean: 8.859635109228182e-17\n",
      "Shifting rewards by 0.8800013151053199\n",
      "new discounted_rewards mean: 0.88000131510532\n",
      "avg loss: 0.563\n",
      "model action_tally: [177.  51. 147.  96. 111. 220.]\n",
      "train_tally:        [65. 57. 37. 70. 66. 71.]\n",
      "\n",
      "140) frames played: 823, score: 210.0\n",
      "discounted_rewards mean: -8.633569085784936e-17\n",
      "Shifting rewards by 0.7640882636845954\n",
      "new discounted_rewards mean: 0.7640882636845956\n",
      "avg loss: 1.208\n",
      "model action_tally: [ 25.  80.  61. 204. 139. 314.]\n",
      "train_tally:        [101. 104.  77.  87.  18. 105.]\n",
      "discounted_rewards mean: -8.633569085784936e-17\n",
      "Shifting rewards by 0.7640882636845954\n",
      "new discounted_rewards mean: 0.7640882636845956\n",
      "avg loss: 0.598\n",
      "model action_tally: [ 25.  80.  61. 204. 139. 314.]\n",
      "train_tally:        [112. 112.  77.  85.  18. 112.]\n",
      "discounted_rewards mean: -8.633569085784936e-17\n",
      "Shifting rewards by 0.7640882636845954\n",
      "new discounted_rewards mean: 0.7640882636845956\n",
      "avg loss: 0.424\n",
      "model action_tally: [ 25.  80.  61. 204. 139. 314.]\n",
      "train_tally:        [105. 106.  77.  87.  18. 105.]\n",
      "\n",
      "141) frames played: 542, score: 105.0\n",
      "discounted_rewards mean: 5.243857828487824e-17\n",
      "Shifting rewards by 0.584329298074918\n",
      "new discounted_rewards mean: 0.5843292980749181\n",
      "avg loss: 1.208\n",
      "model action_tally: [124. 120. 112.  61.   2. 123.]\n",
      "train_tally:        [22. 24. 70. 75. 75. 76.]\n",
      "discounted_rewards mean: 5.243857828487824e-17\n",
      "Shifting rewards by 0.584329298074918\n",
      "new discounted_rewards mean: 0.5843292980749181\n",
      "avg loss: 0.427\n",
      "model action_tally: [124. 120. 112.  61.   2. 123.]\n",
      "train_tally:        [22. 24. 75. 79. 78. 80.]\n",
      "discounted_rewards mean: 5.243857828487824e-17\n",
      "Shifting rewards by 0.584329298074918\n",
      "new discounted_rewards mean: 0.5843292980749181\n",
      "avg loss: 0.325\n",
      "model action_tally: [124. 120. 112.  61.   2. 123.]\n",
      "train_tally:        [22. 24. 75. 77. 74. 77.]\n",
      "\n",
      "142) frames played: 681, score: 120.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.5898713903875961\n",
      "new discounted_rewards mean: 0.5898713903875961\n",
      "avg loss: 1.101\n",
      "model action_tally: [ 22. 164.  49. 215. 127. 104.]\n",
      "train_tally:        [67. 61. 68. 34. 38. 68.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.5898713903875961\n",
      "new discounted_rewards mean: 0.5898713903875961\n",
      "avg loss: 0.534\n",
      "model action_tally: [ 22. 164.  49. 215. 127. 104.]\n",
      "train_tally:        [76. 62. 73. 34. 38. 76.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.5898713903875961\n",
      "new discounted_rewards mean: 0.5898713903875961\n",
      "avg loss: 0.412\n",
      "model action_tally: [ 22. 164.  49. 215. 127. 104.]\n",
      "train_tally:        [76. 63. 76. 34. 38. 76.]\n",
      "\n",
      "143) frames played: 838, score: 145.0\n",
      "discounted_rewards mean: 1.017483631160048e-16\n",
      "Shifting rewards by 0.5851181376221978\n",
      "new discounted_rewards mean: 0.5851181376221977\n",
      "avg loss: 1.154\n",
      "model action_tally: [230. 210. 209.  32.   4. 153.]\n",
      "train_tally:        [ 24.  58.  44. 110. 108. 110.]\n",
      "discounted_rewards mean: 1.017483631160048e-16\n",
      "Shifting rewards by 0.5851181376221978\n",
      "new discounted_rewards mean: 0.5851181376221977\n",
      "avg loss: 0.459\n",
      "model action_tally: [230. 210. 209.  32.   4. 153.]\n",
      "train_tally:        [ 24.  57.  44. 110. 105. 110.]\n",
      "discounted_rewards mean: 1.017483631160048e-16\n",
      "Shifting rewards by 0.5851181376221978\n",
      "new discounted_rewards mean: 0.5851181376221977\n",
      "avg loss: 0.362\n",
      "model action_tally: [230. 210. 209.  32.   4. 153.]\n",
      "train_tally:        [ 24.  58.  44. 115. 116. 115.]\n",
      "\n",
      "144) frames played: 1252, score: 330.0\n",
      "discounted_rewards mean: -4.5402091741859435e-17\n",
      "Shifting rewards by 0.8340117784412958\n",
      "new discounted_rewards mean: 0.8340117784412958\n",
      "avg loss: 1.258\n",
      "model action_tally: [ 19.  75. 159. 381. 232. 386.]\n",
      "train_tally:        [ 78. 119. 100. 102. 112. 120.]\n",
      "discounted_rewards mean: -4.5402091741859435e-17\n",
      "Shifting rewards by 0.8340117784412958\n",
      "new discounted_rewards mean: 0.8340117784412958\n",
      "avg loss: 0.623\n",
      "model action_tally: [ 19.  75. 159. 381. 232. 386.]\n",
      "train_tally:        [ 78. 113. 100. 105. 113. 112.]\n",
      "discounted_rewards mean: -4.5402091741859435e-17\n",
      "Shifting rewards by 0.8340117784412958\n",
      "new discounted_rewards mean: 0.8340117784412958\n",
      "avg loss: 0.436\n",
      "model action_tally: [ 19.  75. 159. 381. 232. 386.]\n",
      "train_tally:        [ 78. 117.  95. 104. 114. 118.]\n",
      "\n",
      "145) frames played: 830, score: 240.0\n",
      "discounted_rewards mean: -1.0272907023037593e-16\n",
      "Shifting rewards by 0.8714015917504702\n",
      "new discounted_rewards mean: 0.8714015917504699\n",
      "avg loss: 1.789\n",
      "model action_tally: [150. 126.  62. 105. 162. 225.]\n",
      "train_tally:        [59. 58. 53. 73. 73. 74.]\n",
      "discounted_rewards mean: -1.0272907023037593e-16\n",
      "Shifting rewards by 0.8714015917504702\n",
      "new discounted_rewards mean: 0.8714015917504699\n",
      "avg loss: 1.081\n",
      "model action_tally: [150. 126.  62. 105. 162. 225.]\n",
      "train_tally:        [59. 58. 53. 72. 85. 86.]\n",
      "discounted_rewards mean: -1.0272907023037593e-16\n",
      "Shifting rewards by 0.8714015917504702\n",
      "new discounted_rewards mean: 0.8714015917504699\n",
      "avg loss: 0.731\n",
      "model action_tally: [150. 126.  62. 105. 162. 225.]\n",
      "train_tally:        [59. 58. 53. 76. 78. 79.]\n",
      "\n",
      "146) frames played: 1177, score: 435.0\n",
      "discounted_rewards mean: -8.451655310655397e-17\n",
      "Shifting rewards by 1.1092692060463403\n",
      "new discounted_rewards mean: 1.1092692060463403\n",
      "avg loss: 1.355\n",
      "model action_tally: [ 47.  63. 195. 149. 312. 411.]\n",
      "train_tally:        [118. 114.  90.  58. 117. 118.]\n",
      "discounted_rewards mean: -8.451655310655397e-17\n",
      "Shifting rewards by 1.1092692060463403\n",
      "new discounted_rewards mean: 1.1092692060463403\n",
      "avg loss: 0.855\n",
      "model action_tally: [ 47.  63. 195. 149. 312. 411.]\n",
      "train_tally:        [116. 117.  90.  58. 117. 118.]\n",
      "discounted_rewards mean: -8.451655310655397e-17\n",
      "Shifting rewards by 1.1092692060463403\n",
      "new discounted_rewards mean: 1.1092692060463403\n",
      "avg loss: 0.631\n",
      "model action_tally: [ 47.  63. 195. 149. 312. 411.]\n",
      "train_tally:        [121. 121.  88.  58. 121. 122.]\n",
      "\n",
      "147) frames played: 642, score: 165.0\n",
      "discounted_rewards mean: 6.640586315514955e-17\n",
      "Shifting rewards by 0.9194072109473598\n",
      "new discounted_rewards mean: 0.9194072109473598\n",
      "avg loss: 1.577\n",
      "model action_tally: [215.  25.  71.  25. 103. 203.]\n",
      "train_tally:        [63. 60. 43. 43. 31. 64.]\n",
      "discounted_rewards mean: 6.640586315514955e-17\n",
      "Shifting rewards by 0.9194072109473598\n",
      "new discounted_rewards mean: 0.9194072109473598\n",
      "avg loss: 0.980\n",
      "model action_tally: [215.  25.  71.  25. 103. 203.]\n",
      "train_tally:        [60. 58. 46. 43. 31. 61.]\n",
      "discounted_rewards mean: 6.640586315514955e-17\n",
      "Shifting rewards by 0.9194072109473598\n",
      "new discounted_rewards mean: 0.9194072109473598\n",
      "avg loss: 0.754\n",
      "model action_tally: [215.  25.  71.  25. 103. 203.]\n",
      "train_tally:        [57. 57. 45. 41. 31. 58.]\n",
      "\n",
      "148) frames played: 1044, score: 540.0\n",
      "discounted_rewards mean: 5.4447719215333345e-17\n",
      "Shifting rewards by 0.6879137779304241\n",
      "new discounted_rewards mean: 0.687913777930424\n",
      "avg loss: 1.652\n",
      "model action_tally: [176. 161. 118.  95.  39. 455.]\n",
      "train_tally:        [69. 65. 33. 65. 39. 70.]\n",
      "discounted_rewards mean: 5.4447719215333345e-17\n",
      "Shifting rewards by 0.6879137779304241\n",
      "new discounted_rewards mean: 0.687913777930424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.976\n",
      "model action_tally: [176. 161. 118.  95.  39. 455.]\n",
      "train_tally:        [70. 68. 33. 65. 39. 71.]\n",
      "discounted_rewards mean: 5.4447719215333345e-17\n",
      "Shifting rewards by 0.6879137779304241\n",
      "new discounted_rewards mean: 0.687913777930424\n",
      "avg loss: 0.640\n",
      "model action_tally: [176. 161. 118.  95.  39. 455.]\n",
      "train_tally:        [70. 68. 33. 65. 39. 71.]\n",
      "\n",
      "149) frames played: 846, score: 210.0\n",
      "discounted_rewards mean: 5.039310182695746e-17\n",
      "Shifting rewards by 0.6652422840947236\n",
      "new discounted_rewards mean: 0.6652422840947236\n",
      "avg loss: 1.411\n",
      "model action_tally: [124. 195.  13. 206.  76. 232.]\n",
      "train_tally:        [109.  69. 108. 100.  57. 110.]\n",
      "discounted_rewards mean: 5.039310182695746e-17\n",
      "Shifting rewards by 0.6652422840947236\n",
      "new discounted_rewards mean: 0.6652422840947236\n",
      "avg loss: 0.878\n",
      "model action_tally: [124. 195.  13. 206.  76. 232.]\n",
      "train_tally:        [106.  73. 107. 101.  57. 107.]\n",
      "discounted_rewards mean: 5.039310182695746e-17\n",
      "Shifting rewards by 0.6652422840947236\n",
      "new discounted_rewards mean: 0.6652422840947236\n",
      "avg loss: 0.646\n",
      "model action_tally: [124. 195.  13. 206.  76. 232.]\n",
      "train_tally:        [106.  73. 105.  98.  57. 107.]\n",
      "\n",
      "150) frames played: 694, score: 160.0\n",
      "discounted_rewards mean: -1.023836795043372e-16\n",
      "Shifting rewards by 0.8627683096413814\n",
      "new discounted_rewards mean: 0.8627683096413814\n",
      "avg loss: 1.547\n",
      "model action_tally: [153.  57. 139. 128.  50. 167.]\n",
      "train_tally:        [57. 45. 76. 76. 30. 77.]\n",
      "discounted_rewards mean: -1.023836795043372e-16\n",
      "Shifting rewards by 0.8627683096413814\n",
      "new discounted_rewards mean: 0.8627683096413814\n",
      "avg loss: 0.904\n",
      "model action_tally: [153.  57. 139. 128.  50. 167.]\n",
      "train_tally:        [57. 45. 79. 77. 30. 79.]\n",
      "discounted_rewards mean: -1.023836795043372e-16\n",
      "Shifting rewards by 0.8627683096413814\n",
      "new discounted_rewards mean: 0.8627683096413814\n",
      "avg loss: 0.632\n",
      "model action_tally: [153.  57. 139. 128.  50. 167.]\n",
      "train_tally:        [55. 45. 78. 78. 30. 79.]\n",
      "\n",
      "151) frames played: 686, score: 145.0\n",
      "discounted_rewards mean: 4.143106330962683e-17\n",
      "Shifting rewards by 0.6916285920360699\n",
      "new discounted_rewards mean: 0.69162859203607\n",
      "avg loss: 1.179\n",
      "model action_tally: [ 83.  57. 227. 185.  11. 123.]\n",
      "train_tally:        [56. 76. 35. 76. 77. 77.]\n",
      "discounted_rewards mean: 4.143106330962683e-17\n",
      "Shifting rewards by 0.6916285920360699\n",
      "new discounted_rewards mean: 0.69162859203607\n",
      "avg loss: 0.614\n",
      "model action_tally: [ 83.  57. 227. 185.  11. 123.]\n",
      "train_tally:        [58. 78. 35. 79. 78. 79.]\n",
      "discounted_rewards mean: 4.143106330962683e-17\n",
      "Shifting rewards by 0.6916285920360699\n",
      "new discounted_rewards mean: 0.69162859203607\n",
      "avg loss: 0.489\n",
      "model action_tally: [ 83.  57. 227. 185.  11. 123.]\n",
      "train_tally:        [58. 79. 35. 82. 78. 83.]\n",
      "\n",
      "152) frames played: 679, score: 105.0\n",
      "discounted_rewards mean: -1.255745630209308e-16\n",
      "Shifting rewards by 0.5060713294374943\n",
      "new discounted_rewards mean: 0.5060713294374942\n",
      "avg loss: 1.274\n",
      "model action_tally: [ 94. 167. 130.  60.  16. 212.]\n",
      "train_tally:        [60. 24. 84. 98. 97. 99.]\n",
      "discounted_rewards mean: -1.255745630209308e-16\n",
      "Shifting rewards by 0.5060713294374943\n",
      "new discounted_rewards mean: 0.5060713294374942\n",
      "avg loss: 0.684\n",
      "model action_tally: [ 94. 167. 130.  60.  16. 212.]\n",
      "train_tally:        [61. 24. 87. 92. 93. 91.]\n",
      "discounted_rewards mean: -1.255745630209308e-16\n",
      "Shifting rewards by 0.5060713294374943\n",
      "new discounted_rewards mean: 0.5060713294374942\n",
      "avg loss: 0.479\n",
      "model action_tally: [ 94. 167. 130.  60.  16. 212.]\n",
      "train_tally:        [61. 24. 85. 98. 99. 99.]\n",
      "\n",
      "153) frames played: 410, score: 85.0\n",
      "discounted_rewards mean: -6.93212425131805e-17\n",
      "Shifting rewards by 0.7072457530188645\n",
      "new discounted_rewards mean: 0.7072457530188644\n",
      "avg loss: 0.962\n",
      "model action_tally: [ 70.  10.  29. 154.  92.  55.]\n",
      "train_tally:        [51. 54.  8. 37. 24. 54.]\n",
      "discounted_rewards mean: -6.93212425131805e-17\n",
      "Shifting rewards by 0.7072457530188645\n",
      "new discounted_rewards mean: 0.7072457530188644\n",
      "avg loss: 0.350\n",
      "model action_tally: [ 70.  10.  29. 154.  92.  55.]\n",
      "train_tally:        [52. 59.  8. 35. 24. 59.]\n",
      "discounted_rewards mean: -6.93212425131805e-17\n",
      "Shifting rewards by 0.7072457530188645\n",
      "new discounted_rewards mean: 0.7072457530188644\n",
      "avg loss: 0.256\n",
      "model action_tally: [ 70.  10.  29. 154.  92.  55.]\n",
      "train_tally:        [51. 59.  8. 37. 24. 59.]\n",
      "\n",
      "154) frames played: 1005, score: 290.0\n",
      "discounted_rewards mean: -2.1210230918211946e-17\n",
      "Shifting rewards by 0.8863321210246822\n",
      "new discounted_rewards mean: 0.8863321210246821\n",
      "avg loss: 1.122\n",
      "model action_tally: [ 54. 184.  72. 153.  60. 482.]\n",
      "train_tally:        [118.  61. 106. 116. 119. 120.]\n",
      "discounted_rewards mean: -2.1210230918211946e-17\n",
      "Shifting rewards by 0.8863321210246822\n",
      "new discounted_rewards mean: 0.8863321210246821\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    #play a game\n",
    "    atari_env = play_game(environment_name, atari_model)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "\n",
    "    #train the model\n",
    "    for ii in range(3):\n",
    "        train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
