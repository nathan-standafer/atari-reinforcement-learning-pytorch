{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# environment_name = \"SpaceInvaders-v4\"\n",
    "# typical_bad_game_frame_count = 250\n",
    "# reward_frame_shift = -15\n",
    "\n",
    "environment_name = \"Pong-v4\"\n",
    "typical_bad_game_frame_count = 1050\n",
    "reward_frame_shift = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pytorch model.  for now, accept a 210 x 160 greyscale image and output an array of actions\n",
    "\n",
    "\n",
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_count, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8320, 512)    #64 * 14 * 14 = 12544\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, action_count)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, img_array):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        \n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #fc layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model, max_frames=5000):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name, reward_frame_shift)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    \n",
    "    while not done:\n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        atari_frame.action_array = action_array\n",
    "        current_action = np.argmax(action_array)\n",
    "        #print(\"{} - {}\".format(current_action, output.detach().cpu().numpy()[0]))\n",
    "        done = atari_frame.done_bool\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env\n",
    "\n",
    "# def get_batch(atari_env, discounted_rewards, batch_size):\n",
    "#     rand_arr = np.arange(len(discounted_rewards))\n",
    "#     np.random.shuffle(rand_arr)\n",
    "    \n",
    "#     frame_batch = np.zeros(batch_size, 3, 160, 210)\n",
    "#     reward_batch = np.zeros(batch_size)\n",
    "    \n",
    "#     for i in range(batch_size):\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(action_count)\n",
    "    train_tally = np.zeros(action_count)\n",
    "    \n",
    "    rand_arr = np.arange(len(discounted_rewards))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    \n",
    "    print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "    if len(discounted_rewards) > typical_bad_game_frame_count:\n",
    "        sorted_rewards = np.sort(discounted_rewards)\n",
    "        desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "        #reward_mean_shift = discounted_rewards_mean - desired_median\n",
    "        reward_mean_shift = (discounted_rewards_mean - desired_median)/2.0\n",
    "        print(\"Shifting rewards by {}\".format(reward_mean_shift))\n",
    "        discounted_rewards = discounted_rewards + reward_mean_shift\n",
    "        print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "        \n",
    "    \n",
    "    total_loss = 0\n",
    "    for ii, reward_ii in enumerate(discounted_rewards):\n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "        i = rand_arr[ii]\n",
    "        \n",
    "        #get frame from the frame buffer and run it through the model\n",
    "        atari_frame = atari_env.frame_buffer[i]\n",
    "        reward = atari_frame.discounted_reward\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        #print(\"train output: {}\".format(output))\n",
    "        \n",
    "        #if the reward was positive, keep the same.  if not, choose lowest option\n",
    "        action_array_from_model_in_training = output.detach().cpu().numpy()[0]\n",
    "        action_array = atari_frame.action_array\n",
    "        train_action = np.argmax(action_array)\n",
    "        action_tally[train_action] += 1\n",
    "        \n",
    "        if reward < 0:\n",
    "            train_action = np.argmin(action_array)\n",
    "            #train_action = np.argsort(action_array)[1] #second highest\n",
    "            #train_action = random.randint(0,action_count-1)\n",
    "\n",
    "        if reward > -0.2 and reward < 0.2:\n",
    "            continue\n",
    "            \n",
    "        if np.argmax(train_tally) == train_action and np.sum(train_tally) != 0:\n",
    "            #keep things even to not introduce bias that will get it stuck on one action\n",
    "            continue\n",
    "\n",
    "        train_tally[train_action] += 1\n",
    "        \n",
    "        target = torch.empty(1, dtype=torch.int64)\n",
    "        target[0] = int(train_action)\n",
    "        target = target.cuda()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / np.sum(train_tally)))\n",
    "    print(\"model action_tally: {}\".format(action_tally))\n",
    "    print(\"train_tally:        {}\".format(train_tally))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "action_count = gym.make(environment_name).action_space.n\n",
    "atari_model = AtariModel(action_count)\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 993, score: -21.0\n",
      "discounted_rewards mean: -1.6099910931120096e-17\n",
      "avg loss: 0.862\n",
      "model action_tally: [  0.   0. 993.   0.   0.   0.]\n",
      "train_tally:        [  0.   0. 206.   0. 207.   0.]\n",
      "discounted_rewards mean: -1.6099910931120096e-17\n",
      "avg loss: 0.845\n",
      "model action_tally: [  0.   0. 993.   0.   0.   0.]\n",
      "train_tally:        [  0.   0. 203.   0. 203.   0.]\n",
      "discounted_rewards mean: -1.6099910931120096e-17\n",
      "avg loss: 0.776\n",
      "model action_tally: [  0.   0. 993.   0.   0.   0.]\n",
      "train_tally:        [  0.   0. 222.   0. 222.   0.]\n",
      "\n",
      "1) frames played: 1033, score: -21.0\n",
      "discounted_rewards mean: 1.1005502199575608e-16\n",
      "avg loss: 0.906\n",
      "model action_tally: [   0.    0.    0.    0. 1033.    0.]\n",
      "train_tally:        [  0. 221.   0.   0. 221.   0.]\n",
      "discounted_rewards mean: 1.1005502199575608e-16\n",
      "avg loss: 0.783\n",
      "model action_tally: [   0.    0.    0.    0. 1033.    0.]\n",
      "train_tally:        [  0. 229.   0.   0. 230.   0.]\n",
      "discounted_rewards mean: 1.1005502199575608e-16\n",
      "avg loss: 0.754\n",
      "model action_tally: [   0.    0.    0.    0. 1033.    0.]\n",
      "train_tally:        [  0. 224.   0.   0. 224.   0.]\n",
      "\n",
      "2) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 1.164\n",
      "model action_tally: [  0. 926.   0.   0.  88.   0.]\n",
      "train_tally:        [  0. 195.   0. 196.  40.   0.]\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 0.982\n",
      "model action_tally: [  0. 926.   0.   0.  88.   0.]\n",
      "train_tally:        [  0. 208.   0. 208.  40.   0.]\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 0.975\n",
      "model action_tally: [  0. 926.   0.   0.  88.   0.]\n",
      "train_tally:        [  0. 205.   0. 206.  40.   0.]\n",
      "\n",
      "3) frames played: 1005, score: -21.0\n",
      "discounted_rewards mean: -3.1903722339477136e-16\n",
      "avg loss: 0.996\n",
      "model action_tally: [   0.    0.    0. 1005.    0.    0.]\n",
      "train_tally:        [  0.   0.   0. 218.   0. 219.]\n",
      "discounted_rewards mean: -3.1903722339477136e-16\n",
      "avg loss: 0.743\n",
      "model action_tally: [   0.    0.    0. 1005.    0.    0.]\n",
      "train_tally:        [  0.   0.   0. 225.   0. 225.]\n",
      "discounted_rewards mean: -3.1903722339477136e-16\n",
      "avg loss: 0.717\n",
      "model action_tally: [   0.    0.    0. 1005.    0.    0.]\n",
      "train_tally:        [  0.   0.   0. 223.   0. 224.]\n",
      "\n",
      "4) frames played: 1013, score: -21.0\n",
      "discounted_rewards mean: 3.6123347375760277e-16\n",
      "avg loss: 1.063\n",
      "model action_tally: [   0.    0.    0.    0.    0. 1013.]\n",
      "train_tally:        [229.   0.   0.   0.   0. 230.]\n",
      "discounted_rewards mean: 3.6123347375760277e-16\n",
      "avg loss: 0.741\n",
      "model action_tally: [   0.    0.    0.    0.    0. 1013.]\n",
      "train_tally:        [215.   0.   0.   0.   0. 216.]\n",
      "discounted_rewards mean: 3.6123347375760277e-16\n",
      "avg loss: 0.699\n",
      "model action_tally: [   0.    0.    0.    0.    0. 1013.]\n",
      "train_tally:        [218.   0.   0.   0.   0. 219.]\n",
      "\n",
      "5) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: -1.0861353455898967e-16\n",
      "avg loss: 0.951\n",
      "model action_tally: [1014.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [222. 222.   0.   0.   0.   0.]\n",
      "discounted_rewards mean: -1.0861353455898967e-16\n",
      "avg loss: 0.747\n",
      "model action_tally: [1014.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [226. 226.   0.   0.   0.   0.]\n",
      "discounted_rewards mean: -1.0861353455898967e-16\n",
      "avg loss: 0.720\n",
      "model action_tally: [1014.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [213. 213.   0.   0.   0.   0.]\n",
      "\n",
      "6) frames played: 1031, score: -21.0\n",
      "discounted_rewards mean: 1.5851098857887783e-16\n",
      "avg loss: 0.962\n",
      "model action_tally: [1027.    4.    0.    0.    0.    0.]\n",
      "train_tally:        [231.   0. 231.   0.   0.   0.]\n",
      "discounted_rewards mean: 1.5851098857887783e-16\n",
      "avg loss: 0.722\n",
      "model action_tally: [1027.    4.    0.    0.    0.    0.]\n",
      "train_tally:        [220.   0. 221.   0.   0.   0.]\n",
      "discounted_rewards mean: 1.5851098857887783e-16\n",
      "avg loss: 0.707\n",
      "model action_tally: [1027.    4.    0.    0.    0.    0.]\n",
      "train_tally:        [222.   0. 223.   0.   0.   0.]\n",
      "\n",
      "7) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: 3.004261321306225e-16\n",
      "avg loss: 0.965\n",
      "model action_tally: [1017.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [234.   0.   0. 235.   0.   0.]\n",
      "discounted_rewards mean: 3.004261321306225e-16\n",
      "avg loss: 0.719\n",
      "model action_tally: [1017.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [223.   0.   0. 223.   0.   0.]\n",
      "discounted_rewards mean: 3.004261321306225e-16\n",
      "avg loss: 0.715\n",
      "model action_tally: [1017.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [234.   0.   0. 234.   0.   0.]\n",
      "\n",
      "8) frames played: 1009, score: -21.0\n",
      "discounted_rewards mean: -3.468209091792362e-16\n",
      "avg loss: 0.954\n",
      "model action_tally: [1009.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [222.   0.   0.   0. 222.   0.]\n",
      "discounted_rewards mean: -3.468209091792362e-16\n",
      "avg loss: 0.710\n",
      "model action_tally: [1009.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [212.   0.   0.   0. 213.   0.]\n",
      "discounted_rewards mean: -3.468209091792362e-16\n",
      "avg loss: 0.694\n",
      "model action_tally: [1009.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [212.   0.   0.   0. 212.   0.]\n",
      "\n",
      "9) frames played: 1031, score: -21.0\n",
      "discounted_rewards mean: 1.8607811702737832e-16\n",
      "avg loss: 0.924\n",
      "model action_tally: [968.   0.   0.   0.  63.   0.]\n",
      "train_tally:        [236. 237.   0.   0.   0.   0.]\n",
      "discounted_rewards mean: 1.8607811702737832e-16\n",
      "avg loss: 0.691\n",
      "model action_tally: [968.   0.   0.   0.  63.   0.]\n",
      "train_tally:        [233. 233.   0.   0.   0.   0.]\n",
      "discounted_rewards mean: 1.8607811702737832e-16\n",
      "avg loss: 0.664\n",
      "model action_tally: [968.   0.   0.   0.  63.   0.]\n",
      "train_tally:        [237. 238.   0.   0.   0.   0.]\n",
      "\n",
      "10) frames played: 1015, score: -21.0\n",
      "discounted_rewards mean: -1.400084208394286e-16\n",
      "avg loss: 0.957\n",
      "model action_tally: [955.  60.   0.   0.   0.   0.]\n",
      "train_tally:        [229.   0.   0.   0.   0. 230.]\n",
      "discounted_rewards mean: -1.400084208394286e-16\n",
      "avg loss: 0.672\n",
      "model action_tally: [955.  60.   0.   0.   0.   0.]\n",
      "train_tally:        [233.   0.   0.   0.   0. 234.]\n",
      "discounted_rewards mean: -1.400084208394286e-16\n",
      "avg loss: 0.660\n",
      "model action_tally: [955.  60.   0.   0.   0.   0.]\n",
      "train_tally:        [227.   0.   0.   0.   0. 227.]\n",
      "\n",
      "11) frames played: 1119, score: -20.0\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.011660833840092102\n",
      "new discounted_rewards mean: 0.011660833840092212\n",
      "avg loss: 0.955\n",
      "model action_tally: [1074.    0.    0.    0.    0.   45.]\n",
      "train_tally:        [228.   0. 228.   0.   0.  13.]\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.011660833840092102\n",
      "new discounted_rewards mean: 0.011660833840092212\n",
      "avg loss: 0.766\n",
      "model action_tally: [1074.    0.    0.    0.    0.   45.]\n",
      "train_tally:        [249.   0. 250.   0.   0.  13.]\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.011660833840092102\n",
      "new discounted_rewards mean: 0.011660833840092212\n",
      "avg loss: 0.737\n",
      "model action_tally: [1074.    0.    0.    0.    0.   45.]\n",
      "train_tally:        [232.   0. 232.   0.   0.  13.]\n",
      "\n",
      "12) frames played: 1030, score: -21.0\n",
      "discounted_rewards mean: 1.5176640957982722e-16\n",
      "avg loss: 0.847\n",
      "model action_tally: [1030.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [212.   0.   0. 212.   0.   0.]\n",
      "discounted_rewards mean: 1.5176640957982722e-16\n",
      "avg loss: 0.620\n",
      "model action_tally: [1030.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [213.   0.   0. 213.   0.   0.]\n",
      "discounted_rewards mean: 1.5176640957982722e-16\n",
      "avg loss: 0.597\n",
      "model action_tally: [1030.    0.    0.    0.    0.    0.]\n",
      "train_tally:        [224.   0.   0. 224.   0.   0.]\n",
      "\n",
      "13) frames played: 1036, score: -21.0\n",
      "discounted_rewards mean: 2.400482215405744e-17\n",
      "avg loss: 1.075\n",
      "model action_tally: [700.   0.   0. 336.   0.   0.]\n",
      "train_tally:        [219.   0.   0.  62. 220.   0.]\n",
      "discounted_rewards mean: 2.400482215405744e-17\n",
      "avg loss: 0.838\n",
      "model action_tally: [700.   0.   0. 336.   0.   0.]\n",
      "train_tally:        [209.   0.   0.  62. 210.   0.]\n",
      "discounted_rewards mean: 2.400482215405744e-17\n",
      "avg loss: 0.785\n",
      "model action_tally: [700.   0.   0. 336.   0.   0.]\n",
      "train_tally:        [221.   0.   0.  62. 222.   0.]\n",
      "\n",
      "14) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: -1.3987061727561027e-17\n",
      "avg loss: 0.984\n",
      "model action_tally: [723.   0.   0.   0. 293.   0.]\n",
      "train_tally:        [200. 201.   0.   0.  66.   0.]\n",
      "discounted_rewards mean: -1.3987061727561027e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.781\n",
      "model action_tally: [723.   0.   0.   0. 293.   0.]\n",
      "train_tally:        [212. 212.   0.   0.  66.   0.]\n",
      "discounted_rewards mean: -1.3987061727561027e-17\n",
      "avg loss: 0.728\n",
      "model action_tally: [723.   0.   0.   0. 293.   0.]\n",
      "train_tally:        [205. 205.   0.   0.  66.   0.]\n",
      "\n",
      "15) frames played: 1029, score: -21.0\n",
      "discounted_rewards mean: -8.631471522838924e-17\n",
      "avg loss: 0.926\n",
      "model action_tally: [701. 328.   0.   0.   0.   0.]\n",
      "train_tally:        [212.  69.   0.   0.   0. 212.]\n",
      "discounted_rewards mean: -8.631471522838924e-17\n",
      "avg loss: 0.762\n",
      "model action_tally: [701. 328.   0.   0.   0.   0.]\n",
      "train_tally:        [198.  68.   0.   0.   0. 199.]\n",
      "discounted_rewards mean: -8.631471522838924e-17\n",
      "avg loss: 0.717\n",
      "model action_tally: [701. 328.   0.   0.   0.   0.]\n",
      "train_tally:        [208.  69.   0.   0.   0. 209.]\n",
      "\n",
      "16) frames played: 1006, score: -21.0\n",
      "discounted_rewards mean: -1.0594573594832507e-17\n",
      "avg loss: 1.360\n",
      "model action_tally: [484.   0.   0.   0.   0. 522.]\n",
      "train_tally:        [150.   0. 149. 150.   0. 145.]\n",
      "discounted_rewards mean: -1.0594573594832507e-17\n",
      "avg loss: 1.133\n",
      "model action_tally: [484.   0.   0.   0.   0. 522.]\n",
      "train_tally:        [149.   0. 150. 148.   0. 146.]\n",
      "discounted_rewards mean: -1.0594573594832507e-17\n",
      "avg loss: 1.019\n",
      "model action_tally: [484.   0.   0.   0.   0. 522.]\n",
      "train_tally:        [149.   0. 150. 148.   0. 140.]\n",
      "\n",
      "17) frames played: 1012, score: -21.0\n",
      "discounted_rewards mean: -4.633974363652827e-16\n",
      "avg loss: 1.109\n",
      "model action_tally: [436.   0. 315. 256.   0.   5.]\n",
      "train_tally:        [184.   0.  84.  96. 184.   4.]\n",
      "discounted_rewards mean: -4.633974363652827e-16\n",
      "avg loss: 0.903\n",
      "model action_tally: [436.   0. 315. 256.   0.   5.]\n",
      "train_tally:        [177.   0.  84.  96. 178.   4.]\n",
      "discounted_rewards mean: -4.633974363652827e-16\n",
      "avg loss: 0.877\n",
      "model action_tally: [436.   0. 315. 256.   0.   5.]\n",
      "train_tally:        [175.   0.  84.  96. 176.   4.]\n",
      "\n",
      "18) frames played: 1032, score: -21.0\n",
      "discounted_rewards mean: 4.716296259647952e-16\n",
      "avg loss: 1.267\n",
      "model action_tally: [421.   0. 169.   4. 438.   0.]\n",
      "train_tally:        [176. 177.  62.   4. 138.   0.]\n",
      "discounted_rewards mean: 4.716296259647952e-16\n",
      "avg loss: 1.113\n",
      "model action_tally: [421.   0. 169.   4. 438.   0.]\n",
      "train_tally:        [167. 167.  62.   4. 138.   0.]\n",
      "discounted_rewards mean: 4.716296259647952e-16\n",
      "avg loss: 1.003\n",
      "model action_tally: [421.   0. 169.   4. 438.   0.]\n",
      "train_tally:        [179. 179.  62.   4. 138.   0.]\n",
      "\n",
      "19) frames played: 1024, score: -21.0\n",
      "discounted_rewards mean: 2.0469737016526324e-16\n",
      "avg loss: 1.074\n",
      "model action_tally: [488. 202. 141.   0. 193.   0.]\n",
      "train_tally:        [176.  35.  52.   0.  89. 176.]\n",
      "discounted_rewards mean: 2.0469737016526324e-16\n",
      "avg loss: 0.838\n",
      "model action_tally: [488. 202. 141.   0. 193.   0.]\n",
      "train_tally:        [191.  35.  52.   0.  87. 192.]\n",
      "discounted_rewards mean: 2.0469737016526324e-16\n",
      "avg loss: 0.829\n",
      "model action_tally: [488. 202. 141.   0. 193.   0.]\n",
      "train_tally:        [176.  35.  52.   0.  89. 177.]\n",
      "\n",
      "20) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: -2.639357180731555e-16\n",
      "avg loss: 0.950\n",
      "model action_tally: [412.   0.   2.   0. 261. 348.]\n",
      "train_tally:        [174.   0.   5. 175. 116.  89.]\n",
      "discounted_rewards mean: -2.639357180731555e-16\n",
      "avg loss: 0.803\n",
      "model action_tally: [412.   0.   2.   0. 261. 348.]\n",
      "train_tally:        [178.   0.   5. 179. 117.  89.]\n",
      "discounted_rewards mean: -2.639357180731555e-16\n",
      "avg loss: 0.752\n",
      "model action_tally: [412.   0.   2.   0. 261. 348.]\n",
      "train_tally:        [175.   0.   5. 176. 117.  89.]\n",
      "\n",
      "21) frames played: 1013, score: -21.0\n",
      "discounted_rewards mean: 2.630340828332059e-16\n",
      "avg loss: 1.299\n",
      "model action_tally: [432.   0.   0. 258. 222. 101.]\n",
      "train_tally:        [144. 144. 144.  36. 108.  60.]\n",
      "discounted_rewards mean: 2.630340828332059e-16\n",
      "avg loss: 1.056\n",
      "model action_tally: [432.   0.   0. 258. 222. 101.]\n",
      "train_tally:        [144. 144. 140.  36. 109.  60.]\n",
      "discounted_rewards mean: 2.630340828332059e-16\n",
      "avg loss: 0.915\n",
      "model action_tally: [432.   0.   0. 258. 222. 101.]\n",
      "train_tally:        [145. 145. 145.  36. 106.  60.]\n",
      "\n",
      "22) frames played: 1025, score: -21.0\n",
      "discounted_rewards mean: 4.2285957933040107e-16\n",
      "avg loss: 1.493\n",
      "model action_tally: [383. 312. 330.   0.   0.   0.]\n",
      "train_tally:        [109.  83. 110.  17.  53.   3.]\n",
      "discounted_rewards mean: 4.2285957933040107e-16\n",
      "avg loss: 1.349\n",
      "model action_tally: [383. 312. 330.   0.   0.   0.]\n",
      "train_tally:        [108.  85. 108.  17.  53.   3.]\n",
      "discounted_rewards mean: 4.2285957933040107e-16\n",
      "avg loss: 1.238\n",
      "model action_tally: [383. 312. 330.   0.   0.   0.]\n",
      "train_tally:        [107.  88. 107.  17.  53.   3.]\n",
      "\n",
      "23) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: -3.1633751934525007e-16\n",
      "avg loss: 1.032\n",
      "model action_tally: [372. 287. 363.   0.   0.   0.]\n",
      "train_tally:        [153.  87. 139.  38.   0. 154.]\n",
      "discounted_rewards mean: -3.1633751934525007e-16\n",
      "avg loss: 0.885\n",
      "model action_tally: [372. 287. 363.   0.   0.   0.]\n",
      "train_tally:        [148.  87. 137.  38.   0. 148.]\n",
      "discounted_rewards mean: -3.1633751934525007e-16\n",
      "avg loss: 0.805\n",
      "model action_tally: [372. 287. 363.   0.   0.   0.]\n",
      "train_tally:        [153.  87. 138.  38.   0. 154.]\n",
      "\n",
      "24) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: -2.1330269134530566e-16\n",
      "avg loss: 1.097\n",
      "model action_tally: [416.   0. 340.   0.   0. 260.]\n",
      "train_tally:        [177.  28. 120.   0. 178.  87.]\n",
      "discounted_rewards mean: -2.1330269134530566e-16\n",
      "avg loss: 0.945\n",
      "model action_tally: [416.   0. 340.   0.   0. 260.]\n",
      "train_tally:        [160.  28. 120.   0. 160.  87.]\n",
      "discounted_rewards mean: -2.1330269134530566e-16\n",
      "avg loss: 0.913\n",
      "model action_tally: [416.   0. 340.   0.   0. 260.]\n",
      "train_tally:        [164.  28. 120.   0. 165.  86.]\n",
      "\n",
      "25) frames played: 1015, score: -21.0\n",
      "discounted_rewards mean: -4.900294729380001e-17\n",
      "avg loss: 1.185\n",
      "model action_tally: [400.   0. 265.   0. 160. 190.]\n",
      "train_tally:        [176.   0. 107. 176.  39.  69.]\n",
      "discounted_rewards mean: -4.900294729380001e-17\n",
      "avg loss: 0.929\n",
      "model action_tally: [400.   0. 265.   0. 160. 190.]\n",
      "train_tally:        [160.   0. 106. 160.  39.  69.]\n",
      "discounted_rewards mean: -4.900294729380001e-17\n",
      "avg loss: 0.830\n",
      "model action_tally: [400.   0. 265.   0. 160. 190.]\n",
      "train_tally:        [170.   0. 106. 171.  39.  69.]\n",
      "\n",
      "26) frames played: 1137, score: -21.0\n",
      "discounted_rewards mean: -2.499710591944064e-16\n",
      "Shifting rewards by -0.012756370826980354\n",
      "new discounted_rewards mean: -0.012756370826980633\n",
      "avg loss: 1.190\n",
      "model action_tally: [501.   0. 219. 238.   0. 179.]\n",
      "train_tally:        [176. 177. 169.  55.   0.  90.]\n",
      "discounted_rewards mean: -2.499710591944064e-16\n",
      "Shifting rewards by -0.012756370826980354\n",
      "new discounted_rewards mean: -0.012756370826980633\n",
      "avg loss: 0.978\n",
      "model action_tally: [501.   0. 219. 238.   0. 179.]\n",
      "train_tally:        [169. 170. 169.  55.   0.  90.]\n",
      "discounted_rewards mean: -2.499710591944064e-16\n",
      "Shifting rewards by -0.012756370826980354\n",
      "new discounted_rewards mean: -0.012756370826980633\n",
      "avg loss: 0.865\n",
      "model action_tally: [501.   0. 219. 238.   0. 179.]\n",
      "train_tally:        [173. 173. 167.  55.   0.  90.]\n",
      "\n",
      "27) frames played: 1221, score: -19.0\n",
      "discounted_rewards mean: 4.655480660180837e-17\n",
      "Shifting rewards by 0.08957973172153068\n",
      "new discounted_rewards mean: 0.08957973172153073\n",
      "avg loss: 1.139\n",
      "model action_tally: [941. 228.  20.   0.   0.  32.]\n",
      "train_tally:        [194. 121.  14.   0. 195.  20.]\n",
      "discounted_rewards mean: 4.655480660180837e-17\n",
      "Shifting rewards by 0.08957973172153068\n",
      "new discounted_rewards mean: 0.08957973172153073\n",
      "avg loss: 0.914\n",
      "model action_tally: [941. 228.  20.   0.   0.  32.]\n",
      "train_tally:        [195. 120.  14.   0. 196.  20.]\n",
      "discounted_rewards mean: 4.655480660180837e-17\n",
      "Shifting rewards by 0.08957973172153068\n",
      "new discounted_rewards mean: 0.08957973172153073\n",
      "avg loss: 0.824\n",
      "model action_tally: [941. 228.  20.   0.   0.  32.]\n",
      "train_tally:        [186. 121.  14.   0. 187.  20.]\n",
      "\n",
      "28) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: 1.5416114582566276e-16\n",
      "avg loss: 1.400\n",
      "model action_tally: [362. 302.   0.   0. 241. 109.]\n",
      "train_tally:        [150.  98.   0. 151.  78. 102.]\n",
      "discounted_rewards mean: 1.5416114582566276e-16\n",
      "avg loss: 1.240\n",
      "model action_tally: [362. 302.   0.   0. 241. 109.]\n",
      "train_tally:        [151.  97.   0. 151.  78. 102.]\n",
      "discounted_rewards mean: 1.5416114582566276e-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.161\n",
      "model action_tally: [362. 302.   0.   0. 241. 109.]\n",
      "train_tally:        [134.  97.   0. 134.  78. 102.]\n",
      "\n",
      "29) frames played: 1046, score: -21.0\n",
      "discounted_rewards mean: 2.5473568442642216e-16\n",
      "avg loss: 1.068\n",
      "model action_tally: [550. 236.   0.  88.  33. 139.]\n",
      "train_tally:        [197.  87. 197.  12.  19.  64.]\n",
      "discounted_rewards mean: 2.5473568442642216e-16\n",
      "avg loss: 0.866\n",
      "model action_tally: [550. 236.   0.  88.  33. 139.]\n",
      "train_tally:        [188.  87. 188.  12.  19.  64.]\n",
      "discounted_rewards mean: 2.5473568442642216e-16\n",
      "avg loss: 0.805\n",
      "model action_tally: [550. 236.   0.  88.  33. 139.]\n",
      "train_tally:        [185.  87. 185.  12.  19.  64.]\n",
      "\n",
      "30) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: -1.0130648641614014e-16\n",
      "avg loss: 1.549\n",
      "model action_tally: [357. 262. 273.   0.   0. 125.]\n",
      "train_tally:        [103. 103.  67.   8.  73.  89.]\n",
      "discounted_rewards mean: -1.0130648641614014e-16\n",
      "avg loss: 1.338\n",
      "model action_tally: [357. 262. 273.   0.   0. 125.]\n",
      "train_tally:        [105. 105.  67.   8.  73.  87.]\n",
      "discounted_rewards mean: -1.0130648641614014e-16\n",
      "avg loss: 1.255\n",
      "model action_tally: [357. 262. 273.   0.   0. 125.]\n",
      "train_tally:        [102. 102.  67.   8.  73.  87.]\n",
      "\n",
      "31) frames played: 1193, score: -21.0\n",
      "discounted_rewards mean: 2.1441356653280475e-16\n",
      "Shifting rewards by 0.02138173751174719\n",
      "new discounted_rewards mean: 0.02138173751174739\n",
      "avg loss: 0.931\n",
      "model action_tally: [536. 360.   1.   0. 188. 108.]\n",
      "train_tally:        [184. 153.   1. 185.  41.  61.]\n",
      "discounted_rewards mean: 2.1441356653280475e-16\n",
      "Shifting rewards by 0.02138173751174719\n",
      "new discounted_rewards mean: 0.02138173751174739\n",
      "avg loss: 0.735\n",
      "model action_tally: [536. 360.   1.   0. 188. 108.]\n",
      "train_tally:        [188. 158.   1. 189.  41.  61.]\n",
      "discounted_rewards mean: 2.1441356653280475e-16\n",
      "Shifting rewards by 0.02138173751174719\n",
      "new discounted_rewards mean: 0.02138173751174739\n",
      "avg loss: 0.748\n",
      "model action_tally: [536. 360.   1.   0. 188. 108.]\n",
      "train_tally:        [185. 157.   1. 186.  41.  61.]\n",
      "\n",
      "32) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: -1.6259959258289695e-16\n",
      "avg loss: 0.828\n",
      "model action_tally: [363. 272.   0. 248.   0. 133.]\n",
      "train_tally:        [169. 108. 169.  59.   0.  63.]\n",
      "discounted_rewards mean: -1.6259959258289695e-16\n",
      "avg loss: 0.681\n",
      "model action_tally: [363. 272.   0. 248.   0. 133.]\n",
      "train_tally:        [165. 108. 166.  59.   0.  63.]\n",
      "discounted_rewards mean: -1.6259959258289695e-16\n",
      "avg loss: 0.631\n",
      "model action_tally: [363. 272.   0. 248.   0. 133.]\n",
      "train_tally:        [162. 108. 163.  59.   0.  63.]\n",
      "\n",
      "33) frames played: 1128, score: -20.0\n",
      "discounted_rewards mean: 7.558965274043619e-17\n",
      "Shifting rewards by 0.03144709049809456\n",
      "new discounted_rewards mean: 0.03144709049809466\n",
      "avg loss: 0.906\n",
      "model action_tally: [1032.   16.   66.    2.    0.   12.]\n",
      "train_tally:        [224.  16.  12.   2. 225.   9.]\n",
      "discounted_rewards mean: 7.558965274043619e-17\n",
      "Shifting rewards by 0.03144709049809456\n",
      "new discounted_rewards mean: 0.03144709049809466\n",
      "avg loss: 0.713\n",
      "model action_tally: [1032.   16.   66.    2.    0.   12.]\n",
      "train_tally:        [244.  16.  12.   2. 244.   9.]\n",
      "discounted_rewards mean: 7.558965274043619e-17\n",
      "Shifting rewards by 0.03144709049809456\n",
      "new discounted_rewards mean: 0.03144709049809466\n",
      "avg loss: 0.627\n",
      "model action_tally: [1032.   16.   66.    2.    0.   12.]\n",
      "train_tally:        [217.  16.  12.   2. 218.   9.]\n",
      "\n",
      "34) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: 2.515195524814514e-16\n",
      "avg loss: 1.270\n",
      "model action_tally: [371. 237.   0.   0. 307. 102.]\n",
      "train_tally:        [168. 169.   0.   0.  62. 163.]\n",
      "discounted_rewards mean: 2.515195524814514e-16\n",
      "avg loss: 1.028\n",
      "model action_tally: [371. 237.   0.   0. 307. 102.]\n",
      "train_tally:        [174. 173.   0.   0.  62. 174.]\n",
      "discounted_rewards mean: 2.515195524814514e-16\n",
      "avg loss: 0.962\n",
      "model action_tally: [371. 237.   0.   0. 307. 102.]\n",
      "train_tally:        [166. 167.   0.   0.  62. 162.]\n",
      "\n",
      "35) frames played: 1028, score: -21.0\n",
      "discounted_rewards mean: 1.209581505428186e-16\n",
      "avg loss: 1.135\n",
      "model action_tally: [410. 301.   0.   0.   0. 317.]\n",
      "train_tally:        [156. 125.  88. 156.   0.  87.]\n",
      "discounted_rewards mean: 1.209581505428186e-16\n",
      "avg loss: 0.905\n",
      "model action_tally: [410. 301.   0.   0.   0. 317.]\n",
      "train_tally:        [153. 124.  88. 154.   0.  87.]\n",
      "discounted_rewards mean: 1.209581505428186e-16\n",
      "avg loss: 0.796\n",
      "model action_tally: [410. 301.   0.   0.   0. 317.]\n",
      "train_tally:        [140. 125.  88. 140.   0.  87.]\n",
      "\n",
      "36) frames played: 1266, score: -21.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.061653997010163344\n",
      "new discounted_rewards mean: 0.06165399701016329\n",
      "avg loss: 1.443\n",
      "model action_tally: [983. 117.  49.  79.   0.  38.]\n",
      "train_tally:        [201.  76.  27.  46. 163. 202.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.061653997010163344\n",
      "new discounted_rewards mean: 0.06165399701016329\n",
      "avg loss: 1.154\n",
      "model action_tally: [983. 117.  49.  79.   0.  38.]\n",
      "train_tally:        [183.  76.  27.  46. 161. 183.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.061653997010163344\n",
      "new discounted_rewards mean: 0.06165399701016329\n",
      "avg loss: 1.079\n",
      "model action_tally: [983. 117.  49.  79.   0.  38.]\n",
      "train_tally:        [194.  76.  27.  46. 160. 195.]\n",
      "\n",
      "37) frames played: 1056, score: -21.0\n",
      "discounted_rewards mean: -3.061524098208765e-16\n",
      "Shifting rewards by -0.08257684974550321\n",
      "new discounted_rewards mean: -0.08257684974550351\n",
      "avg loss: 1.558\n",
      "model action_tally: [397. 202.   0.  68. 253. 136.]\n",
      "train_tally:        [131. 132.  75. 119.  66.  74.]\n",
      "discounted_rewards mean: -3.061524098208765e-16\n",
      "Shifting rewards by -0.08257684974550321\n",
      "new discounted_rewards mean: -0.08257684974550351\n",
      "avg loss: 1.336\n",
      "model action_tally: [397. 202.   0.  68. 253. 136.]\n",
      "train_tally:        [128. 128.  74. 129.  66.  74.]\n",
      "discounted_rewards mean: -3.061524098208765e-16\n",
      "Shifting rewards by -0.08257684974550321\n",
      "new discounted_rewards mean: -0.08257684974550351\n",
      "avg loss: 1.257\n",
      "model action_tally: [397. 202.   0.  68. 253. 136.]\n",
      "train_tally:        [138. 138.  74. 127.  66.  74.]\n",
      "\n",
      "38) frames played: 1018, score: -21.0\n",
      "discounted_rewards mean: 1.4832056124658279e-16\n",
      "avg loss: 1.084\n",
      "model action_tally: [351. 247.   0. 346.   0.  74.]\n",
      "train_tally:        [162.  97.   0.  79.  89. 162.]\n",
      "discounted_rewards mean: 1.4832056124658279e-16\n",
      "avg loss: 0.820\n",
      "model action_tally: [351. 247.   0. 346.   0.  74.]\n",
      "train_tally:        [166.  99.   0.  79.  87. 166.]\n",
      "discounted_rewards mean: 1.4832056124658279e-16\n",
      "avg loss: 0.753\n",
      "model action_tally: [351. 247.   0. 346.   0.  74.]\n",
      "train_tally:        [167.  99.   0.  79.  89. 168.]\n",
      "\n",
      "39) frames played: 1083, score: -21.0\n",
      "discounted_rewards mean: 1.4433924456807206e-16\n",
      "Shifting rewards by -0.06215894726147366\n",
      "new discounted_rewards mean: -0.062158947261473495\n",
      "avg loss: 0.955\n",
      "model action_tally: [946.  29.   0.   9.  19.  80.]\n",
      "train_tally:        [222.  21. 222.   8.  14.  34.]\n",
      "discounted_rewards mean: 1.4433924456807206e-16\n",
      "Shifting rewards by -0.06215894726147366\n",
      "new discounted_rewards mean: -0.062158947261473495\n",
      "avg loss: 0.753\n",
      "model action_tally: [946.  29.   0.   9.  19.  80.]\n",
      "train_tally:        [219.  21. 219.   8.  14.  34.]\n",
      "discounted_rewards mean: 1.4433924456807206e-16\n",
      "Shifting rewards by -0.06215894726147366\n",
      "new discounted_rewards mean: -0.062158947261473495\n",
      "avg loss: 0.682\n",
      "model action_tally: [946.  29.   0.   9.  19.  80.]\n",
      "train_tally:        [218.  21. 218.   8.  14.  34.]\n",
      "\n",
      "40) frames played: 1018, score: -21.0\n",
      "discounted_rewards mean: 1.3959582234972498e-16\n",
      "avg loss: 1.241\n",
      "model action_tally: [255. 143. 453.   0. 138.  29.]\n",
      "train_tally:        [136. 132. 136.   0.  53.  10.]\n",
      "discounted_rewards mean: 1.3959582234972498e-16\n",
      "avg loss: 1.111\n",
      "model action_tally: [255. 143. 453.   0. 138.  29.]\n",
      "train_tally:        [136. 131. 136.   0.  53.  10.]\n",
      "discounted_rewards mean: 1.3959582234972498e-16\n",
      "avg loss: 1.007\n",
      "model action_tally: [255. 143. 453.   0. 138.  29.]\n",
      "train_tally:        [143. 132. 143.   0.  53.  10.]\n",
      "\n",
      "41) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: -2.72479515188239e-16\n",
      "avg loss: 1.186\n",
      "model action_tally: [385. 151. 359.   0. 122.   0.]\n",
      "train_tally:        [140.  82. 137. 141.  59. 139.]\n",
      "discounted_rewards mean: -2.72479515188239e-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.909\n",
      "model action_tally: [385. 151. 359.   0. 122.   0.]\n",
      "train_tally:        [144.  81. 141. 144.  59. 144.]\n",
      "discounted_rewards mean: -2.72479515188239e-16\n",
      "avg loss: 0.769\n",
      "model action_tally: [385. 151. 359.   0. 122.   0.]\n",
      "train_tally:        [138.  82. 139. 139.  59. 140.]\n",
      "\n",
      "42) frames played: 1021, score: -21.0\n",
      "discounted_rewards mean: 1.0612905700628332e-16\n",
      "avg loss: 1.300\n",
      "model action_tally: [264. 151. 285. 123.  39. 159.]\n",
      "train_tally:        [154. 155. 130.   4. 154.  45.]\n",
      "discounted_rewards mean: 1.0612905700628332e-16\n",
      "avg loss: 1.027\n",
      "model action_tally: [264. 151. 285. 123.  39. 159.]\n",
      "train_tally:        [150. 150. 136.   4. 151.  46.]\n",
      "discounted_rewards mean: 1.0612905700628332e-16\n",
      "avg loss: 0.929\n",
      "model action_tally: [264. 151. 285. 123.  39. 159.]\n",
      "train_tally:        [154. 155. 134.   4. 152.  46.]\n",
      "\n",
      "43) frames played: 1000, score: -21.0\n",
      "discounted_rewards mean: 1.9184653865522705e-16\n",
      "avg loss: 1.146\n",
      "model action_tally: [277. 227. 169.   0. 285.  42.]\n",
      "train_tally:        [130.  69. 130. 131.  83. 129.]\n",
      "discounted_rewards mean: 1.9184653865522705e-16\n",
      "avg loss: 0.841\n",
      "model action_tally: [277. 227. 169.   0. 285.  42.]\n",
      "train_tally:        [131.  69. 132. 132.  84. 128.]\n",
      "discounted_rewards mean: 1.9184653865522705e-16\n",
      "avg loss: 0.741\n",
      "model action_tally: [277. 227. 169.   0. 285.  42.]\n",
      "train_tally:        [129.  69. 129. 130.  84. 126.]\n",
      "\n",
      "44) frames played: 1105, score: -21.0\n",
      "discounted_rewards mean: -2.5721004009415394e-16\n",
      "Shifting rewards by -0.037218395119593885\n",
      "new discounted_rewards mean: -0.03721839511959418\n",
      "avg loss: 1.464\n",
      "model action_tally: [514.  74. 360.  66.  28.  63.]\n",
      "train_tally:        [186. 187.  84.  13.  17.  68.]\n",
      "discounted_rewards mean: -2.5721004009415394e-16\n",
      "Shifting rewards by -0.037218395119593885\n",
      "new discounted_rewards mean: -0.03721839511959418\n",
      "avg loss: 1.225\n",
      "model action_tally: [514.  74. 360.  66.  28.  63.]\n",
      "train_tally:        [173. 173.  84.  13.  17.  68.]\n",
      "discounted_rewards mean: -2.5721004009415394e-16\n",
      "Shifting rewards by -0.037218395119593885\n",
      "new discounted_rewards mean: -0.03721839511959418\n",
      "avg loss: 1.091\n",
      "model action_tally: [514.  74. 360.  66.  28.  63.]\n",
      "train_tally:        [188. 188.  84.  13.  17.  68.]\n",
      "\n",
      "45) frames played: 1202, score: -20.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.04695506017847132\n",
      "new discounted_rewards mean: 0.04695506017847133\n",
      "avg loss: 1.003\n",
      "model action_tally: [886. 156.  57.   0.   1. 102.]\n",
      "train_tally:        [201.  78.  39. 201.  29.  79.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.04695506017847132\n",
      "new discounted_rewards mean: 0.04695506017847133\n",
      "avg loss: 0.786\n",
      "model action_tally: [886. 156.  57.   0.   1. 102.]\n",
      "train_tally:        [203.  78.  39. 204.  29.  79.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.04695506017847132\n",
      "new discounted_rewards mean: 0.04695506017847133\n",
      "avg loss: 0.662\n",
      "model action_tally: [886. 156.  57.   0.   1. 102.]\n",
      "train_tally:        [190.  78.  39. 190.  29.  79.]\n",
      "\n",
      "46) frames played: 1122, score: -20.0\n",
      "discounted_rewards mean: -1.013251673098182e-16\n",
      "Shifting rewards by 0.007691915243366838\n",
      "new discounted_rewards mean: 0.007691915243366695\n",
      "avg loss: 1.237\n",
      "model action_tally: [168. 140.  17. 692.  56.  49.]\n",
      "train_tally:        [180.  70.  35. 181.  11. 182.]\n",
      "discounted_rewards mean: -1.013251673098182e-16\n",
      "Shifting rewards by 0.007691915243366838\n",
      "new discounted_rewards mean: 0.007691915243366695\n",
      "avg loss: 1.034\n",
      "model action_tally: [168. 140.  17. 692.  56.  49.]\n",
      "train_tally:        [176.  69.  35. 174.  11. 176.]\n",
      "discounted_rewards mean: -1.013251673098182e-16\n",
      "Shifting rewards by 0.007691915243366838\n",
      "new discounted_rewards mean: 0.007691915243366695\n",
      "avg loss: 0.995\n",
      "model action_tally: [168. 140.  17. 692.  56.  49.]\n",
      "train_tally:        [181.  70.  35. 184.  11. 185.]\n",
      "\n",
      "47) frames played: 1226, score: -20.0\n",
      "discounted_rewards mean: 4.6364941974557926e-17\n",
      "Shifting rewards by 0.07919107406155888\n",
      "new discounted_rewards mean: 0.07919107406155894\n",
      "avg loss: 1.022\n",
      "model action_tally: [208.  43.   6. 565.   0. 404.]\n",
      "train_tally:        [174. 175.   5. 175.  22.  47.]\n",
      "discounted_rewards mean: 4.6364941974557926e-17\n",
      "Shifting rewards by 0.07919107406155888\n",
      "new discounted_rewards mean: 0.07919107406155894\n",
      "avg loss: 0.809\n",
      "model action_tally: [208.  43.   6. 565.   0. 404.]\n",
      "train_tally:        [175. 175.   5. 175.  22.  47.]\n",
      "discounted_rewards mean: 4.6364941974557926e-17\n",
      "Shifting rewards by 0.07919107406155888\n",
      "new discounted_rewards mean: 0.07919107406155894\n",
      "avg loss: 0.676\n",
      "model action_tally: [208.  43.   6. 565.   0. 404.]\n",
      "train_tally:        [169. 178.   5. 178.  22.  47.]\n",
      "\n",
      "48) frames played: 1096, score: -21.0\n",
      "discounted_rewards mean: 1.4262719148469164e-16\n",
      "Shifting rewards by -0.038593599141451884\n",
      "new discounted_rewards mean: -0.03859359914145175\n",
      "avg loss: 1.394\n",
      "model action_tally: [859. 231.   0.   0.   1.   5.]\n",
      "train_tally:        [155.  47.  55. 156.   0. 144.]\n",
      "discounted_rewards mean: 1.4262719148469164e-16\n",
      "Shifting rewards by -0.038593599141451884\n",
      "new discounted_rewards mean: -0.03859359914145175\n",
      "avg loss: 1.166\n",
      "model action_tally: [859. 231.   0.   0.   1.   5.]\n",
      "train_tally:        [160.  47.  55. 161.   0. 142.]\n",
      "discounted_rewards mean: 1.4262719148469164e-16\n",
      "Shifting rewards by -0.038593599141451884\n",
      "new discounted_rewards mean: -0.03859359914145175\n",
      "avg loss: 1.038\n",
      "model action_tally: [859. 231.   0.   0.   1.   5.]\n",
      "train_tally:        [170.  47.  55. 170.   0. 140.]\n",
      "\n",
      "49) frames played: 1025, score: -21.0\n",
      "discounted_rewards mean: 1.0398186376977076e-16\n",
      "avg loss: 1.021\n",
      "model action_tally: [256. 301.   0. 244.   0. 224.]\n",
      "train_tally:        [142. 135.   4.  41. 142. 128.]\n",
      "discounted_rewards mean: 1.0398186376977076e-16\n",
      "avg loss: 0.830\n",
      "model action_tally: [256. 301.   0. 244.   0. 224.]\n",
      "train_tally:        [150. 135.   4.  41. 150. 131.]\n",
      "discounted_rewards mean: 1.0398186376977076e-16\n",
      "avg loss: 0.778\n",
      "model action_tally: [256. 301.   0. 244.   0. 224.]\n",
      "train_tally:        [148. 131.   4.  41. 148. 130.]\n",
      "\n",
      "50) frames played: 1025, score: -21.0\n",
      "discounted_rewards mean: 1.490406714033381e-16\n",
      "avg loss: 1.094\n",
      "model action_tally: [315. 282.   0.  42. 264. 122.]\n",
      "train_tally:        [143. 145. 145.  22.  44. 120.]\n",
      "discounted_rewards mean: 1.490406714033381e-16\n",
      "avg loss: 0.876\n",
      "model action_tally: [315. 282.   0.  42. 264. 122.]\n",
      "train_tally:        [153. 151. 153.  22.  44. 118.]\n",
      "discounted_rewards mean: 1.490406714033381e-16\n",
      "avg loss: 0.803\n",
      "model action_tally: [315. 282.   0.  42. 264. 122.]\n",
      "train_tally:        [146. 147. 147.  22.  44. 120.]\n",
      "\n",
      "51) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: 1.7117302877209885e-16\n",
      "avg loss: 1.228\n",
      "model action_tally: [282. 251. 343.   0.  20. 121.]\n",
      "train_tally:        [122. 122.  38.  87.  28. 115.]\n",
      "discounted_rewards mean: 1.7117302877209885e-16\n",
      "avg loss: 0.937\n",
      "model action_tally: [282. 251. 343.   0.  20. 121.]\n",
      "train_tally:        [115. 116.  38.  87.  28. 111.]\n",
      "discounted_rewards mean: 1.7117302877209885e-16\n",
      "avg loss: 0.925\n",
      "model action_tally: [282. 251. 343.   0.  20. 121.]\n",
      "train_tally:        [116. 116.  38.  87.  28. 107.]\n",
      "\n",
      "52) frames played: 1192, score: -21.0\n",
      "discounted_rewards mean: -3.0996830754635243e-16\n",
      "Shifting rewards by 0.016148531177829457\n",
      "new discounted_rewards mean: 0.016148531177829183\n",
      "avg loss: 1.284\n",
      "model action_tally: [575. 120.  33.  68.   1. 395.]\n",
      "train_tally:        [187. 187.  13.  62.  46. 183.]\n",
      "discounted_rewards mean: -3.0996830754635243e-16\n",
      "Shifting rewards by 0.016148531177829457\n",
      "new discounted_rewards mean: 0.016148531177829183\n",
      "avg loss: 1.117\n",
      "model action_tally: [575. 120.  33.  68.   1. 395.]\n",
      "train_tally:        [178. 180.  13.  62.  46. 181.]\n",
      "discounted_rewards mean: -3.0996830754635243e-16\n",
      "Shifting rewards by 0.016148531177829457\n",
      "new discounted_rewards mean: 0.016148531177829183\n",
      "avg loss: 1.043\n",
      "model action_tally: [575. 120.  33.  68.   1. 395.]\n",
      "train_tally:        [197. 198.  13.  62.  46. 174.]\n",
      "\n",
      "53) frames played: 1018, score: -21.0\n",
      "discounted_rewards mean: 1.3785087457035342e-16\n",
      "avg loss: 0.851\n",
      "model action_tally: [253. 551.   0.  27.   0. 187.]\n",
      "train_tally:        [165. 167. 167.   1.  44.  75.]\n",
      "discounted_rewards mean: 1.3785087457035342e-16\n",
      "avg loss: 0.646\n",
      "model action_tally: [253. 551.   0.  27.   0. 187.]\n",
      "train_tally:        [172. 173. 173.   1.  44.  75.]\n",
      "discounted_rewards mean: 1.3785087457035342e-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.550\n",
      "model action_tally: [253. 551.   0.  27.   0. 187.]\n",
      "train_tally:        [169. 171. 172.   1.  44.  75.]\n",
      "\n",
      "54) frames played: 1020, score: -21.0\n",
      "discounted_rewards mean: 1.2713142085903753e-16\n",
      "avg loss: 1.027\n",
      "model action_tally: [294. 262. 328.   0.  73.  63.]\n",
      "train_tally:        [162. 154.  44. 162.  14. 163.]\n",
      "discounted_rewards mean: 1.2713142085903753e-16\n",
      "avg loss: 0.804\n",
      "model action_tally: [294. 262. 328.   0.  73.  63.]\n",
      "train_tally:        [160. 159.  44. 156.  14. 161.]\n",
      "discounted_rewards mean: 1.2713142085903753e-16\n",
      "avg loss: 0.748\n",
      "model action_tally: [294. 262. 328.   0.  73.  63.]\n",
      "train_tally:        [157. 157.  44. 158.  14. 157.]\n",
      "\n",
      "55) frames played: 1028, score: -21.0\n",
      "discounted_rewards mean: 2.3500440676890473e-16\n",
      "avg loss: 0.952\n",
      "model action_tally: [288. 269.   0. 197.   0. 274.]\n",
      "train_tally:        [180. 181.   0.   7.  42.  86.]\n",
      "discounted_rewards mean: 2.3500440676890473e-16\n",
      "avg loss: 0.745\n",
      "model action_tally: [288. 269.   0. 197.   0. 274.]\n",
      "train_tally:        [183. 184.   0.   7.  42.  86.]\n",
      "discounted_rewards mean: 2.3500440676890473e-16\n",
      "avg loss: 0.667\n",
      "model action_tally: [288. 269.   0. 197.   0. 274.]\n",
      "train_tally:        [193. 193.   0.   7.  42.  86.]\n",
      "\n",
      "56) frames played: 1052, score: -21.0\n",
      "discounted_rewards mean: -2.161346724745552e-16\n",
      "Shifting rewards by -0.08037654681935895\n",
      "new discounted_rewards mean: -0.08037654681935912\n",
      "avg loss: 1.273\n",
      "model action_tally: [735. 312.   0.   0.   0.   5.]\n",
      "train_tally:        [158.  64.  25. 144.   0. 159.]\n",
      "discounted_rewards mean: -2.161346724745552e-16\n",
      "Shifting rewards by -0.08037654681935895\n",
      "new discounted_rewards mean: -0.08037654681935912\n",
      "avg loss: 0.975\n",
      "model action_tally: [735. 312.   0.   0.   0.   5.]\n",
      "train_tally:        [158.  64.  25. 146.   0. 158.]\n",
      "discounted_rewards mean: -2.161346724745552e-16\n",
      "Shifting rewards by -0.08037654681935895\n",
      "new discounted_rewards mean: -0.08037654681935912\n",
      "avg loss: 0.888\n",
      "model action_tally: [735. 312.   0.   0.   0.   5.]\n",
      "train_tally:        [159.  64.  25. 144.   0. 160.]\n",
      "\n",
      "57) frames played: 1007, score: -21.0\n",
      "discounted_rewards mean: -1.2524462323477435e-16\n",
      "avg loss: 0.915\n",
      "model action_tally: [253. 209.   0.  56.   0. 489.]\n",
      "train_tally:        [141. 117.  64.  23. 141. 138.]\n",
      "discounted_rewards mean: -1.2524462323477435e-16\n",
      "avg loss: 0.699\n",
      "model action_tally: [253. 209.   0.  56.   0. 489.]\n",
      "train_tally:        [140. 118.  64.  23. 141. 139.]\n",
      "discounted_rewards mean: -1.2524462323477435e-16\n",
      "avg loss: 0.569\n",
      "model action_tally: [253. 209.   0.  56.   0. 489.]\n",
      "train_tally:        [150. 114.  64.  23. 150. 150.]\n",
      "\n",
      "58) frames played: 1028, score: -21.0\n",
      "discounted_rewards mean: -9.503854685507176e-17\n",
      "avg loss: 1.268\n",
      "model action_tally: [299. 221.  78.  27. 292. 111.]\n",
      "train_tally:        [168. 167.  17.  81.  32. 168.]\n",
      "discounted_rewards mean: -9.503854685507176e-17\n",
      "avg loss: 1.092\n",
      "model action_tally: [299. 221.  78.  27. 292. 111.]\n",
      "train_tally:        [162. 162.  17.  81.  32. 162.]\n",
      "discounted_rewards mean: -9.503854685507176e-17\n",
      "avg loss: 0.983\n",
      "model action_tally: [299. 221.  78.  27. 292. 111.]\n",
      "train_tally:        [166. 166.  17.  81.  32. 164.]\n",
      "\n",
      "59) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: 2.849863826990559e-16\n",
      "avg loss: 0.810\n",
      "model action_tally: [362. 272.   0.  60.   0. 322.]\n",
      "train_tally:        [161. 156. 162.  34.  14. 112.]\n",
      "discounted_rewards mean: 2.849863826990559e-16\n",
      "avg loss: 0.582\n",
      "model action_tally: [362. 272.   0.  60.   0. 322.]\n",
      "train_tally:        [159. 157. 160.  34.  14. 112.]\n",
      "discounted_rewards mean: 2.849863826990559e-16\n",
      "avg loss: 0.525\n",
      "model action_tally: [362. 272.   0.  60.   0. 322.]\n",
      "train_tally:        [156. 152. 157.  34.  14. 111.]\n",
      "\n",
      "60) frames played: 1019, score: -21.0\n",
      "discounted_rewards mean: 1.952423611509598e-16\n",
      "avg loss: 1.056\n",
      "model action_tally: [294. 305. 276.  40.   0. 104.]\n",
      "train_tally:        [145. 142.  25.  71. 145. 144.]\n",
      "discounted_rewards mean: 1.952423611509598e-16\n",
      "avg loss: 0.861\n",
      "model action_tally: [294. 305. 276.  40.   0. 104.]\n",
      "train_tally:        [145. 145.  25.  71. 144. 145.]\n",
      "discounted_rewards mean: 1.952423611509598e-16\n",
      "avg loss: 0.777\n",
      "model action_tally: [294. 305. 276.  40.   0. 104.]\n",
      "train_tally:        [143. 143.  25.  71. 141. 141.]\n",
      "\n",
      "61) frames played: 1113, score: -21.0\n",
      "discounted_rewards mean: -8.937644474969814e-17\n",
      "Shifting rewards by -0.047614315422459294\n",
      "new discounted_rewards mean: -0.047614315422459384\n",
      "avg loss: 1.281\n",
      "model action_tally: [536.  89.   0. 108. 127. 253.]\n",
      "train_tally:        [171. 171. 168.  36.  23.  80.]\n",
      "discounted_rewards mean: -8.937644474969814e-17\n",
      "Shifting rewards by -0.047614315422459294\n",
      "new discounted_rewards mean: -0.047614315422459384\n",
      "avg loss: 1.175\n",
      "model action_tally: [536.  89.   0. 108. 127. 253.]\n",
      "train_tally:        [176. 175. 176.  36.  23.  80.]\n",
      "discounted_rewards mean: -8.937644474969814e-17\n",
      "Shifting rewards by -0.047614315422459294\n",
      "new discounted_rewards mean: -0.047614315422459384\n",
      "avg loss: 1.062\n",
      "model action_tally: [536.  89.   0. 108. 127. 253.]\n",
      "train_tally:        [167. 167. 164.  36.  23.  80.]\n",
      "\n",
      "62) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: 2.4333655334250005e-16\n",
      "avg loss: 1.055\n",
      "model action_tally: [255. 462. 179.   0.  17. 109.]\n",
      "train_tally:        [134. 134.  59.  81. 134. 130.]\n",
      "discounted_rewards mean: 2.4333655334250005e-16\n",
      "avg loss: 0.743\n",
      "model action_tally: [255. 462. 179.   0.  17. 109.]\n",
      "train_tally:        [139. 139.  59.  81. 140. 135.]\n",
      "discounted_rewards mean: 2.4333655334250005e-16\n",
      "avg loss: 0.675\n",
      "model action_tally: [255. 462. 179.   0.  17. 109.]\n",
      "train_tally:        [141. 141.  59.  81. 141. 133.]\n",
      "\n",
      "63) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: -2.153159805333637e-16\n",
      "avg loss: 0.931\n",
      "model action_tally: [266. 229.  55.  31. 288. 154.]\n",
      "train_tally:        [143. 138. 144. 140.  23. 130.]\n",
      "discounted_rewards mean: -2.153159805333637e-16\n",
      "avg loss: 0.576\n",
      "model action_tally: [266. 229.  55.  31. 288. 154.]\n",
      "train_tally:        [135. 136. 135. 136.  23. 127.]\n",
      "discounted_rewards mean: -2.153159805333637e-16\n",
      "avg loss: 0.502\n",
      "model action_tally: [266. 229.  55.  31. 288. 154.]\n",
      "train_tally:        [138. 137. 139. 133.  23. 130.]\n",
      "\n",
      "64) frames played: 1184, score: -21.0\n",
      "discounted_rewards mean: -3.3606751015680416e-16\n",
      "Shifting rewards by 0.009800116787865661\n",
      "new discounted_rewards mean: 0.009800116787865302\n",
      "avg loss: 1.366\n",
      "model action_tally: [456.  96. 422.  68.   0. 142.]\n",
      "train_tally:        [160. 160. 114. 160.  11.  43.]\n",
      "discounted_rewards mean: -3.3606751015680416e-16\n",
      "Shifting rewards by 0.009800116787865661\n",
      "new discounted_rewards mean: 0.009800116787865302\n",
      "avg loss: 1.155\n",
      "model action_tally: [456.  96. 422.  68.   0. 142.]\n",
      "train_tally:        [164. 164. 114. 164.  11.  43.]\n",
      "discounted_rewards mean: -3.3606751015680416e-16\n",
      "Shifting rewards by 0.009800116787865661\n",
      "new discounted_rewards mean: 0.009800116787865302\n",
      "avg loss: 1.089\n",
      "model action_tally: [456.  96. 422.  68.   0. 142.]\n",
      "train_tally:        [166. 166. 114. 166.  11.  43.]\n",
      "\n",
      "65) frames played: 1194, score: -21.0\n",
      "discounted_rewards mean: -1.8447926975345984e-16\n",
      "Shifting rewards by 0.02146580663629922\n",
      "new discounted_rewards mean: 0.02146580663629902\n",
      "avg loss: 1.156\n",
      "model action_tally: [699. 294.  87.  89.   0.  25.]\n",
      "train_tally:        [222.  71.  19.  31.  63. 223.]\n",
      "discounted_rewards mean: -1.8447926975345984e-16\n",
      "Shifting rewards by 0.02146580663629922\n",
      "new discounted_rewards mean: 0.02146580663629902\n",
      "avg loss: 0.942\n",
      "model action_tally: [699. 294.  87.  89.   0.  25.]\n",
      "train_tally:        [215.  71.  19.  31.  63. 216.]\n",
      "discounted_rewards mean: -1.8447926975345984e-16\n",
      "Shifting rewards by 0.02146580663629922\n",
      "new discounted_rewards mean: 0.02146580663629902\n",
      "avg loss: 0.909\n",
      "model action_tally: [699. 294.  87.  89.   0.  25.]\n",
      "train_tally:        [208.  71.  19.  31.  63. 208.]\n",
      "\n",
      "66) frames played: 1108, score: -21.0\n",
      "discounted_rewards mean: -1.7955953611266068e-16\n",
      "Shifting rewards by -0.04352789445824346\n",
      "new discounted_rewards mean: -0.04352789445824364\n",
      "avg loss: 1.411\n",
      "model action_tally: [548.  37.   0.   9.   9. 505.]\n",
      "train_tally:        [139. 139.  43.  12.   4.  84.]\n",
      "discounted_rewards mean: -1.7955953611266068e-16\n",
      "Shifting rewards by -0.04352789445824346\n",
      "new discounted_rewards mean: -0.04352789445824364\n",
      "avg loss: 1.155\n",
      "model action_tally: [548.  37.   0.   9.   9. 505.]\n",
      "train_tally:        [135. 135.  43.  12.   4.  84.]\n",
      "discounted_rewards mean: -1.7955953611266068e-16\n",
      "Shifting rewards by -0.04352789445824346\n",
      "new discounted_rewards mean: -0.04352789445824364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.039\n",
      "model action_tally: [548.  37.   0.   9.   9. 505.]\n",
      "train_tally:        [142. 142.  43.  12.   4.  84.]\n",
      "\n",
      "67) frames played: 1035, score: -21.0\n",
      "discounted_rewards mean: -3.0721533744216894e-16\n",
      "avg loss: 0.649\n",
      "model action_tally: [260. 494.   0.   0.   0. 281.]\n",
      "train_tally:        [176. 172.   0.   0. 177. 106.]\n",
      "discounted_rewards mean: -3.0721533744216894e-16\n",
      "avg loss: 0.491\n",
      "model action_tally: [260. 494.   0.   0.   0. 281.]\n",
      "train_tally:        [170. 171.   0.   0. 172. 106.]\n",
      "discounted_rewards mean: -3.0721533744216894e-16\n",
      "avg loss: 0.423\n",
      "model action_tally: [260. 494.   0.   0.   0. 281.]\n",
      "train_tally:        [166. 166.   0.   0. 166. 106.]\n",
      "\n",
      "68) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: -1.7117302877209885e-16\n",
      "avg loss: 0.704\n",
      "model action_tally: [281. 227.   0.   0. 406. 103.]\n",
      "train_tally:        [141. 141. 141. 103.  25.  99.]\n",
      "discounted_rewards mean: -1.7117302877209885e-16\n",
      "avg loss: 0.511\n",
      "model action_tally: [281. 227.   0.   0. 406. 103.]\n",
      "train_tally:        [144. 141. 145. 104.  25.  99.]\n",
      "discounted_rewards mean: -1.7117302877209885e-16\n",
      "avg loss: 0.467\n",
      "model action_tally: [281. 227.   0.   0. 406. 103.]\n",
      "train_tally:        [137. 138. 138. 103.  25.  99.]\n",
      "\n",
      "69) frames played: 1101, score: -21.0\n",
      "discounted_rewards mean: -9.035057493770575e-17\n",
      "Shifting rewards by -0.03190684398627533\n",
      "new discounted_rewards mean: -0.03190684398627542\n",
      "avg loss: 1.483\n",
      "model action_tally: [733.  29. 152.  53.   0. 134.]\n",
      "train_tally:        [156.  80.  46.  23. 156.  85.]\n",
      "discounted_rewards mean: -9.035057493770575e-17\n",
      "Shifting rewards by -0.03190684398627533\n",
      "new discounted_rewards mean: -0.03190684398627542\n",
      "avg loss: 1.169\n",
      "model action_tally: [733.  29. 152.  53.   0. 134.]\n",
      "train_tally:        [164.  80.  46.  23. 165.  85.]\n",
      "discounted_rewards mean: -9.035057493770575e-17\n",
      "Shifting rewards by -0.03190684398627533\n",
      "new discounted_rewards mean: -0.03190684398627542\n",
      "avg loss: 1.127\n",
      "model action_tally: [733.  29. 152.  53.   0. 134.]\n",
      "train_tally:        [157.  80.  46.  23. 158.  85.]\n",
      "\n",
      "70) frames played: 1018, score: -21.0\n",
      "discounted_rewards mean: 1.3261603123223875e-16\n",
      "avg loss: 1.015\n",
      "model action_tally: [197. 111. 165.  35. 397. 113.]\n",
      "train_tally:        [113. 102.   7.  58. 113. 103.]\n",
      "discounted_rewards mean: 1.3261603123223875e-16\n",
      "avg loss: 0.794\n",
      "model action_tally: [197. 111. 165.  35. 397. 113.]\n",
      "train_tally:        [109. 102.   7.  58. 109. 103.]\n",
      "discounted_rewards mean: 1.3261603123223875e-16\n",
      "avg loss: 0.714\n",
      "model action_tally: [197. 111. 165.  35. 397. 113.]\n",
      "train_tally:        [119. 100.   7.  58. 119. 103.]\n",
      "\n",
      "71) frames played: 1133, score: -20.0\n",
      "discounted_rewards mean: -1.254267847767167e-16\n",
      "Shifting rewards by 0.013420471114487917\n",
      "new discounted_rewards mean: 0.01342047111448782\n",
      "avg loss: 0.954\n",
      "model action_tally: [153.  45.   0. 128. 750.  57.]\n",
      "train_tally:        [ 98.  50. 187.  13. 187.  25.]\n",
      "discounted_rewards mean: -1.254267847767167e-16\n",
      "Shifting rewards by 0.013420471114487917\n",
      "new discounted_rewards mean: 0.01342047111448782\n",
      "avg loss: 0.747\n",
      "model action_tally: [153.  45.   0. 128. 750.  57.]\n",
      "train_tally:        [ 98.  50. 192.  13. 193.  25.]\n",
      "discounted_rewards mean: -1.254267847767167e-16\n",
      "Shifting rewards by 0.013420471114487917\n",
      "new discounted_rewards mean: 0.01342047111448782\n",
      "avg loss: 0.671\n",
      "model action_tally: [153.  45.   0. 128. 750.  57.]\n",
      "train_tally:        [ 98.  50. 188.  13. 188.  25.]\n",
      "\n",
      "72) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: 4.1221259990015646e-16\n",
      "avg loss: 0.901\n",
      "model action_tally: [358. 169.  92.  59. 253.  86.]\n",
      "train_tally:        [147. 146.   5. 148. 102. 148.]\n",
      "discounted_rewards mean: 4.1221259990015646e-16\n",
      "avg loss: 0.635\n",
      "model action_tally: [358. 169.  92.  59. 253.  86.]\n",
      "train_tally:        [141. 137.   5. 142. 102. 141.]\n",
      "discounted_rewards mean: 4.1221259990015646e-16\n",
      "avg loss: 0.563\n",
      "model action_tally: [358. 169.  92.  59. 253.  86.]\n",
      "train_tally:        [141. 140.   5. 142. 100. 141.]\n",
      "\n",
      "73) frames played: 1296, score: -19.0\n",
      "discounted_rewards mean: -2.1930331350620375e-17\n",
      "Shifting rewards by 0.11852045680590459\n",
      "new discounted_rewards mean: 0.11852045680590456\n",
      "avg loss: 1.440\n",
      "model action_tally: [390.  38.   0. 298. 535.  35.]\n",
      "train_tally:        [128.  39. 128.  98. 128.  21.]\n",
      "discounted_rewards mean: -2.1930331350620375e-17\n",
      "Shifting rewards by 0.11852045680590459\n",
      "new discounted_rewards mean: 0.11852045680590456\n",
      "avg loss: 1.170\n",
      "model action_tally: [390.  38.   0. 298. 535.  35.]\n",
      "train_tally:        [138.  39. 138.  98. 138.  21.]\n",
      "discounted_rewards mean: -2.1930331350620375e-17\n",
      "Shifting rewards by 0.11852045680590459\n",
      "new discounted_rewards mean: 0.11852045680590456\n",
      "avg loss: 1.037\n",
      "model action_tally: [390.  38.   0. 298. 535.  35.]\n",
      "train_tally:        [137.  39. 137.  98. 134.  21.]\n",
      "\n",
      "74) frames played: 1367, score: -20.0\n",
      "discounted_rewards mean: -4.158260340951574e-17\n",
      "Shifting rewards by 0.1283312152727464\n",
      "new discounted_rewards mean: 0.12833121527274638\n",
      "avg loss: 1.453\n",
      "model action_tally: [381.  96. 138. 504. 123. 125.]\n",
      "train_tally:        [156. 110.  73. 146. 167. 167.]\n",
      "discounted_rewards mean: -4.158260340951574e-17\n",
      "Shifting rewards by 0.1283312152727464\n",
      "new discounted_rewards mean: 0.12833121527274638\n",
      "avg loss: 1.138\n",
      "model action_tally: [381.  96. 138. 504. 123. 125.]\n",
      "train_tally:        [162. 110.  73. 146. 164. 164.]\n",
      "discounted_rewards mean: -4.158260340951574e-17\n",
      "Shifting rewards by 0.1283312152727464\n",
      "new discounted_rewards mean: 0.12833121527274638\n",
      "avg loss: 1.009\n",
      "model action_tally: [381.  96. 138. 504. 123. 125.]\n",
      "train_tally:        [161. 110.  73. 146. 164. 164.]\n",
      "\n",
      "75) frames played: 1172, score: -20.0\n",
      "discounted_rewards mean: -1.4550363189626624e-16\n",
      "Shifting rewards by 0.03255648620633082\n",
      "new discounted_rewards mean: 0.032556486206330684\n",
      "avg loss: 1.477\n",
      "model action_tally: [294. 131.  24. 112. 223. 388.]\n",
      "train_tally:        [149. 149. 150.  36.  66. 145.]\n",
      "discounted_rewards mean: -1.4550363189626624e-16\n",
      "Shifting rewards by 0.03255648620633082\n",
      "new discounted_rewards mean: 0.032556486206330684\n",
      "avg loss: 1.178\n",
      "model action_tally: [294. 131.  24. 112. 223. 388.]\n",
      "train_tally:        [148. 148. 149.  36.  66. 147.]\n",
      "discounted_rewards mean: -1.4550363189626624e-16\n",
      "Shifting rewards by 0.03255648620633082\n",
      "new discounted_rewards mean: 0.032556486206330684\n",
      "avg loss: 1.075\n",
      "model action_tally: [294. 131.  24. 112. 223. 388.]\n",
      "train_tally:        [151. 151. 151.  36.  66. 151.]\n",
      "\n",
      "76) frames played: 1019, score: -21.0\n",
      "discounted_rewards mean: -8.367529763612564e-17\n",
      "avg loss: 1.037\n",
      "model action_tally: [506. 188. 188.  11.   0. 126.]\n",
      "train_tally:        [148. 148.   7.  88. 124. 144.]\n",
      "discounted_rewards mean: -8.367529763612564e-17\n",
      "avg loss: 0.743\n",
      "model action_tally: [506. 188. 188.  11.   0. 126.]\n",
      "train_tally:        [141. 142.   7.  88. 124. 141.]\n",
      "discounted_rewards mean: -8.367529763612564e-17\n",
      "avg loss: 0.684\n",
      "model action_tally: [506. 188. 188.  11.   0. 126.]\n",
      "train_tally:        [147. 148.   7.  88. 124. 145.]\n",
      "\n",
      "77) frames played: 1100, score: -21.0\n",
      "discounted_rewards mean: -3.875687649600547e-17\n",
      "Shifting rewards by -0.045994716962687685\n",
      "new discounted_rewards mean: -0.045994716962687775\n",
      "avg loss: 1.362\n",
      "model action_tally: [666.  59.   0. 147.  53. 175.]\n",
      "train_tally:        [142. 119. 142.  27. 100.  58.]\n",
      "discounted_rewards mean: -3.875687649600547e-17\n",
      "Shifting rewards by -0.045994716962687685\n",
      "new discounted_rewards mean: -0.045994716962687775\n",
      "avg loss: 1.025\n",
      "model action_tally: [666.  59.   0. 147.  53. 175.]\n",
      "train_tally:        [143. 117. 143.  27.  98.  58.]\n",
      "discounted_rewards mean: -3.875687649600547e-17\n",
      "Shifting rewards by -0.045994716962687685\n",
      "new discounted_rewards mean: -0.045994716962687775\n",
      "avg loss: 0.981\n",
      "model action_tally: [666.  59.   0. 147.  53. 175.]\n",
      "train_tally:        [143. 119. 143.  27. 100.  58.]\n",
      "\n",
      "78) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: -3.493327117797936e-16\n",
      "avg loss: 0.731\n",
      "model action_tally: [232. 252. 234.   0. 216.  83.]\n",
      "train_tally:        [149. 151.  49. 141.  13. 151.]\n",
      "discounted_rewards mean: -3.493327117797936e-16\n",
      "avg loss: 0.526\n",
      "model action_tally: [232. 252. 234.   0. 216.  83.]\n",
      "train_tally:        [147. 146.  49. 146.  13. 148.]\n",
      "discounted_rewards mean: -3.493327117797936e-16\n",
      "avg loss: 0.469\n",
      "model action_tally: [232. 252. 234.   0. 216.  83.]\n",
      "train_tally:        [150. 150.  49. 147.  13. 151.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "79) frames played: 1012, score: -21.0\n",
      "discounted_rewards mean: 1.4042346556523718e-17\n",
      "avg loss: 1.011\n",
      "model action_tally: [220. 232.  92. 163.   2. 303.]\n",
      "train_tally:        [141. 142.  79.  53.  53.  90.]\n",
      "discounted_rewards mean: 1.4042346556523718e-17\n",
      "avg loss: 0.745\n",
      "model action_tally: [220. 232.  92. 163.   2. 303.]\n",
      "train_tally:        [135. 135.  79.  53.  53.  90.]\n",
      "discounted_rewards mean: 1.4042346556523718e-17\n",
      "avg loss: 0.678\n",
      "model action_tally: [220. 232.  92. 163.   2. 303.]\n",
      "train_tally:        [137. 137.  78.  53.  53.  90.]\n",
      "\n",
      "80) frames played: 998, score: -21.0\n",
      "discounted_rewards mean: 4.9837666836880775e-17\n",
      "avg loss: 0.742\n",
      "model action_tally: [368. 213. 142.  77. 112.  86.]\n",
      "train_tally:        [134. 132. 110.   3. 117. 135.]\n",
      "discounted_rewards mean: 4.9837666836880775e-17\n",
      "avg loss: 0.500\n",
      "model action_tally: [368. 213. 142.  77. 112.  86.]\n",
      "train_tally:        [135. 137. 110.   3. 117. 137.]\n",
      "discounted_rewards mean: 4.9837666836880775e-17\n",
      "avg loss: 0.476\n",
      "model action_tally: [368. 213. 142.  77. 112.  86.]\n",
      "train_tally:        [131. 132. 110.   3. 115. 132.]\n",
      "\n",
      "81) frames played: 1101, score: -21.0\n",
      "discounted_rewards mean: 3.872167497330246e-17\n",
      "Shifting rewards by -0.0525413195358871\n",
      "new discounted_rewards mean: -0.052541319535887074\n",
      "avg loss: 0.956\n",
      "model action_tally: [228. 208. 195.   0. 205. 265.]\n",
      "train_tally:        [149. 149. 101. 150.  28.  90.]\n",
      "discounted_rewards mean: 3.872167497330246e-17\n",
      "Shifting rewards by -0.0525413195358871\n",
      "new discounted_rewards mean: -0.052541319535887074\n",
      "avg loss: 0.791\n",
      "model action_tally: [228. 208. 195.   0. 205. 265.]\n",
      "train_tally:        [153. 148. 101. 154.  27.  91.]\n",
      "discounted_rewards mean: 3.872167497330246e-17\n",
      "Shifting rewards by -0.0525413195358871\n",
      "new discounted_rewards mean: -0.052541319535887074\n",
      "avg loss: 0.711\n",
      "model action_tally: [228. 208. 195.   0. 205. 265.]\n",
      "train_tally:        [147. 148. 100. 147.  28.  91.]\n",
      "\n",
      "82) frames played: 1175, score: -21.0\n",
      "discounted_rewards mean: 1.6932082213857707e-16\n",
      "Shifting rewards by 0.003008384751086019\n",
      "new discounted_rewards mean: 0.003008384751086164\n",
      "avg loss: 1.142\n",
      "model action_tally: [307. 182. 241. 303.  13. 129.]\n",
      "train_tally:        [141. 142. 143.  48. 136. 143.]\n",
      "discounted_rewards mean: 1.6932082213857707e-16\n",
      "Shifting rewards by 0.003008384751086019\n",
      "new discounted_rewards mean: 0.003008384751086164\n",
      "avg loss: 0.929\n",
      "model action_tally: [307. 182. 241. 303.  13. 129.]\n",
      "train_tally:        [139. 150. 150.  48. 140. 149.]\n",
      "discounted_rewards mean: 1.6932082213857707e-16\n",
      "Shifting rewards by 0.003008384751086019\n",
      "new discounted_rewards mean: 0.003008384751086164\n",
      "avg loss: 0.825\n",
      "model action_tally: [307. 182. 241. 303.  13. 129.]\n",
      "train_tally:        [141. 147. 147.  48. 143. 148.]\n",
      "\n",
      "83) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: 3.854028645641569e-17\n",
      "avg loss: 0.897\n",
      "model action_tally: [187. 254. 212.  39. 200. 122.]\n",
      "train_tally:        [116. 117. 111. 110.  82. 105.]\n",
      "discounted_rewards mean: 3.854028645641569e-17\n",
      "avg loss: 0.559\n",
      "model action_tally: [187. 254. 212.  39. 200. 122.]\n",
      "train_tally:        [117. 117. 115. 107.  82. 107.]\n",
      "discounted_rewards mean: 3.854028645641569e-17\n",
      "avg loss: 0.520\n",
      "model action_tally: [187. 254. 212.  39. 200. 122.]\n",
      "train_tally:        [115. 115. 106. 111.  82. 107.]\n",
      "\n",
      "84) frames played: 1047, score: -21.0\n",
      "discounted_rewards mean: 1.4930219853602868e-16\n",
      "avg loss: 1.018\n",
      "model action_tally: [408. 143. 292.  67.  67.  70.]\n",
      "train_tally:        [166. 134. 166.  12.  23. 166.]\n",
      "discounted_rewards mean: 1.4930219853602868e-16\n",
      "avg loss: 0.711\n",
      "model action_tally: [408. 143. 292.  67.  67.  70.]\n",
      "train_tally:        [167. 138. 164.  12.  23. 167.]\n",
      "discounted_rewards mean: 1.4930219853602868e-16\n",
      "avg loss: 0.600\n",
      "model action_tally: [408. 143. 292.  67.  67.  70.]\n",
      "train_tally:        [168. 138. 168.  12.  23. 166.]\n",
      "\n",
      "85) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: -3.7159370833983736e-16\n",
      "avg loss: 0.533\n",
      "model action_tally: [224. 272. 345.   0.   1. 181.]\n",
      "train_tally:        [136. 136. 108. 137.  10.  87.]\n",
      "discounted_rewards mean: -3.7159370833983736e-16\n",
      "avg loss: 0.333\n",
      "model action_tally: [224. 272. 345.   0.   1. 181.]\n",
      "train_tally:        [130. 127. 109. 130.  10.  87.]\n",
      "discounted_rewards mean: -3.7159370833983736e-16\n",
      "avg loss: 0.277\n",
      "model action_tally: [224. 272. 345.   0.   1. 181.]\n",
      "train_tally:        [131. 132. 105. 131.  10.  87.]\n",
      "\n",
      "86) frames played: 1430, score: -21.0\n",
      "discounted_rewards mean: -1.2422075800001752e-16\n",
      "Shifting rewards by 0.1263225076655213\n",
      "new discounted_rewards mean: 0.12632250766552114\n",
      "avg loss: 1.309\n",
      "model action_tally: [618. 180. 298. 198.   1. 135.]\n",
      "train_tally:        [191. 191. 137.  57. 167. 189.]\n",
      "discounted_rewards mean: -1.2422075800001752e-16\n",
      "Shifting rewards by 0.1263225076655213\n",
      "new discounted_rewards mean: 0.12632250766552114\n",
      "avg loss: 1.137\n",
      "model action_tally: [618. 180. 298. 198.   1. 135.]\n",
      "train_tally:        [182. 184. 136.  57. 165. 184.]\n",
      "discounted_rewards mean: -1.2422075800001752e-16\n",
      "Shifting rewards by 0.1263225076655213\n",
      "new discounted_rewards mean: 0.12632250766552114\n",
      "avg loss: 1.030\n",
      "model action_tally: [618. 180. 298. 198.   1. 135.]\n",
      "train_tally:        [182. 183. 137.  57. 167. 181.]\n",
      "\n",
      "87) frames played: 1278, score: -20.0\n",
      "discounted_rewards mean: 1.1119604628483571e-16\n",
      "Shifting rewards by 0.07977426313050795\n",
      "new discounted_rewards mean: 0.07977426313050807\n",
      "avg loss: 1.061\n",
      "model action_tally: [108. 347. 114.  28. 501. 180.]\n",
      "train_tally:        [154. 120.  44. 154. 155. 121.]\n",
      "discounted_rewards mean: 1.1119604628483571e-16\n",
      "Shifting rewards by 0.07977426313050795\n",
      "new discounted_rewards mean: 0.07977426313050807\n",
      "avg loss: 0.841\n",
      "model action_tally: [108. 347. 114.  28. 501. 180.]\n",
      "train_tally:        [156. 120.  44. 163. 163. 122.]\n",
      "discounted_rewards mean: 1.1119604628483571e-16\n",
      "Shifting rewards by 0.07977426313050795\n",
      "new discounted_rewards mean: 0.07977426313050807\n",
      "avg loss: 0.744\n",
      "model action_tally: [108. 347. 114.  28. 501. 180.]\n",
      "train_tally:        [151. 120.  44. 152. 151. 122.]\n",
      "\n",
      "88) frames played: 1105, score: -21.0\n",
      "discounted_rewards mean: 1.4146552205178464e-16\n",
      "Shifting rewards by -0.03913762924270417\n",
      "new discounted_rewards mean: -0.03913762924270403\n",
      "avg loss: 0.942\n",
      "model action_tally: [394. 216. 123. 110.   9. 253.]\n",
      "train_tally:        [148. 146. 149.  11. 149. 148.]\n",
      "discounted_rewards mean: 1.4146552205178464e-16\n",
      "Shifting rewards by -0.03913762924270417\n",
      "new discounted_rewards mean: -0.03913762924270403\n",
      "avg loss: 0.664\n",
      "model action_tally: [394. 216. 123. 110.   9. 253.]\n",
      "train_tally:        [153. 147. 153.  11. 154. 146.]\n",
      "discounted_rewards mean: 1.4146552205178464e-16\n",
      "Shifting rewards by -0.03913762924270417\n",
      "new discounted_rewards mean: -0.03913762924270403\n",
      "avg loss: 0.629\n",
      "model action_tally: [394. 216. 123. 110.   9. 253.]\n",
      "train_tally:        [145. 149. 148.  11. 149. 146.]\n",
      "\n",
      "89) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: 1.416189999915554e-16\n",
      "avg loss: 0.595\n",
      "model action_tally: [235. 161. 159.   0. 369.  92.]\n",
      "train_tally:        [140. 143.  87. 144.  17.  98.]\n",
      "discounted_rewards mean: 1.416189999915554e-16\n",
      "avg loss: 0.342\n",
      "model action_tally: [235. 161. 159.   0. 369.  92.]\n",
      "train_tally:        [151. 152.  87. 152.  17.  98.]\n",
      "discounted_rewards mean: 1.416189999915554e-16\n",
      "avg loss: 0.331\n",
      "model action_tally: [235. 161. 159.   0. 369.  92.]\n",
      "train_tally:        [152. 150.  87. 153.  17.  98.]\n",
      "\n",
      "90) frames played: 1594, score: -20.0\n",
      "discounted_rewards mean: -7.132173006374907e-17\n",
      "Shifting rewards by 0.1864950378878811\n",
      "new discounted_rewards mean: 0.18649503788788102\n",
      "avg loss: 1.562\n",
      "model action_tally: [442. 278. 340. 462.   2.  70.]\n",
      "train_tally:        [208. 207. 208. 138. 123. 118.]\n",
      "discounted_rewards mean: -7.132173006374907e-17\n",
      "Shifting rewards by 0.1864950378878811\n",
      "new discounted_rewards mean: 0.18649503788788102\n",
      "avg loss: 1.351\n",
      "model action_tally: [442. 278. 340. 462.   2.  70.]\n",
      "train_tally:        [200. 201. 201. 138. 123. 118.]\n",
      "discounted_rewards mean: -7.132173006374907e-17\n",
      "Shifting rewards by 0.1864950378878811\n",
      "new discounted_rewards mean: 0.18649503788788102\n",
      "avg loss: 1.239\n",
      "model action_tally: [442. 278. 340. 462.   2.  70.]\n",
      "train_tally:        [194. 195. 193. 138. 123. 118.]\n",
      "\n",
      "91) frames played: 1009, score: -21.0\n",
      "discounted_rewards mean: -5.633639133875918e-17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.745\n",
      "model action_tally: [223. 382. 182. 125.  28.  69.]\n",
      "train_tally:        [131. 130.  85.  44. 125. 132.]\n",
      "discounted_rewards mean: -5.633639133875918e-17\n",
      "avg loss: 0.465\n",
      "model action_tally: [223. 382. 182. 125.  28.  69.]\n",
      "train_tally:        [134. 133.  85.  44. 122. 135.]\n",
      "discounted_rewards mean: -5.633639133875918e-17\n",
      "avg loss: 0.422\n",
      "model action_tally: [223. 382. 182. 125.  28.  69.]\n",
      "train_tally:        [134. 134.  85.  44. 122. 135.]\n",
      "\n",
      "92) frames played: 1175, score: -20.0\n",
      "discounted_rewards mean: 9.675475550775832e-17\n",
      "Shifting rewards by 0.04276449026526491\n",
      "new discounted_rewards mean: 0.04276449026526499\n",
      "avg loss: 1.307\n",
      "model action_tally: [150.  57. 190.  23.  31. 724.]\n",
      "train_tally:        [187. 188.  61.  31.  20. 188.]\n",
      "discounted_rewards mean: 9.675475550775832e-17\n",
      "Shifting rewards by 0.04276449026526491\n",
      "new discounted_rewards mean: 0.04276449026526499\n",
      "avg loss: 1.058\n",
      "model action_tally: [150.  57. 190.  23.  31. 724.]\n",
      "train_tally:        [190. 187.  61.  31.  20. 190.]\n",
      "discounted_rewards mean: 9.675475550775832e-17\n",
      "Shifting rewards by 0.04276449026526491\n",
      "new discounted_rewards mean: 0.04276449026526499\n",
      "avg loss: 0.977\n",
      "model action_tally: [150.  57. 190.  23.  31. 724.]\n",
      "train_tally:        [191. 192.  61.  31.  20. 193.]\n",
      "\n",
      "93) frames played: 1216, score: -20.0\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.053300937773381796\n",
      "new discounted_rewards mean: 0.05330093777338179\n",
      "avg loss: 1.349\n",
      "model action_tally: [303. 285.  47.  19.  28. 534.]\n",
      "train_tally:        [127.  69.  77. 119. 159. 159.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.053300937773381796\n",
      "new discounted_rewards mean: 0.05330093777338179\n",
      "avg loss: 1.081\n",
      "model action_tally: [303. 285.  47.  19.  28. 534.]\n",
      "train_tally:        [127.  69.  77. 119. 160. 160.]\n",
      "discounted_rewards mean: 0.0\n",
      "Shifting rewards by 0.053300937773381796\n",
      "new discounted_rewards mean: 0.05330093777338179\n",
      "avg loss: 0.997\n",
      "model action_tally: [303. 285.  47.  19.  28. 534.]\n",
      "train_tally:        [127.  69.  77. 119. 155. 156.]\n",
      "\n",
      "94) frames played: 1157, score: -20.0\n",
      "discounted_rewards mean: -4.9130007658433895e-17\n",
      "Shifting rewards by 0.028653767300716008\n",
      "new discounted_rewards mean: 0.028653767300715966\n",
      "avg loss: 1.542\n",
      "model action_tally: [230.  50. 101. 283. 183. 310.]\n",
      "train_tally:        [150. 133. 151.  74.  65. 152.]\n",
      "discounted_rewards mean: -4.9130007658433895e-17\n",
      "Shifting rewards by 0.028653767300716008\n",
      "new discounted_rewards mean: 0.028653767300715966\n",
      "avg loss: 1.322\n",
      "model action_tally: [230.  50. 101. 283. 183. 310.]\n",
      "train_tally:        [152. 132. 155.  74.  65. 155.]\n",
      "discounted_rewards mean: -4.9130007658433895e-17\n",
      "Shifting rewards by 0.028653767300716008\n",
      "new discounted_rewards mean: 0.028653767300715966\n",
      "avg loss: 1.217\n",
      "model action_tally: [230.  50. 101. 283. 183. 310.]\n",
      "train_tally:        [149. 133. 152.  74.  65. 152.]\n",
      "\n",
      "95) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: -5.17959234971893e-16\n",
      "avg loss: 0.911\n",
      "model action_tally: [298. 208. 253.   7.  12. 244.]\n",
      "train_tally:        [163. 164.  81.  24.  27.  86.]\n",
      "discounted_rewards mean: -5.17959234971893e-16\n",
      "avg loss: 0.587\n",
      "model action_tally: [298. 208. 253.   7.  12. 244.]\n",
      "train_tally:        [160. 160.  81.  24.  27.  86.]\n",
      "discounted_rewards mean: -5.17959234971893e-16\n",
      "avg loss: 0.519\n",
      "model action_tally: [298. 208. 253.   7.  12. 244.]\n",
      "train_tally:        [169. 169.  81.  24.  27.  86.]\n",
      "\n",
      "96) frames played: 1026, score: -21.0\n",
      "discounted_rewards mean: 4.0513401600356587e-16\n",
      "avg loss: 0.673\n",
      "model action_tally: [424. 415.  82.   9.  21.  75.]\n",
      "train_tally:        [149. 138. 149.   0.  93. 131.]\n",
      "discounted_rewards mean: 4.0513401600356587e-16\n",
      "avg loss: 0.413\n",
      "model action_tally: [424. 415.  82.   9.  21.  75.]\n",
      "train_tally:        [139. 143. 143.   0.  93. 124.]\n",
      "discounted_rewards mean: 4.0513401600356587e-16\n",
      "avg loss: 0.366\n",
      "model action_tally: [424. 415.  82.   9.  21.  75.]\n",
      "train_tally:        [137. 138. 137.   0.  93. 132.]\n",
      "\n",
      "97) frames played: 1011, score: -21.0\n",
      "discounted_rewards mean: -2.7409660429914844e-16\n",
      "avg loss: 0.615\n",
      "model action_tally: [255. 204. 347.   0. 124.  81.]\n",
      "train_tally:        [142. 136.  82. 142.  35.  76.]\n",
      "discounted_rewards mean: -2.7409660429914844e-16\n",
      "avg loss: 0.428\n",
      "model action_tally: [255. 204. 347.   0. 124.  81.]\n",
      "train_tally:        [139. 140.  81. 138.  35.  76.]\n",
      "discounted_rewards mean: -2.7409660429914844e-16\n",
      "avg loss: 0.420\n",
      "model action_tally: [255. 204. 347.   0. 124.  81.]\n",
      "train_tally:        [140. 141.  80. 141.  35.  76.]\n",
      "\n",
      "98) frames played: 1209, score: -21.0\n",
      "discounted_rewards mean: 7.052533357420349e-17\n",
      "Shifting rewards by 0.02492762946311518\n",
      "new discounted_rewards mean: 0.024927629463115223\n",
      "avg loss: 1.342\n",
      "model action_tally: [647. 129. 161. 216.   5.  51.]\n",
      "train_tally:        [160. 158.  85.  50. 160. 149.]\n",
      "discounted_rewards mean: 7.052533357420349e-17\n",
      "Shifting rewards by 0.02492762946311518\n",
      "new discounted_rewards mean: 0.024927629463115223\n",
      "avg loss: 1.056\n",
      "model action_tally: [647. 129. 161. 216.   5.  51.]\n",
      "train_tally:        [155. 156.  85.  50. 157. 144.]\n",
      "discounted_rewards mean: 7.052533357420349e-17\n",
      "Shifting rewards by 0.02492762946311518\n",
      "new discounted_rewards mean: 0.024927629463115223\n",
      "avg loss: 0.964\n",
      "model action_tally: [647. 129. 161. 216.   5.  51.]\n",
      "train_tally:        [166. 167.  85.  50. 167. 148.]\n",
      "\n",
      "99) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: -2.398603168661786e-16\n",
      "avg loss: 0.876\n",
      "model action_tally: [171. 245.  89.  75. 230. 212.]\n",
      "train_tally:        [122. 122.  40.   6. 116.  93.]\n",
      "discounted_rewards mean: -2.398603168661786e-16\n",
      "avg loss: 0.582\n",
      "model action_tally: [171. 245.  89.  75. 230. 212.]\n",
      "train_tally:        [119. 118.  40.   6. 119.  93.]\n",
      "discounted_rewards mean: -2.398603168661786e-16\n",
      "avg loss: 0.517\n",
      "model action_tally: [171. 245.  89.  75. 230. 212.]\n",
      "train_tally:        [117. 118.  40.   6. 115.  93.]\n",
      "\n",
      "100) frames played: 1020, score: -21.0\n",
      "discounted_rewards mean: 2.5600436803121254e-16\n",
      "avg loss: 0.808\n",
      "model action_tally: [444. 135.  34.   0. 279. 128.]\n",
      "train_tally:        [127. 130. 131.  13. 102.  98.]\n",
      "discounted_rewards mean: 2.5600436803121254e-16\n",
      "avg loss: 0.566\n",
      "model action_tally: [444. 135.  34.   0. 279. 128.]\n",
      "train_tally:        [129. 132. 133.  13. 102.  98.]\n",
      "discounted_rewards mean: 2.5600436803121254e-16\n",
      "avg loss: 0.489\n",
      "model action_tally: [444. 135.  34.   0. 279. 128.]\n",
      "train_tally:        [130. 131. 131.  13. 101.  98.]\n",
      "\n",
      "101) frames played: 1013, score: -21.0\n",
      "discounted_rewards mean: -1.6132757080436628e-16\n",
      "avg loss: 0.867\n",
      "model action_tally: [127. 230. 367.   0. 200.  89.]\n",
      "train_tally:        [150. 147.  43.  49. 151. 100.]\n",
      "discounted_rewards mean: -1.6132757080436628e-16\n",
      "avg loss: 0.571\n",
      "model action_tally: [127. 230. 367.   0. 200.  89.]\n",
      "train_tally:        [150. 150.  43.  49. 151. 100.]\n",
      "discounted_rewards mean: -1.6132757080436628e-16\n",
      "avg loss: 0.494\n",
      "model action_tally: [127. 230. 367.   0. 200.  89.]\n",
      "train_tally:        [150. 148.  43.  49. 151. 100.]\n",
      "\n",
      "102) frames played: 1019, score: -21.0\n",
      "discounted_rewards mean: -1.2551294645418847e-16\n",
      "avg loss: 0.796\n",
      "model action_tally: [321. 132.  26.  45. 406.  89.]\n",
      "train_tally:        [119. 127. 123.  79. 128. 127.]\n",
      "discounted_rewards mean: -1.2551294645418847e-16\n",
      "avg loss: 0.544\n",
      "model action_tally: [321. 132.  26.  45. 406.  89.]\n",
      "train_tally:        [113. 128. 125.  79. 128. 129.]\n",
      "discounted_rewards mean: -1.2551294645418847e-16\n",
      "avg loss: 0.486\n",
      "model action_tally: [321. 132.  26.  45. 406.  89.]\n",
      "train_tally:        [123. 131. 131.  79. 129. 132.]\n",
      "\n",
      "103) frames played: 1051, score: -21.0\n",
      "discounted_rewards mean: -1.4873396942647197e-16\n",
      "Shifting rewards by -0.0897609900073872\n",
      "new discounted_rewards mean: -0.08976099000738738\n",
      "avg loss: 1.062\n",
      "model action_tally: [118.  72.  86.  38. 653.  84.]\n",
      "train_tally:        [196.  54.  28.  15. 196.  43.]\n",
      "discounted_rewards mean: -1.4873396942647197e-16\n",
      "Shifting rewards by -0.0897609900073872\n",
      "new discounted_rewards mean: -0.08976099000738738\n",
      "avg loss: 0.801\n",
      "model action_tally: [118.  72.  86.  38. 653.  84.]\n",
      "train_tally:        [185.  54.  28.  15. 185.  43.]\n",
      "discounted_rewards mean: -1.4873396942647197e-16\n",
      "Shifting rewards by -0.0897609900073872\n",
      "new discounted_rewards mean: -0.08976099000738738\n",
      "avg loss: 0.743\n",
      "model action_tally: [118.  72.  86.  38. 653.  84.]\n",
      "train_tally:        [185.  54.  28.  15. 185.  43.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "104) frames played: 1019, score: -21.0\n",
      "discounted_rewards mean: -2.091882440903141e-17\n",
      "avg loss: 0.830\n",
      "model action_tally: [612. 164.  26.  25. 121.  71.]\n",
      "train_tally:        [152. 152. 153.   5. 154.  80.]\n",
      "discounted_rewards mean: -2.091882440903141e-17\n",
      "avg loss: 0.643\n",
      "model action_tally: [612. 164.  26.  25. 121.  71.]\n",
      "train_tally:        [151. 151. 151.   5. 149.  80.]\n",
      "discounted_rewards mean: -2.091882440903141e-17\n",
      "avg loss: 0.598\n",
      "model action_tally: [612. 164.  26.  25. 121.  71.]\n",
      "train_tally:        [151. 152. 153.   5. 151.  80.]\n",
      "\n",
      "105) frames played: 1027, score: -21.0\n",
      "discounted_rewards mean: 1.2453524093166314e-16\n",
      "avg loss: 0.664\n",
      "model action_tally: [153. 158. 297.   0. 350.  69.]\n",
      "train_tally:        [129. 128.  57. 129. 100. 127.]\n",
      "discounted_rewards mean: 1.2453524093166314e-16\n",
      "avg loss: 0.401\n",
      "model action_tally: [153. 158. 297.   0. 350.  69.]\n",
      "train_tally:        [127. 122.  57. 129. 101. 130.]\n",
      "discounted_rewards mean: 1.2453524093166314e-16\n",
      "avg loss: 0.342\n",
      "model action_tally: [153. 158. 297.   0. 350.  69.]\n",
      "train_tally:        [132. 127.  57. 132. 101. 132.]\n",
      "\n",
      "106) frames played: 1166, score: -21.0\n",
      "discounted_rewards mean: 9.750157609057978e-17\n",
      "Shifting rewards by -0.003965053247555006\n",
      "new discounted_rewards mean: -0.003965053247554953\n",
      "avg loss: 1.121\n",
      "model action_tally: [161.  51. 161.  64. 614. 115.]\n",
      "train_tally:        [106. 196.  62.  48. 197.  36.]\n",
      "discounted_rewards mean: 9.750157609057978e-17\n",
      "Shifting rewards by -0.003965053247555006\n",
      "new discounted_rewards mean: -0.003965053247554953\n",
      "avg loss: 0.896\n",
      "model action_tally: [161.  51. 161.  64. 614. 115.]\n",
      "train_tally:        [104. 183.  62.  48. 184.  36.]\n",
      "discounted_rewards mean: 9.750157609057978e-17\n",
      "Shifting rewards by -0.003965053247555006\n",
      "new discounted_rewards mean: -0.003965053247554953\n",
      "avg loss: 0.818\n",
      "model action_tally: [161.  51. 161.  64. 614. 115.]\n",
      "train_tally:        [106. 172.  62.  48. 172.  36.]\n",
      "\n",
      "107) frames played: 1308, score: -21.0\n",
      "discounted_rewards mean: 1.0864567825077985e-17\n",
      "Shifting rewards by 0.0811477963014481\n",
      "new discounted_rewards mean: 0.08114779630144812\n",
      "avg loss: 1.535\n",
      "model action_tally: [156. 546.  45. 224. 264.  73.]\n",
      "train_tally:        [120. 166. 154.  67. 166. 167.]\n",
      "discounted_rewards mean: 1.0864567825077985e-17\n",
      "Shifting rewards by 0.0811477963014481\n",
      "new discounted_rewards mean: 0.08114779630144812\n",
      "avg loss: 1.353\n",
      "model action_tally: [156. 546.  45. 224. 264.  73.]\n",
      "train_tally:        [119. 171. 154.  67. 172. 171.]\n",
      "discounted_rewards mean: 1.0864567825077985e-17\n",
      "Shifting rewards by 0.0811477963014481\n",
      "new discounted_rewards mean: 0.08114779630144812\n",
      "avg loss: 1.258\n",
      "model action_tally: [156. 546.  45. 224. 264.  73.]\n",
      "train_tally:        [120. 175. 150.  67. 176. 174.]\n",
      "\n",
      "108) frames played: 1257, score: -20.0\n",
      "discounted_rewards mean: 1.8088597887289743e-16\n",
      "Shifting rewards by 0.08303458658903862\n",
      "new discounted_rewards mean: 0.08303458658903877\n",
      "avg loss: 1.498\n",
      "model action_tally: [151. 242. 193.  78. 341. 252.]\n",
      "train_tally:        [120. 143.  36. 137. 119. 144.]\n",
      "discounted_rewards mean: 1.8088597887289743e-16\n",
      "Shifting rewards by 0.08303458658903862\n",
      "new discounted_rewards mean: 0.08303458658903877\n",
      "avg loss: 1.299\n",
      "model action_tally: [151. 242. 193.  78. 341. 252.]\n",
      "train_tally:        [119. 140.  36. 137. 123. 140.]\n",
      "discounted_rewards mean: 1.8088597887289743e-16\n",
      "Shifting rewards by 0.08303458658903862\n",
      "new discounted_rewards mean: 0.08303458658903877\n",
      "avg loss: 1.241\n",
      "model action_tally: [151. 242. 193.  78. 341. 252.]\n",
      "train_tally:        [121. 145.  36. 133. 123. 145.]\n",
      "\n",
      "109) frames played: 1139, score: -21.0\n",
      "discounted_rewards mean: -1.5595758028097018e-16\n",
      "Shifting rewards by -0.03130525735442243\n",
      "new discounted_rewards mean: -0.03130525735442256\n",
      "avg loss: 1.202\n",
      "model action_tally: [225. 372.  45. 142. 212. 143.]\n",
      "train_tally:        [149. 142. 150.  48. 150.  76.]\n",
      "discounted_rewards mean: -1.5595758028097018e-16\n",
      "Shifting rewards by -0.03130525735442243\n",
      "new discounted_rewards mean: -0.03130525735442256\n",
      "avg loss: 1.025\n",
      "model action_tally: [225. 372.  45. 142. 212. 143.]\n",
      "train_tally:        [151. 148. 154.  48. 154.  76.]\n",
      "discounted_rewards mean: -1.5595758028097018e-16\n",
      "Shifting rewards by -0.03130525735442243\n",
      "new discounted_rewards mean: -0.03130525735442256\n",
      "avg loss: 0.984\n",
      "model action_tally: [225. 372.  45. 142. 212. 143.]\n",
      "train_tally:        [146. 144. 148.  48. 148.  76.]\n",
      "\n",
      "110) frames played: 1028, score: -21.0\n",
      "discounted_rewards mean: -6.566299600895868e-17\n",
      "avg loss: 0.891\n",
      "model action_tally: [118. 216. 452.   1. 171.  70.]\n",
      "train_tally:        [142. 143.  52.  61. 115. 142.]\n",
      "discounted_rewards mean: -6.566299600895868e-17\n",
      "avg loss: 0.566\n",
      "model action_tally: [118. 216. 452.   1. 171.  70.]\n",
      "train_tally:        [136. 137.  52.  61. 115. 135.]\n",
      "discounted_rewards mean: -6.566299600895868e-17\n",
      "avg loss: 0.449\n",
      "model action_tally: [118. 216. 452.   1. 171.  70.]\n",
      "train_tally:        [150. 150.  52.  61. 115. 148.]\n",
      "\n",
      "111) frames played: 1133, score: -20.0\n",
      "discounted_rewards mean: -3.261096404194635e-16\n",
      "Shifting rewards by 0.013178789936690695\n",
      "new discounted_rewards mean: 0.013178789936690324\n",
      "avg loss: 1.213\n",
      "model action_tally: [266. 220.  47.   8. 531.  61.]\n",
      "train_tally:        [ 79.  50.  79. 160. 161.  36.]\n",
      "discounted_rewards mean: -3.261096404194635e-16\n",
      "Shifting rewards by 0.013178789936690695\n",
      "new discounted_rewards mean: 0.013178789936690324\n",
      "avg loss: 0.931\n",
      "model action_tally: [266. 220.  47.   8. 531.  61.]\n",
      "train_tally:        [ 79.  50.  79. 166. 166.  36.]\n",
      "discounted_rewards mean: -3.261096404194635e-16\n",
      "Shifting rewards by 0.013178789936690695\n",
      "new discounted_rewards mean: 0.013178789936690324\n",
      "avg loss: 0.862\n",
      "model action_tally: [266. 220.  47.   8. 531.  61.]\n",
      "train_tally:        [ 78.  50.  79. 162. 163.  36.]\n",
      "\n",
      "112) frames played: 1018, score: -21.0\n",
      "discounted_rewards mean: 1.7449477793715622e-18\n",
      "avg loss: 0.856\n",
      "model action_tally: [209. 198. 165.  79. 191. 176.]\n",
      "train_tally:        [150. 150.  95.  22.  80. 149.]\n",
      "discounted_rewards mean: 1.7449477793715622e-18\n",
      "avg loss: 0.663\n",
      "model action_tally: [209. 198. 165.  79. 191. 176.]\n",
      "train_tally:        [150. 153.  95.  22.  80. 153.]\n",
      "discounted_rewards mean: 1.7449477793715622e-18\n",
      "avg loss: 0.598\n",
      "model action_tally: [209. 198. 165.  79. 191. 176.]\n",
      "train_tally:        [143. 144.  95.  22.  80. 140.]\n",
      "\n",
      "113) frames played: 1016, score: -21.0\n",
      "discounted_rewards mean: 3.4268301232524515e-16\n",
      "avg loss: 0.682\n",
      "model action_tally: [239. 365. 100.   0. 150. 162.]\n",
      "train_tally:        [130. 131. 122.  46. 132. 131.]\n",
      "discounted_rewards mean: 3.4268301232524515e-16\n",
      "avg loss: 0.484\n",
      "model action_tally: [239. 365. 100.   0. 150. 162.]\n",
      "train_tally:        [127. 127. 126.  46. 127. 126.]\n",
      "discounted_rewards mean: 3.4268301232524515e-16\n",
      "avg loss: 0.403\n",
      "model action_tally: [239. 365. 100.   0. 150. 162.]\n",
      "train_tally:        [123. 135. 133.  46. 135. 133.]\n",
      "\n",
      "114) frames played: 1020, score: -21.0\n",
      "discounted_rewards mean: -1.7415263131375004e-18\n",
      "avg loss: 0.838\n",
      "model action_tally: [156. 219. 143.  27. 291. 184.]\n",
      "train_tally:        [143. 142. 142.  41. 143.  92.]\n",
      "discounted_rewards mean: -1.7415263131375004e-18\n",
      "avg loss: 0.560\n",
      "model action_tally: [156. 219. 143.  27. 291. 184.]\n",
      "train_tally:        [141. 143. 143.  41. 143.  91.]\n",
      "discounted_rewards mean: -1.7415263131375004e-18\n",
      "avg loss: 0.439\n",
      "model action_tally: [156. 219. 143.  27. 291. 184.]\n",
      "train_tally:        [142. 143. 143.  41. 142.  92.]\n",
      "\n",
      "115) frames played: 1034, score: -21.0\n",
      "discounted_rewards mean: 6.8717866127669264e-18\n",
      "avg loss: 0.441\n",
      "model action_tally: [128. 270. 186.  41. 319.  90.]\n",
      "train_tally:        [155. 138.  86.   2. 156. 155.]\n",
      "discounted_rewards mean: 6.8717866127669264e-18\n",
      "avg loss: 0.256\n",
      "model action_tally: [128. 270. 186.  41. 319.  90.]\n",
      "train_tally:        [151. 147.  86.   2. 150. 151.]\n",
      "discounted_rewards mean: 6.8717866127669264e-18\n",
      "avg loss: 0.234\n",
      "model action_tally: [128. 270. 186.  41. 319.  90.]\n",
      "train_tally:        [150. 141.  86.   2. 151. 151.]\n",
      "\n",
      "116) frames played: 1158, score: -20.0\n",
      "discounted_rewards mean: 4.908758105423835e-17\n",
      "Shifting rewards by 0.027250641415619475\n",
      "new discounted_rewards mean: 0.02725064141561955\n",
      "avg loss: 1.118\n",
      "model action_tally: [196.  76. 145.   1. 564. 176.]\n",
      "train_tally:        [ 77. 156.  83.  74. 157.  75.]\n",
      "discounted_rewards mean: 4.908758105423835e-17\n",
      "Shifting rewards by 0.027250641415619475\n",
      "new discounted_rewards mean: 0.02725064141561955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 0.861\n",
      "model action_tally: [196.  76. 145.   1. 564. 176.]\n",
      "train_tally:        [ 79. 151.  83.  74. 151.  75.]\n",
      "discounted_rewards mean: 4.908758105423835e-17\n",
      "Shifting rewards by 0.027250641415619475\n",
      "new discounted_rewards mean: 0.02725064141561955\n",
      "avg loss: 0.783\n",
      "model action_tally: [196.  76. 145.   1. 564. 176.]\n",
      "train_tally:        [ 79. 154.  83.  74. 154.  75.]\n",
      "\n",
      "117) frames played: 1161, score: -20.0\n",
      "discounted_rewards mean: -1.9584295903809824e-16\n",
      "Shifting rewards by 0.030004636237914117\n",
      "new discounted_rewards mean: 0.030004636237913957\n",
      "avg loss: 1.071\n",
      "model action_tally: [122. 414. 131.  72. 221. 201.]\n",
      "train_tally:        [168. 170.  29.   6. 170.  95.]\n",
      "discounted_rewards mean: -1.9584295903809824e-16\n",
      "Shifting rewards by 0.030004636237914117\n",
      "new discounted_rewards mean: 0.030004636237913957\n",
      "avg loss: 0.876\n",
      "model action_tally: [122. 414. 131.  72. 221. 201.]\n",
      "train_tally:        [177. 172.  29.   6. 177.  95.]\n",
      "discounted_rewards mean: -1.9584295903809824e-16\n",
      "Shifting rewards by 0.030004636237914117\n",
      "new discounted_rewards mean: 0.030004636237913957\n",
      "avg loss: 0.782\n",
      "model action_tally: [122. 414. 131.  72. 221. 201.]\n",
      "train_tally:        [181. 175.  29.   6. 182.  95.]\n",
      "\n",
      "118) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: -1.9466924267400004e-16\n",
      "avg loss: 0.703\n",
      "model action_tally: [352. 165.  87.   0. 323.  95.]\n",
      "train_tally:        [120. 124. 125. 125.  72. 119.]\n",
      "discounted_rewards mean: -1.9466924267400004e-16\n",
      "avg loss: 0.427\n",
      "model action_tally: [352. 165.  87.   0. 323.  95.]\n",
      "train_tally:        [129. 131. 132. 131.  72. 120.]\n",
      "discounted_rewards mean: -1.9466924267400004e-16\n",
      "avg loss: 0.373\n",
      "model action_tally: [352. 165.  87.   0. 323.  95.]\n",
      "train_tally:        [121. 123. 123. 122.  72. 122.]\n",
      "\n",
      "119) frames played: 1114, score: -21.0\n",
      "discounted_rewards mean: 1.5307922493933936e-16\n",
      "Shifting rewards by -0.03398596477519115\n",
      "new discounted_rewards mean: -0.03398596477519102\n",
      "avg loss: 0.959\n",
      "model action_tally: [123.  48. 453.  85. 386.  19.]\n",
      "train_tally:        [ 65. 123.  85.  14. 178. 178.]\n",
      "discounted_rewards mean: 1.5307922493933936e-16\n",
      "Shifting rewards by -0.03398596477519115\n",
      "new discounted_rewards mean: -0.03398596477519102\n",
      "avg loss: 0.655\n",
      "model action_tally: [123.  48. 453.  85. 386.  19.]\n",
      "train_tally:        [ 65. 123.  85.  14. 181. 182.]\n",
      "discounted_rewards mean: 1.5307922493933936e-16\n",
      "Shifting rewards by -0.03398596477519115\n",
      "new discounted_rewards mean: -0.03398596477519102\n",
      "avg loss: 0.530\n",
      "model action_tally: [123.  48. 453.  85. 386.  19.]\n",
      "train_tally:        [ 65. 123.  85.  14. 172. 172.]\n",
      "\n",
      "120) frames played: 1119, score: -20.0\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.012517211086961562\n",
      "new discounted_rewards mean: 0.012517211086961669\n",
      "avg loss: 1.208\n",
      "model action_tally: [137. 244.  98.  68. 308. 264.]\n",
      "train_tally:        [165. 132.  32.  31. 166.  71.]\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.012517211086961562\n",
      "new discounted_rewards mean: 0.012517211086961669\n",
      "avg loss: 0.965\n",
      "model action_tally: [137. 244.  98.  68. 308. 264.]\n",
      "train_tally:        [158. 131.  32.  31. 158.  71.]\n",
      "discounted_rewards mean: 1.0159681655193568e-16\n",
      "Shifting rewards by 0.012517211086961562\n",
      "new discounted_rewards mean: 0.012517211086961669\n",
      "avg loss: 0.840\n",
      "model action_tally: [137. 244.  98.  68. 308. 264.]\n",
      "train_tally:        [158. 134.  32.  31. 158.  71.]\n",
      "\n",
      "121) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: -4.171483771585715e-17\n",
      "avg loss: 0.797\n",
      "model action_tally: [297. 254.  90.   9. 216. 156.]\n",
      "train_tally:        [128. 138. 138.  25. 138. 139.]\n",
      "discounted_rewards mean: -4.171483771585715e-17\n",
      "avg loss: 0.559\n",
      "model action_tally: [297. 254.  90.   9. 216. 156.]\n",
      "train_tally:        [128. 130. 138.  25. 138. 138.]\n",
      "discounted_rewards mean: -4.171483771585715e-17\n",
      "avg loss: 0.518\n",
      "model action_tally: [297. 254.  90.   9. 216. 156.]\n",
      "train_tally:        [128. 133. 133.  25. 133. 133.]\n",
      "\n",
      "122) frames played: 1010, score: -21.0\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "avg loss: 0.704\n",
      "model action_tally: [125. 134. 235.   0. 334. 182.]\n",
      "train_tally:        [142. 142. 101.  35. 130.  82.]\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "avg loss: 0.401\n",
      "model action_tally: [125. 134. 235.   0. 334. 182.]\n",
      "train_tally:        [134. 135. 102.  35. 131.  82.]\n",
      "discounted_rewards mean: 3.51753829584208e-17\n",
      "avg loss: 0.348\n",
      "model action_tally: [125. 134. 235.   0. 334. 182.]\n",
      "train_tally:        [132. 132. 102.  35. 126.  82.]\n",
      "\n",
      "123) frames played: 1026, score: -21.0\n",
      "discounted_rewards mean: -6.232831015439475e-17\n",
      "avg loss: 0.638\n",
      "model action_tally: [331. 205. 131.  34. 245.  80.]\n",
      "train_tally:        [117. 130. 137.  53.  99. 138.]\n",
      "discounted_rewards mean: -6.232831015439475e-17\n",
      "avg loss: 0.423\n",
      "model action_tally: [331. 205. 131.  34. 245.  80.]\n",
      "train_tally:        [117. 125. 130.  53.  98. 131.]\n",
      "discounted_rewards mean: -6.232831015439475e-17\n",
      "avg loss: 0.371\n",
      "model action_tally: [331. 205. 131.  34. 245.  80.]\n",
      "train_tally:        [115. 125. 136.  53. 100. 136.]\n",
      "\n",
      "124) frames played: 1031, score: -21.0\n",
      "discounted_rewards mean: 1.068226227379394e-16\n",
      "avg loss: 0.680\n",
      "model action_tally: [142. 179. 279.  51. 123. 257.]\n",
      "train_tally:        [115. 115. 104.  15.  51. 114.]\n",
      "discounted_rewards mean: 1.068226227379394e-16\n",
      "avg loss: 0.430\n",
      "model action_tally: [142. 179. 279.  51. 123. 257.]\n",
      "train_tally:        [109. 111. 106.  15.  51. 111.]\n",
      "discounted_rewards mean: 1.068226227379394e-16\n",
      "avg loss: 0.364\n",
      "model action_tally: [142. 179. 279.  51. 123. 257.]\n",
      "train_tally:        [124. 125. 104.  15.  51. 112.]\n",
      "\n",
      "125) frames played: 1028, score: -21.0\n",
      "discounted_rewards mean: -1.658854636015798e-16\n",
      "avg loss: 0.618\n",
      "model action_tally: [134. 571. 114.   2. 113.  94.]\n",
      "train_tally:        [105. 129. 137. 137. 136.  88.]\n",
      "discounted_rewards mean: -1.658854636015798e-16\n",
      "avg loss: 0.339\n",
      "model action_tally: [134. 571. 114.   2. 113.  94.]\n",
      "train_tally:        [100. 134. 135. 134. 135.  88.]\n",
      "discounted_rewards mean: -1.658854636015798e-16\n",
      "avg loss: 0.279\n",
      "model action_tally: [134. 571. 114.   2. 113.  94.]\n",
      "train_tally:        [105. 127. 135. 134. 135.  88.]\n",
      "\n",
      "126) frames played: 1123, score: -21.0\n",
      "discounted_rewards mean: -1.5185241013572934e-16\n",
      "Shifting rewards by -0.02920131426457544\n",
      "new discounted_rewards mean: -0.029201314264575583\n",
      "avg loss: 0.992\n",
      "model action_tally: [147. 127. 202. 228. 343.  76.]\n",
      "train_tally:        [120. 117.  80.  42. 121. 100.]\n",
      "discounted_rewards mean: -1.5185241013572934e-16\n",
      "Shifting rewards by -0.02920131426457544\n",
      "new discounted_rewards mean: -0.029201314264575583\n",
      "avg loss: 0.784\n",
      "model action_tally: [147. 127. 202. 228. 343.  76.]\n",
      "train_tally:        [126. 125.  80.  42. 126. 100.]\n",
      "discounted_rewards mean: -1.5185241013572934e-16\n",
      "Shifting rewards by -0.02920131426457544\n",
      "new discounted_rewards mean: -0.029201314264575583\n",
      "avg loss: 0.713\n",
      "model action_tally: [147. 127. 202. 228. 343.  76.]\n",
      "train_tally:        [118. 118.  80.  42. 117.  98.]\n",
      "\n",
      "127) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: 2.187888189290631e-16\n",
      "avg loss: 0.787\n",
      "model action_tally: [534. 162. 103.  10. 116.  98.]\n",
      "train_tally:        [115. 128. 129.  44. 130. 127.]\n",
      "discounted_rewards mean: 2.187888189290631e-16\n",
      "avg loss: 0.511\n",
      "model action_tally: [534. 162. 103.  10. 116.  98.]\n",
      "train_tally:        [114. 133. 135.  44. 136. 125.]\n",
      "discounted_rewards mean: 2.187888189290631e-16\n",
      "avg loss: 0.423\n",
      "model action_tally: [534. 162. 103.  10. 116.  98.]\n",
      "train_tally:        [111. 136. 139.  44. 140. 124.]\n",
      "\n",
      "128) frames played: 1021, score: -21.0\n",
      "discounted_rewards mean: -2.6097309099905734e-16\n",
      "avg loss: 0.522\n",
      "model action_tally: [114. 176. 218.  50. 330. 133.]\n",
      "train_tally:        [149. 150.  99.   9.  67. 150.]\n",
      "discounted_rewards mean: -2.6097309099905734e-16\n",
      "avg loss: 0.351\n",
      "model action_tally: [114. 176. 218.  50. 330. 133.]\n",
      "train_tally:        [153. 160.  99.   9.  67. 160.]\n",
      "discounted_rewards mean: -2.6097309099905734e-16\n",
      "avg loss: 0.286\n",
      "model action_tally: [114. 176. 218.  50. 330. 133.]\n",
      "train_tally:        [154. 155.  99.   9.  67. 154.]\n",
      "\n",
      "129) frames played: 1246, score: -19.0\n",
      "discounted_rewards mean: -4.5620721397117187e-17\n",
      "Shifting rewards by 0.09414801767794362\n",
      "new discounted_rewards mean: 0.0941480176779436\n",
      "avg loss: 1.259\n",
      "model action_tally: [517. 143. 132.   2. 382.  70.]\n",
      "train_tally:        [143.  73.  92. 158. 158.  39.]\n",
      "discounted_rewards mean: -4.5620721397117187e-17\n",
      "Shifting rewards by 0.09414801767794362\n",
      "new discounted_rewards mean: 0.0941480176779436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg loss: 1.043\n",
      "model action_tally: [517. 143. 132.   2. 382.  70.]\n",
      "train_tally:        [140.  73.  92. 164. 164.  39.]\n",
      "discounted_rewards mean: -4.5620721397117187e-17\n",
      "Shifting rewards by 0.09414801767794362\n",
      "new discounted_rewards mean: 0.0941480176779436\n",
      "avg loss: 0.945\n",
      "model action_tally: [517. 143. 132.   2. 382.  70.]\n",
      "train_tally:        [142.  73.  92. 178. 179.  39.]\n",
      "\n",
      "130) frames played: 1521, score: -20.0\n",
      "discounted_rewards mean: 1.4014649620514797e-16\n",
      "Shifting rewards by 0.1631584226899891\n",
      "new discounted_rewards mean: 0.16315842268998926\n",
      "avg loss: 1.519\n",
      "model action_tally: [289. 247. 281. 283. 262. 159.]\n",
      "train_tally:        [134. 171. 136. 171. 170. 172.]\n",
      "discounted_rewards mean: 1.4014649620514797e-16\n",
      "Shifting rewards by 0.1631584226899891\n",
      "new discounted_rewards mean: 0.16315842268998926\n",
      "avg loss: 1.306\n",
      "model action_tally: [289. 247. 281. 283. 262. 159.]\n",
      "train_tally:        [134. 177. 135. 175. 167. 178.]\n",
      "discounted_rewards mean: 1.4014649620514797e-16\n",
      "Shifting rewards by 0.1631584226899891\n",
      "new discounted_rewards mean: 0.16315842268998926\n",
      "avg loss: 1.142\n",
      "model action_tally: [289. 247. 281. 283. 262. 159.]\n",
      "train_tally:        [135. 176. 136. 173. 172. 177.]\n",
      "\n",
      "131) frames played: 1382, score: -20.0\n",
      "discounted_rewards mean: -4.113127269233575e-17\n",
      "Shifting rewards by 0.11191934729877247\n",
      "new discounted_rewards mean: 0.11191934729877251\n",
      "avg loss: 1.461\n",
      "model action_tally: [225. 126. 202. 357. 344. 128.]\n",
      "train_tally:        [201.  75. 201. 117. 202.  57.]\n",
      "discounted_rewards mean: -4.113127269233575e-17\n",
      "Shifting rewards by 0.11191934729877247\n",
      "new discounted_rewards mean: 0.11191934729877251\n",
      "avg loss: 1.273\n",
      "model action_tally: [225. 126. 202. 357. 344. 128.]\n",
      "train_tally:        [193.  75. 193. 117. 193.  57.]\n",
      "discounted_rewards mean: -4.113127269233575e-17\n",
      "Shifting rewards by 0.11191934729877247\n",
      "new discounted_rewards mean: 0.11191934729877251\n",
      "avg loss: 1.190\n",
      "model action_tally: [225. 126. 202. 357. 344. 128.]\n",
      "train_tally:        [191.  75. 191. 117. 192.  57.]\n",
      "\n",
      "132) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "avg loss: 1.028\n",
      "model action_tally: [172. 298. 113.  70. 274.  96.]\n",
      "train_tally:        [127. 132.  74. 133. 133.  66.]\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "avg loss: 0.704\n",
      "model action_tally: [172. 298. 113.  70. 274.  96.]\n",
      "train_tally:        [121. 129.  74. 130. 130.  66.]\n",
      "discounted_rewards mean: -7.640244470538712e-17\n",
      "avg loss: 0.651\n",
      "model action_tally: [172. 298. 113.  70. 274.  96.]\n",
      "train_tally:        [127. 134.  74. 131. 135.  66.]\n",
      "\n",
      "133) frames played: 1101, score: -21.0\n",
      "discounted_rewards mean: 3.549486872552726e-16\n",
      "Shifting rewards by -0.04179832820619313\n",
      "new discounted_rewards mean: -0.04179832820619275\n",
      "avg loss: 0.737\n",
      "model action_tally: [139. 289. 145. 172. 283.  73.]\n",
      "train_tally:        [111. 117. 118.  16.  88. 112.]\n",
      "discounted_rewards mean: 3.549486872552726e-16\n",
      "Shifting rewards by -0.04179832820619313\n",
      "new discounted_rewards mean: -0.04179832820619275\n",
      "avg loss: 0.540\n",
      "model action_tally: [139. 289. 145. 172. 283.  73.]\n",
      "train_tally:        [112. 118. 118.  16.  87. 117.]\n",
      "discounted_rewards mean: 3.549486872552726e-16\n",
      "Shifting rewards by -0.04179832820619313\n",
      "new discounted_rewards mean: -0.04179832820619275\n",
      "avg loss: 0.445\n",
      "model action_tally: [139. 289. 145. 172. 283.  73.]\n",
      "train_tally:        [108. 122. 123.  16.  87. 118.]\n",
      "\n",
      "134) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 0.795\n",
      "model action_tally: [137. 137. 495.   0. 118. 127.]\n",
      "train_tally:        [142. 143.  94.  14.  97.  93.]\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 0.486\n",
      "model action_tally: [137. 137. 495.   0. 118. 127.]\n",
      "train_tally:        [149. 150.  94.  14.  97.  93.]\n",
      "discounted_rewards mean: 1.7518312025643496e-16\n",
      "avg loss: 0.386\n",
      "model action_tally: [137. 137. 495.   0. 118. 127.]\n",
      "train_tally:        [150. 150.  94.  14.  97.  93.]\n",
      "\n",
      "135) frames played: 1010, score: -21.0\n",
      "discounted_rewards mean: -4.502449018677863e-16\n",
      "avg loss: 0.619\n",
      "model action_tally: [289. 342.  99.   8. 183.  89.]\n",
      "train_tally:        [112. 122. 107. 114. 122. 123.]\n",
      "discounted_rewards mean: -4.502449018677863e-16\n",
      "avg loss: 0.372\n",
      "model action_tally: [289. 342.  99.   8. 183.  89.]\n",
      "train_tally:        [113. 119. 107. 120. 120. 121.]\n",
      "discounted_rewards mean: -4.502449018677863e-16\n",
      "avg loss: 0.333\n",
      "model action_tally: [289. 342.  99.   8. 183.  89.]\n",
      "train_tally:        [ 99. 124. 107. 116. 126. 126.]\n",
      "\n",
      "136) frames played: 1152, score: -20.0\n",
      "discounted_rewards mean: -7.401486830834377e-17\n",
      "Shifting rewards by 0.025162766944421343\n",
      "new discounted_rewards mean: 0.025162766944421284\n",
      "avg loss: 0.995\n",
      "model action_tally: [131. 158. 147. 145. 368. 203.]\n",
      "train_tally:        [153. 153.  52.  64. 153.  84.]\n",
      "discounted_rewards mean: -7.401486830834377e-17\n",
      "Shifting rewards by 0.025162766944421343\n",
      "new discounted_rewards mean: 0.025162766944421284\n",
      "avg loss: 0.801\n",
      "model action_tally: [131. 158. 147. 145. 368. 203.]\n",
      "train_tally:        [156. 157.  52.  64. 154.  83.]\n",
      "discounted_rewards mean: -7.401486830834377e-17\n",
      "Shifting rewards by 0.025162766944421343\n",
      "new discounted_rewards mean: 0.025162766944421284\n",
      "avg loss: 0.686\n",
      "model action_tally: [131. 158. 147. 145. 368. 203.]\n",
      "train_tally:        [149. 149.  52.  64. 148.  84.]\n",
      "\n",
      "137) frames played: 1017, score: -21.0\n",
      "discounted_rewards mean: 6.986654235595872e-17\n",
      "avg loss: 0.488\n",
      "model action_tally: [473. 201. 106.  43. 104.  90.]\n",
      "train_tally:        [127. 125. 128.   2.  34.  97.]\n",
      "discounted_rewards mean: 6.986654235595872e-17\n",
      "avg loss: 0.306\n",
      "model action_tally: [473. 201. 106.  43. 104.  90.]\n",
      "train_tally:        [126. 134. 135.   2.  34.  97.]\n",
      "discounted_rewards mean: 6.986654235595872e-17\n",
      "avg loss: 0.251\n",
      "model action_tally: [473. 201. 106.  43. 104.  90.]\n",
      "train_tally:        [125. 126. 127.   2.  34.  97.]\n",
      "\n",
      "138) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: 4.6929192430339295e-17\n",
      "avg loss: 0.678\n",
      "model action_tally: [203. 145. 501.   0.  95.  78.]\n",
      "train_tally:        [122. 126. 109. 127. 127.  77.]\n",
      "discounted_rewards mean: 4.6929192430339295e-17\n",
      "avg loss: 0.396\n",
      "model action_tally: [203. 145. 501.   0.  95.  78.]\n",
      "train_tally:        [121. 127. 108. 132. 133.  77.]\n",
      "discounted_rewards mean: 4.6929192430339295e-17\n",
      "avg loss: 0.298\n",
      "model action_tally: [203. 145. 501.   0.  95.  78.]\n",
      "train_tally:        [127. 130. 109. 131. 129.  77.]\n",
      "\n",
      "139) frames played: 1136, score: -21.0\n",
      "discounted_rewards mean: 7.50573312422641e-17\n",
      "Shifting rewards by -0.011335196846161263\n",
      "new discounted_rewards mean: -0.011335196846161166\n",
      "avg loss: 0.892\n",
      "model action_tally: [165. 200. 224. 176. 307.  64.]\n",
      "train_tally:        [110.  97. 144.  43. 145. 146.]\n",
      "discounted_rewards mean: 7.50573312422641e-17\n",
      "Shifting rewards by -0.011335196846161263\n",
      "new discounted_rewards mean: -0.011335196846161166\n",
      "avg loss: 0.569\n",
      "model action_tally: [165. 200. 224. 176. 307.  64.]\n",
      "train_tally:        [112.  96. 146.  43. 146. 147.]\n",
      "discounted_rewards mean: 7.50573312422641e-17\n",
      "Shifting rewards by -0.011335196846161263\n",
      "new discounted_rewards mean: -0.011335196846161166\n",
      "avg loss: 0.524\n",
      "model action_tally: [165. 200. 224. 176. 307.  64.]\n",
      "train_tally:        [110.  96. 143.  43. 143. 144.]\n",
      "\n",
      "140) frames played: 1026, score: -21.0\n",
      "discounted_rewards mean: 2.7008934400237725e-16\n",
      "avg loss: 0.735\n",
      "model action_tally: [164. 155. 271.  14.  72. 350.]\n",
      "train_tally:        [137. 139. 112.   5. 140.  79.]\n",
      "discounted_rewards mean: 2.7008934400237725e-16\n",
      "avg loss: 0.436\n",
      "model action_tally: [164. 155. 271.  14.  72. 350.]\n",
      "train_tally:        [143. 143. 113.   5. 143.  79.]\n",
      "discounted_rewards mean: 2.7008934400237725e-16\n",
      "avg loss: 0.397\n",
      "model action_tally: [164. 155. 271.  14.  72. 350.]\n",
      "train_tally:        [142. 137. 113.   5. 142.  79.]\n",
      "\n",
      "141) frames played: 1023, score: -21.0\n",
      "discounted_rewards mean: 2.0837030374196486e-17\n",
      "avg loss: 0.750\n",
      "model action_tally: [211. 139.  99.   0. 465. 109.]\n",
      "train_tally:        [115. 133. 134.  51.  64. 132.]\n",
      "discounted_rewards mean: 2.0837030374196486e-17\n",
      "avg loss: 0.426\n",
      "model action_tally: [211. 139.  99.   0. 465. 109.]\n",
      "train_tally:        [112. 145. 145.  51.  64. 144.]\n",
      "discounted_rewards mean: 2.0837030374196486e-17\n",
      "avg loss: 0.361\n",
      "model action_tally: [211. 139.  99.   0. 465. 109.]\n",
      "train_tally:        [115. 142. 143.  51.  64. 140.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "142) frames played: 1015, score: -21.0\n",
      "discounted_rewards mean: 2.24450999658209e-16\n",
      "avg loss: 0.496\n",
      "model action_tally: [141. 223. 329.  64.  90. 168.]\n",
      "train_tally:        [115. 115.  96.  42.  86.  91.]\n",
      "discounted_rewards mean: 2.24450999658209e-16\n",
      "avg loss: 0.305\n",
      "model action_tally: [141. 223. 329.  64.  90. 168.]\n",
      "train_tally:        [116. 116.  94.  42.  86.  91.]\n",
      "discounted_rewards mean: 2.24450999658209e-16\n",
      "avg loss: 0.272\n",
      "model action_tally: [141. 223. 329.  64.  90. 168.]\n",
      "train_tally:        [113. 113.  92.  42.  86.  91.]\n",
      "\n",
      "143) frames played: 1012, score: -21.0\n",
      "discounted_rewards mean: -9.303054593696964e-17\n",
      "avg loss: 0.637\n",
      "model action_tally: [441. 140.  98.  59. 183.  91.]\n",
      "train_tally:        [120. 139. 139.  16.  60. 133.]\n",
      "discounted_rewards mean: -9.303054593696964e-17\n",
      "avg loss: 0.313\n",
      "model action_tally: [441. 140.  98.  59. 183.  91.]\n",
      "train_tally:        [122. 136. 137.  16.  60. 132.]\n",
      "discounted_rewards mean: -9.303054593696964e-17\n",
      "avg loss: 0.303\n",
      "model action_tally: [441. 140.  98.  59. 183.  91.]\n",
      "train_tally:        [123. 145. 145.  16.  60. 131.]\n",
      "\n",
      "144) frames played: 1014, score: -21.0\n",
      "discounted_rewards mean: -2.4876003076413763e-16\n",
      "avg loss: 0.475\n",
      "model action_tally: [151. 346. 220.  33.  93. 171.]\n",
      "train_tally:        [124. 129.  95.   4. 130.  97.]\n",
      "discounted_rewards mean: -2.4876003076413763e-16\n",
      "avg loss: 0.316\n",
      "model action_tally: [151. 346. 220.  33.  93. 171.]\n",
      "train_tally:        [118. 120.  96.   4. 121.  97.]\n",
      "discounted_rewards mean: -2.4876003076413763e-16\n",
      "avg loss: 0.296\n",
      "model action_tally: [151. 346. 220.  33.  93. 171.]\n",
      "train_tally:        [116. 126.  91.   4. 126.  97.]\n",
      "\n",
      "145) frames played: 1022, score: -21.0\n",
      "discounted_rewards mean: 9.733462133700002e-17\n",
      "avg loss: 0.584\n",
      "model action_tally: [134. 142.  89.   0. 563.  94.]\n",
      "train_tally:        [126. 141. 140. 141.  63.  90.]\n",
      "discounted_rewards mean: 9.733462133700002e-17\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    #play a game\n",
    "    atari_env = play_game(environment_name, atari_model)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "\n",
    "    #train the model\n",
    "    for ii in range(3):\n",
    "        train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discounted_rewards mean: 5.4898445367778163e-17\n",
      "Shifting rewards by 0.48340174895687293\n",
      "new discounted_rewards mean: 0.48340174895687305\n"
     ]
    }
   ],
   "source": [
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "#display frame\n",
    "frame_num=600\n",
    "discounted_rewards = atari_env.get_discounted_rewards()\n",
    "discounted_rewards_mean_shifted = atari_env.get_discounted_rewards()\n",
    "\n",
    "print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "if len(discounted_rewards_mean_shifted) > typical_bad_game_frame_count:\n",
    "    sorted_rewards = np.sort(discounted_rewards_mean_shifted)\n",
    "    desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "    discounted_rewards_mean = np.mean(discounted_rewards_mean_shifted)\n",
    "    reward_shift = (discounted_rewards_mean - desired_median)/2.0\n",
    "    print(\"Shifting rewards by {}\".format(reward_shift))\n",
    "    discounted_rewards_mean_shifted = discounted_rewards_mean_shifted + reward_shift\n",
    "    print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards_mean_shifted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-d4a74d8a4b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0matari_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n\u001b[1;32m      5\u001b[0m     frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    }
   ],
   "source": [
    "frame_num += 4\n",
    "atari_frame = atari_env.frame_buffer[frame_num]\n",
    "\n",
    "print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n",
    "    frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
    "atari_frame.show_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
