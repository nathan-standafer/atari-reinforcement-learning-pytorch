{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "environment_name = \"SpaceInvaders-v4\"\n",
    "typical_bad_game_frame_count = 1200\n",
    "reward_frame_shift = -15\n",
    "\n",
    "# environment_name = \"Pong-v4\"\n",
    "# typical_bad_game_frame_count = 1100\n",
    "# reward_frame_shift = -1\n",
    "\n",
    "action_count = gym.make(environment_name).action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a pytorch model.  for now, accept a 210 x 160 greyscale image and output an array of actions\n",
    "\n",
    "\n",
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_count, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(128, 512, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8320*8, 256)    #64 * 14 * 14 = 12544\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_count)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.20)\n",
    "\n",
    "    def forward(self, img_array):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        \n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320*8)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #fc layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model, use_probability_based_action, max_frames=5000):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name, reward_frame_shift)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    \n",
    "    while not done:\n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        img_array = img_array.reshape((1,3,160,210))\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "\n",
    "        if frame_counter % 10 == 0:  #keep action for 5 steps\n",
    "            output = model(img_tensor)\n",
    "            action_array = output.detach().cpu().numpy()[0]\n",
    "            \n",
    "            if use_probability_based_action:\n",
    "                choices = np.arange(0,6)\n",
    "                action_array_softmax = softmax(action_array)\n",
    "\n",
    "                if np.max(action_array_softmax) < 0.7:\n",
    "                    probability_based_action = np.random.choice(choices, p=action_array_softmax)\n",
    "                else:\n",
    "                    probability_based_action = np.random.choice(choices)\n",
    "                    action_array_softmax[probability_based_action] = 1.0\n",
    "\n",
    "                #print(\"{} - {}\".format(np.argmax(action_array), probability_based_action))\n",
    "\n",
    "                #### use probability_based_action\n",
    "                atari_frame.action_array = action_array_softmax\n",
    "                current_action = probability_based_action\n",
    "            else:\n",
    "                atari_frame.action_array = action_array\n",
    "                current_action = np.argmax(action_array)\n",
    "                \n",
    "            last_action_array = atari_frame.action_array\n",
    "            last_action = current_action\n",
    "        else:\n",
    "            atari_frame.action_array = last_action_array\n",
    "                \n",
    "        #print(f\"current_action: {current_action}, action_array: {action_array}\")\n",
    "        done = atari_frame.done_bool\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def get_batch(atari_env, batch_size):\n",
    "    rand_arr = np.arange(len(atari_env.frame_buffer))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    frame_batch = np.zeros((batch_size, 3, 160, 210))\n",
    "    target_batch = np.zeros(batch_size)\n",
    "    reward_batch = np.zeros(batch_size)\n",
    "    batch_tally = np.zeros(6)\n",
    "    \n",
    "    batch_counter = 0\n",
    "    #for ii in range(batch_size):\n",
    "    for ii in range(len(rand_arr)):\n",
    "        i = rand_arr[ii]\n",
    "        atari_frame = atari_env.frame_buffer[i]\n",
    "        reward = atari_frame.discounted_reward\n",
    "        if reward > 0:\n",
    "            img_array = atari_frame.img_array\n",
    "            img_array = img_array.reshape((3,160,210))\n",
    "            \n",
    "            action_array = atari_frame.action_array\n",
    "            #train_action = np.argmax(action_array)  ###################3\n",
    "            train_action = atari_frame.action_taken\n",
    "\n",
    "            if train_action != np.argmax(batch_tally):\n",
    "                reward_batch[batch_counter] = reward\n",
    "                frame_batch[batch_counter] = img_array\n",
    "                target_batch[batch_counter] = train_action\n",
    "                batch_counter += 1\n",
    "                batch_tally[train_action] += 1\n",
    "        if batch_counter >= batch_size:\n",
    "            break\n",
    "    #print(\"batch_tally: {}\".format(batch_tally))\n",
    "    return frame_batch, target_batch, reward_batch\n",
    "    \n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    model.train()\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(action_count)\n",
    "    train_tally = np.zeros(action_count)\n",
    "    \n",
    "    reward_mean_shift = 0\n",
    "    if len(discounted_rewards) > typical_bad_game_frame_count:\n",
    "        sorted_rewards = np.sort(discounted_rewards)\n",
    "        desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "#        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "#        reward_mean_shift = (discounted_rewards_mean - desired_median)  #/4.0\n",
    "        reward_mean_shift = -1.0 * desired_median\n",
    "    print(\"shifting rewards by: {:.3f}\".format(reward_mean_shift))\n",
    "    \n",
    "    total_loss = 0\n",
    "    epochs = 25\n",
    "    #for ii, reward_ii in enumerate(discounted_rewards):\n",
    "    for i in range(epochs):\n",
    "    \n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        frame_batch, target_batch, reward_batch = get_batch(atari_env, 50)\n",
    "        \n",
    "        reward_batch = reward_batch + reward_mean_shift #shift rewards for long games\n",
    "        #iterate through rewards, update targets as necessary\n",
    "        for i_batch in range(len(reward_batch)):\n",
    "            if reward_batch[i_batch] < 0:\n",
    "                target_batch[i_batch] = target_batch[i_batch]+1\n",
    "                if target_batch[i_batch] >= action_count:\n",
    "                    target_batch[i_batch] = 0\n",
    "                #target_batch[i_batch] = random.randint(0,action_count-1) #update to use min action\n",
    "        \n",
    "        img_tensor = torch.from_numpy(frame_batch).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        output_actions = np.sum(output.cpu().detach().numpy(), axis=0)\n",
    "        #print(\"output_actions: {}\".format(output_actions))\n",
    "        #print(\"output: {}\".format(output))\n",
    "\n",
    "        target = torch.from_numpy(target_batch)\n",
    "        target = target.long().cuda()\n",
    "        #print(\"target: {}\".format(target))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        #print(\"loss: {}\".format(loss))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / epochs))\n",
    "#    print(\"model action_tally: {}\".format(action_tally))\n",
    "#    print(\"train_tally:        {}\".format(train_tally))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "\n",
    "atari_model = AtariModel(action_count)\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 967, score: 285.0\n",
      "actions taken: [  1. 160.   0.   0.   0. 806.]\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.160\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.083\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.067\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.048\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.044\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.023\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.025\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.022\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.018\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.020\n",
      "\n",
      "1) frames played: 963, score: 285.0\n",
      "actions taken: [  1. 130.   0.   0.   0. 832.]\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.156\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.061\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.032\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.030\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.018\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.013\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.008\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.007\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.003\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.001\n",
      "\n",
      "2) frames played: 970, score: 285.0\n",
      "actions taken: [  1. 170.   0.   0.   0. 799.]\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.210\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.043\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.036\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.035\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.023\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.021\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.028\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.015\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.016\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.012\n",
      "\n",
      "3) frames played: 959, score: 285.0\n",
      "actions taken: [  1. 220.   0.   0.   0. 738.]\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.224\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.049\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.025\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.029\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.018\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.019\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.019\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.014\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.009\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.005\n",
      "\n",
      "4) frames played: 970, score: 285.0\n",
      "actions taken: [  1. 290.   0.   0.   0. 679.]\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.147\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.037\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.021\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.018\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.007\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.016\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.009\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.005\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.009\n",
      "shifting rewards by: 0.000\n",
      "avg loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.0001)\n",
    "\n",
    "for i in range(5):\n",
    "    #play a game\n",
    "    use_probability_based_action = False #i % 2 != 0\n",
    "    atari_env = play_game(environment_name, atari_model, use_probability_based_action)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "    print(\"actions taken: {}\".format(atari_env.get_actions_taken()))\n",
    "\n",
    "    #train the model\n",
    "    #if use_probability_based_action:\n",
    "    for y in range(10):\n",
    "        train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discounted_rewards mean: 2.4975140097015822e-17\n"
     ]
    }
   ],
   "source": [
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "#display frame\n",
    "frame_num=600\n",
    "discounted_rewards = atari_env.get_discounted_rewards()\n",
    "discounted_rewards_mean_shifted = atari_env.get_discounted_rewards()\n",
    "\n",
    "print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "if len(discounted_rewards_mean_shifted) > typical_bad_game_frame_count:\n",
    "    sorted_rewards = np.sort(discounted_rewards_mean_shifted)\n",
    "    desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "    discounted_rewards_mean = np.mean(discounted_rewards_mean_shifted)\n",
    "    reward_shift = (discounted_rewards_mean - desired_median)/2.0\n",
    "    print(\"Shifting rewards by {}\".format(reward_shift))\n",
    "    discounted_rewards_mean_shifted = discounted_rewards_mean_shifted + reward_shift\n",
    "    print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards_mean_shifted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-d4a74d8a4b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mframe_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0matari_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matari_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n\u001b[1;32m      5\u001b[0m     frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    }
   ],
   "source": [
    "frame_num += 4\n",
    "atari_frame = atari_env.frame_buffer[frame_num]\n",
    "\n",
    "print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n",
    "    frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
    "atari_frame.show_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
