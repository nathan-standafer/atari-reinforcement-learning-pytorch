{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "environment_name = \"SpaceInvaders-v4\"\n",
    "typical_bad_game_frame_count = 1200\n",
    "reward_frame_shift = -15\n",
    "\n",
    "# environment_name = \"Pong-v4\"\n",
    "# typical_bad_game_frame_count = 1100\n",
    "# reward_frame_shift = -1\n",
    "\n",
    "action_count = gym.make(environment_name).action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariModel(nn.Module):\n",
    "\n",
    "    def __init__(self, action_count, dropout=0.25):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch AtariModel Module\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(AtariModel, self).__init__()\n",
    "        self.action_count = action_count\n",
    "        \n",
    "        # convolutional layer 1  (in_channels, out_channels, kernel_size, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        # convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(32, 128, 3, stride=1, padding=1)\n",
    "        # convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(128, 512, 3, stride=1, padding=1)\n",
    "\n",
    "        # max pooling layer\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        #then into an RNN\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.n_layers = 2\n",
    "        self.hidden_dim = 512\n",
    "        self.lstm = nn.LSTM(512*10*13, self.hidden_dim, self.n_layers, dropout=dropout, batch_first=True)  #10 frames???\n",
    "        \n",
    "        #self.fc1 = nn.Linear(8320*8, 512)  \n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, action_count)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, img_array, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param img_array: The input img array to the neural network\n",
    "        :return\n",
    "        \"\"\"\n",
    "        ## Define forward behavior\n",
    "        #print(\"forward received img_array of shape: {}\".format(img_array.shape))\n",
    "        \n",
    "        batch_size = img_array.size(0)\n",
    "        sequence_length = 5 #### todo - paramertize this\n",
    "        #print(\"batch_size: {}, sequence_length: {}\".format(batch_size, sequence_length))\n",
    "        \n",
    "        #convolutional layers\n",
    "        x = self.maxpool(F.relu(self.conv1(img_array)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))  \n",
    "        \n",
    "        #print(\"x.shape after exiting last max pool: {}\".format(x.shape)) #([1, 512, 10, 13])\n",
    "        \n",
    "        #flatten\n",
    "        x = x.view(-1, 8320*8)  \n",
    "        #print(\"x.view shape: {}\".format(x.shape))  #torch.Size([1, 8320])\n",
    "        \n",
    "        #out_contiguous = x.contiguous().view(-1, batch_size, 512*10*13)  #66560\n",
    "        out_contiguous = x.contiguous().view(-1, sequence_length, 512*10*13)  #66560\n",
    "        \n",
    "        #into LTSM\n",
    "        #print(\"out_contiguous.shape: {}, hidden[0].shape: {}\".format(out_contiguous.shape, hidden[0].shape))\n",
    "\n",
    "        r_out, hidden = self.lstm(out_contiguous, hidden) \n",
    "        \n",
    "        #fc layers\n",
    "        #x = self.dropout(x)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        r_out = self.dropout(r_out)\n",
    "        r_out = F.relu(self.fc2(r_out))\n",
    "        out_fc = self.fc3(r_out)\n",
    "        #print(\"out_fc.shape: {}\".format(out_fc.shape))  #out_fc.shape: torch.Size([50, 5, 6])\n",
    "        #out_reshaped = out_fc.view(batch_size, -1, self.action_count)  # reshape to be batch_size first\n",
    "        #out_reshaped = out_fc.view(-1, self.action_count)  # reshape to be batch_size first\n",
    "        #print(\"out_reshaped is: {}\".format(out_reshaped))\n",
    "        #out = out_reshaped[:, -1] ##### get last batch of labels\n",
    "        out = out_fc[:, -1, :]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "       \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(env_name, model, use_probability_based_action, max_frames=5000):\n",
    "    model.eval()\n",
    "    atari_env = AtariEnv(environment_name, reward_frame_shift)\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    \n",
    "    hidden = model.init_hidden(1) #one batch needed for playing\n",
    "    \n",
    "    while not done:\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "    \n",
    "        atari_frame = atari_env.step(current_action)\n",
    "        img_array = get_play_sequence(atari_env)\n",
    "        img_tensor = torch.from_numpy(img_array).float().cuda()\n",
    "        output, hidden = model(img_tensor, hidden)\n",
    "        #print(\"output: {}\".format(output))\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        \n",
    "        if use_probability_based_action:\n",
    "            choices = np.arange(0,6)\n",
    "            action_array_softmax = softmax(action_array)\n",
    "            probability_based_action = np.random.choice(choices, p=action_array_softmax)\n",
    "            #print(\"{} - {}\".format(np.argmax(action_array), probability_based_action))\n",
    "\n",
    "            #### use probability_based_action\n",
    "            atari_frame.action_array = action_array_softmax\n",
    "            current_action = probability_based_action\n",
    "        else:\n",
    "            atari_frame.action_array = action_array\n",
    "            current_action = np.argmax(action_array)\n",
    "\n",
    "        done = atari_frame.done_bool\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "    atari_env.close()\n",
    "    return atari_env, hidden\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def get_play_sequence(atari_env, sequence_length=5):\n",
    "    frame_batch = np.zeros((sequence_length, 3, 160, 210))\n",
    "    \n",
    "    for ii in range(-sequence_length, 0):\n",
    "        this_frame_index = len(atari_env.frame_buffer) + ii\n",
    "        if this_frame_index < 0:\n",
    "            this_frame_index = 0\n",
    "        \n",
    "        atari_frame = atari_env.frame_buffer[this_frame_index]\n",
    "        \n",
    "        img_array = atari_frame.img_array\n",
    "        img_array = img_array.reshape((3,160,210))\n",
    "        frame_batch[ii + sequence_length] = img_array\n",
    "        \n",
    "    return frame_batch\n",
    "\n",
    "def get_train_batch(atari_env, batch_size, sequence_length=5):\n",
    "    rand_arr = np.arange(len(atari_env.frame_buffer))\n",
    "    np.random.shuffle(rand_arr)\n",
    "    \n",
    "    batch_index_counter = 0\n",
    "    frame_batch = np.zeros((batch_size*sequence_length, 3, 160, 210))\n",
    "    target_batch = np.zeros(batch_size*sequence_length)\n",
    "    reward_batch = np.zeros(batch_size*sequence_length)\n",
    "    \n",
    "    for ii_batch in range(batch_size):\n",
    "        batch_end_index = rand_arr[ii_batch]\n",
    "        \n",
    "        for ii in range(batch_end_index-sequence_length, batch_end_index):\n",
    "            this_frame_index = ii\n",
    "            if this_frame_index < 0:\n",
    "                this_frame_index = 0\n",
    "\n",
    "            #print(\"batch_end_index: {}\".format(batch_end_index))\n",
    "            #print(\"this_frame_index: {}\".format(this_frame_index))\n",
    "            atari_frame = atari_env.frame_buffer[this_frame_index]\n",
    "\n",
    "            img_array = atari_frame.img_array\n",
    "            img_array = img_array.reshape((3,160,210))\n",
    "            frame_batch[batch_index_counter] = img_array\n",
    "      \n",
    "            batch_index_counter += 1\n",
    "    \n",
    "        action_array = atari_frame.action_array\n",
    "        train_action = np.argmax(action_array)\n",
    "        target_batch[batch_index_counter-1] = train_action\n",
    "\n",
    "        reward = atari_frame.discounted_reward\n",
    "        reward_batch[batch_index_counter-1] = reward\n",
    "            \n",
    "    return frame_batch, target_batch, reward_batch\n",
    "    \n",
    "\n",
    "def train(atari_env, model, optimizer, criterion):\n",
    "    train_batch_size = 50\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(train_batch_size)\n",
    "    print(\"init hidden[0] shape: {}\".format(hidden[0].shape))\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    frame_buffer = atari_env.frame_buffer\n",
    "    action_tally = np.zeros(action_count)\n",
    "    train_tally = np.zeros(action_count)\n",
    "    \n",
    "    reward_mean_shift = 0\n",
    "    if len(discounted_rewards) > typical_bad_game_frame_count:\n",
    "        sorted_rewards = np.sort(discounted_rewards)\n",
    "        desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "#        discounted_rewards_mean = np.mean(discounted_rewards)\n",
    "#        reward_mean_shift = (discounted_rewards_mean - desired_median)  #/4.0\n",
    "        reward_mean_shift = -1.0 * desired_median\n",
    "    print(\"shifting rewards by: {:.3f}\".format(reward_mean_shift))\n",
    "    \n",
    "    total_loss = 0\n",
    "    epochs = 25\n",
    "    #for ii, reward_ii in enumerate(discounted_rewards):\n",
    "    for i in range(epochs):\n",
    "    \n",
    "        #print(\"{}: {}\".format(i, reward))\n",
    "        optimizer.zero_grad()\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        frame_batch, target_batch, reward_batch = get_train_batch(atari_env, train_batch_size)\n",
    "        \n",
    "        reward_batch = reward_batch + reward_mean_shift #shift rewards for long games\n",
    "        #iterate through rewards, update targets as necessary\n",
    "        for i_batch in range(len(reward_batch)):\n",
    "            if reward_batch[i_batch] < 0:\n",
    "#                 target_batch[i_batch] = target_batch[i_batch]+1\n",
    "#                 if target_batch[i_batch] >= action_count:\n",
    "#                     target_batch[i_batch] = 0\n",
    "                target_batch[i_batch] = random.randint(0,action_count-1) #update to use min action\n",
    "        \n",
    "        print(\"train frame batch shape: {}\".format(frame_batch.shape))\n",
    "        img_tensor = torch.from_numpy(frame_batch).float().cuda()\n",
    "        output, hidden = model(img_tensor, hidden)\n",
    "        output_actions = np.sum(output.cpu().detach().numpy(), axis=0)\n",
    "        #print(\"output_actions: {}\".format(output_actions))\n",
    "        #print(\"output: {}\".format(output))\n",
    "\n",
    "        target = torch.from_numpy(target_batch)\n",
    "        target = target.long().cuda()\n",
    "        print(\"output: {}, target: {}\".format(output.shape, target.shape))\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        #print(\"loss: {}\".format(loss))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"avg loss: {:.3f}\".format(total_loss / epochs))\n",
    "#    print(\"model action_tally: {}\".format(action_tally))\n",
    "#    print(\"train_tally:        {}\".format(train_tally))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new model\n",
    "\n",
    "atari_model = AtariModel(action_count)\n",
    "atari_model.cuda()\n",
    "\n",
    "### loss function\n",
    "atari_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0) frames played: 1251, score: 395.0\n",
      "actions taken: [  1. 571. 679.   0.   0.   0.]\n",
      "init hidden[0] shape: torch.Size([2, 50, 512])\n",
      "shifting rewards by: 0.409\n",
      "train frame batch shape: (250, 3, 160, 210)\n",
      "output: torch.Size([50, 6]), target: torch.Size([250])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (50) to match target batch_size (250).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dbf83ab38e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matari_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matari_criterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-cd52602d5945>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(atari_env, model, optimizer, criterion)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output: {}, target: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m#print(\"loss: {}\".format(loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 942\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2054\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2055\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2056\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1867\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1869\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (50) to match target batch_size (250)."
     ]
    }
   ],
   "source": [
    "### optimizer\n",
    "atari_optimizer = optim.Adam(atari_model.parameters(), lr=0.0001)\n",
    "\n",
    "for i in range(1):\n",
    "    #play a game\n",
    "    use_probability_based_action = i % 1 != 0\n",
    "    atari_env, hidden = play_game(environment_name, atari_model, use_probability_based_action)\n",
    "\n",
    "    #discounted_rewards = atari_env.get_discounted_rewards()\n",
    "    #print()\n",
    "    #print(discounted_rewards)\n",
    "    print(\"\\n{}) frames played: {}, score: {}\".format(i, len(atari_env.frame_buffer), atari_env.get_total_score()))\n",
    "    print(\"actions taken: {}\".format(atari_env.get_actions_taken()))\n",
    "\n",
    "    #train the model\n",
    "    train(atari_env, atari_model, atari_optimizer, atari_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no points granted. Setting all discounted rewards to -1\n",
      "no points granted. Setting all discounted rewards to -1\n",
      "discounted_rewards mean: -1.0\n"
     ]
    }
   ],
   "source": [
    "from gym_utils import AtariEnv\n",
    "from gym_utils import AtariFrame\n",
    "\n",
    "#display frame\n",
    "frame_num=600\n",
    "discounted_rewards = atari_env.get_discounted_rewards()\n",
    "discounted_rewards_mean_shifted = atari_env.get_discounted_rewards()\n",
    "\n",
    "print(\"discounted_rewards mean: {}\".format(np.mean(discounted_rewards)))\n",
    "if len(discounted_rewards_mean_shifted) > typical_bad_game_frame_count:\n",
    "    sorted_rewards = np.sort(discounted_rewards_mean_shifted)\n",
    "    desired_median = sorted_rewards[typical_bad_game_frame_count//2]\n",
    "    discounted_rewards_mean = np.mean(discounted_rewards_mean_shifted)\n",
    "    reward_shift = (discounted_rewards_mean - desired_median)/2.0\n",
    "    print(\"Shifting rewards by {}\".format(reward_shift))\n",
    "    discounted_rewards_mean_shifted = discounted_rewards_mean_shifted + reward_shift\n",
    "    print(\"new discounted_rewards mean: {}\".format(np.mean(discounted_rewards_mean_shifted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'discounted_rewards_shifted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d4a74d8a4b95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n\u001b[0;32m----> 5\u001b[0;31m     frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0matari_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'discounted_rewards_shifted' is not defined"
     ]
    }
   ],
   "source": [
    "frame_num += 4\n",
    "atari_frame = atari_env.frame_buffer[frame_num]\n",
    "\n",
    "print(\"frame: {}, original reward: {:.3f}, shifted reward: {:.3f}\".format(\n",
    "    frame_num, discounted_rewards[frame_num], discounted_rewards_shifted[frame_num]))\n",
    "atari_frame.show_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
