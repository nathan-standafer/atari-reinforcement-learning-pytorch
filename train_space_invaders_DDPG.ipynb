{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym_utils_q_learning import AtariEnv\n",
    "from gym_utils_q_learning import AtariFrame\n",
    "from AtariACModels import AtariActorModel, AtariCriticModel\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#environment_name = \"SpaceInvaders-v4\"\n",
    "environment_name  = \"SpaceInvadersNoFrameskip-v4\"\n",
    "#environment_name = 'DemonAttackNoFrameskip-v4'\n",
    "#environment_name = 'Riverraid-v4'\n",
    "\n",
    "# environment_name = \"Pong-v4\"\n",
    "# typical_bad_game_frame_count = 1100\n",
    "# reward_frame_shift = -1\n",
    "\n",
    "action_count = gym.make(environment_name).action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "import time \n",
    "\n",
    "def play_game(atari_env, model, epsilon, max_frames=5000, debug=False):\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    choices = np.arange(action_count)\n",
    "    \n",
    "    while not done:\n",
    "        save_step = frame_counter >= 60\n",
    "        atari_frame = atari_env.step(current_action, save_step=save_step)\n",
    "\n",
    "        if atari_frame is None:  #processed_frames == None\n",
    "            done = True\n",
    "            continue\n",
    "            \n",
    "        processed_frames = atari_frame.get_processed_frames()\n",
    "\n",
    "        processed_frames_batch = np.reshape(processed_frames, (1,)+processed_frames.shape)\n",
    "        img_tensor = torch.from_numpy(processed_frames_batch).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        if debug:\n",
    "            time.sleep(2/60)\n",
    "            print(\"actions from model: {}\".format(action_array))\n",
    "            \n",
    "            #action_for_critic_reshaped = np.reshape(action_array, (1, 1))\n",
    "            action_for_critic_reshaped = np.zeros((1,1))\n",
    "            action_for_critic_reshaped[0][0] = current_action\n",
    "            action_for_critic_reshaped = action_batch_to_one_hot(action_count, action_for_critic_reshaped)\n",
    "            action_for_critic_tensor = torch.from_numpy(action_for_critic_reshaped).float().cuda()\n",
    "            \n",
    "            critic_model_target.eval()\n",
    "            critic_val = critic_model_target(img_tensor, action_for_critic_tensor)\n",
    "            critic_model_target.train()\n",
    "            print(\"critic val for action: {} = {}\".format(action_for_critic_tensor, critic_val))\n",
    "    \n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < epsilon:\n",
    "            current_action = atari_env.env.action_space.sample()\n",
    "        else:\n",
    "            #current_action = np.argmax(action_array)\n",
    "            current_action = np.random.choice(choices, p=action_array)\n",
    "\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "def game_step(atari_env, model, epsilon, max_frames=3000):\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    \n",
    "    frame = atari_env.frame_buffer[-1]\n",
    "    processed_frames = frame.get_next_processed_frames()\n",
    "\n",
    "    processed_frames_batch = np.reshape(processed_frames, (1,)+processed_frames.shape)\n",
    "    img_tensor = torch.from_numpy(processed_frames_batch).float().cuda()\n",
    "    output = model(img_tensor)\n",
    "    action_array = output.detach().cpu().numpy()[0]\n",
    "\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand < epsilon:\n",
    "        current_action = atari_env.env.action_space.sample()\n",
    "    else:\n",
    "        current_action = np.argmax(action_array)\n",
    "\n",
    "    atari_frame = atari_env.step(current_action, save_step=save_step)\n",
    "\n",
    "    if atari_frame is None:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "            \n",
    "def get_train_batch(atari_env, batch_size, lambda_frames, gamma):\n",
    "    rand_arr = np.arange(len(atari_env.frame_buffer)-1-lambda_frames)\n",
    "    np.random.shuffle(rand_arr)\n",
    "   \n",
    "    index_counter = 0\n",
    "    batch_index_counter = 0\n",
    "    \n",
    "    frame_batch        = np.zeros((batch_size, 4, 210, 160))\n",
    "    next_frame_batch   = np.zeros((batch_size, 4, 210, 160))\n",
    "    reward_batch       = np.zeros(batch_size)\n",
    "    actions_batch      = np.zeros(batch_size)\n",
    "    next_actions_batch = np.zeros(batch_size)\n",
    "    frame_number_batch = np.zeros(batch_size)\n",
    "    lambda_reward      = np.zeros(batch_size)\n",
    "    \n",
    "    for batch_index in range(batch_size):\n",
    "        frame_number = rand_arr[batch_index]\n",
    "        \n",
    "        for i in range(lambda_frames):\n",
    "            lambda_reward[batch_index] += gamma**(i+1) * atari_env.frame_buffer[frame_number+i].getReward()\n",
    "        \n",
    "        atari_frame = atari_env.frame_buffer[frame_number]\n",
    "        next_atari_frame = atari_env.frame_buffer[frame_number+1]\n",
    "        lambda_end_atari_frame = atari_env.frame_buffer[frame_number+lambda_frames]\n",
    "        \n",
    "        frame_batch[batch_index]      = atari_frame.get_processed_frames()\n",
    "        next_frame_batch[batch_index] = lambda_end_atari_frame.get_next_processed_frames()\n",
    "        reward_batch[batch_index]     = atari_frame.getReward()\n",
    "        actions_batch[batch_index]    = atari_frame.action_taken\n",
    "        #next_actions_batch[batch_index] = next_atari_frame.action_taken\n",
    "        next_actions_batch[batch_index] = lambda_end_atari_frame.action_taken\n",
    "        frame_number_batch[batch_index] = frame_number\n",
    "\n",
    "    return frame_batch, next_frame_batch, actions_batch, next_actions_batch, reward_batch, frame_number_batch, lambda_reward\n",
    "    \n",
    "def action_batch_to_one_hot(action_count, action_batch):\n",
    "#     print(action_batch.shape)\n",
    "#     print(action_batch)\n",
    "    return_value = np.zeros((action_batch.shape[0], action_count))\n",
    "    for i in range(action_batch.shape[0]):\n",
    "        this_row = np.zeros(action_count)\n",
    "#         print(\"action_batch[i][0]: {}\".format(action_batch[i][0]))\n",
    "        this_row[int(action_batch[i][0])] = 1.0\n",
    "        return_value[i] = this_row\n",
    "    return return_value\n",
    "    \n",
    "    \n",
    "\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_model_local_0  = AtariActorModel(action_count).cuda()\n",
    "actor_model_target_0 = AtariActorModel(action_count).cuda()\n",
    "critic_model_local_0  = AtariCriticModel(action_count).cuda()\n",
    "critic_model_target_0 = AtariCriticModel(action_count).cuda()\n",
    "\n",
    "actor_model_local_1  = AtariActorModel(action_count).cuda()\n",
    "actor_model_target_1 = AtariActorModel(action_count).cuda()\n",
    "critic_model_local_1  = AtariCriticModel(action_count).cuda()\n",
    "critic_model_target_1 = AtariCriticModel(action_count).cuda()\n",
    "\n",
    "atari_env_train = AtariEnv(environment_name)\n",
    "\n",
    "\n",
    "actor_model_local_0.load_state_dict(torch.load(\"{}actor_model_local_0.pt\".format(environment_name)))\n",
    "actor_model_target_0.load_state_dict(torch.load(\"{}actor_model_target_0.pt\".format(environment_name)))\n",
    "critic_model_local_0.load_state_dict(torch.load(\"{}critic_model_local_0.pt\".format(environment_name)))\n",
    "critic_model_target_0.load_state_dict(torch.load(\"{}critic_model_target_0.pt\".format(environment_name)))\n",
    "\n",
    "actor_model_local_1.load_state_dict(torch.load(\"{}actor_model_local_1.pt\".format(environment_name)))\n",
    "actor_model_target_1.load_state_dict(torch.load(\"{}actor_model_target_1.pt\".format(environment_name)))\n",
    "critic_model_local_1.load_state_dict(torch.load(\"{}critic_model_local_1.pt\".format(environment_name)))\n",
    "critic_model_target_1.load_state_dict(torch.load(\"{}critic_model_target_1.pt\".format(environment_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, actor: 0, score: 16.0, epsilon: 0.050, frames ran: 340536, critic loss: 0.106, actor_loss: -0.583\n",
      "  its a playoff!\n",
      "  Actor 0 score: 15.0, Actor 1 score: 20.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 1, actor: 1, score: 8.0, epsilon: 0.050, frames ran: 348216, critic loss: 0.125, actor_loss: -0.587\n",
      "epoch: 2, actor: 0, score: 29.0, epsilon: 0.050, frames ran: 353076, critic loss: 0.122, actor_loss: -0.586\n",
      "epoch: 3, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 355128, critic loss: 0.116, actor_loss: -0.591\n",
      "epoch: 4, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 357132, critic loss: 0.113, actor_loss: -0.591\n",
      "epoch: 5, actor: 1, score: 15.0, epsilon: 0.050, frames ran: 360356, critic loss: 0.120, actor_loss: -0.593\n",
      "epoch: 6, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 362268, critic loss: 0.101, actor_loss: -0.593\n",
      "epoch: 7, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 364296, critic loss: 0.109, actor_loss: -0.594\n",
      "epoch: 8, actor: 0, score: 17.0, epsilon: 0.050, frames ran: 367264, critic loss: 0.110, actor_loss: -0.596\n",
      "epoch: 9, actor: 1, score: 15.0, epsilon: 0.050, frames ran: 372180, critic loss: 0.107, actor_loss: -0.599\n",
      "epoch: 10, actor: 0, score: 10.0, epsilon: 0.050, frames ran: 374036, critic loss: 0.115, actor_loss: -0.598\n",
      "  its a playoff!\n",
      "  Actor 0 score: 10.0, Actor 1 score: 14.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 11, actor: 1, score: 18.0, epsilon: 0.050, frames ran: 382544, critic loss: 0.128, actor_loss: -0.600\n",
      "epoch: 12, actor: 0, score: 22.0, epsilon: 0.050, frames ran: 388288, critic loss: 0.120, actor_loss: -0.599\n",
      "epoch: 13, actor: 1, score: 10.0, epsilon: 0.050, frames ran: 390332, critic loss: 0.122, actor_loss: -0.599\n",
      "epoch: 14, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 393704, critic loss: 0.116, actor_loss: -0.605\n",
      "epoch: 15, actor: 1, score: 8.0, epsilon: 0.050, frames ran: 395652, critic loss: 0.119, actor_loss: -0.603\n",
      "epoch: 16, actor: 0, score: 14.0, epsilon: 0.050, frames ran: 397568, critic loss: 0.115, actor_loss: -0.601\n",
      "epoch: 17, actor: 1, score: 17.0, epsilon: 0.050, frames ran: 400200, critic loss: 0.094, actor_loss: -0.603\n",
      "epoch: 18, actor: 0, score: 11.0, epsilon: 0.050, frames ran: 402172, critic loss: 0.100, actor_loss: -0.605\n",
      "epoch: 19, actor: 1, score: 25.0, epsilon: 0.050, frames ran: 405904, critic loss: 0.102, actor_loss: -0.604\n",
      "epoch: 20, actor: 0, score: 23.0, epsilon: 0.050, frames ran: 409328, critic loss: 0.112, actor_loss: -0.603\n",
      "  its a playoff!\n",
      "  Actor 0 score: 14.0, Actor 1 score: 4.0\n",
      "Actor 0 won.  Overwriting Actor 1.\n",
      "epoch: 21, actor: 1, score: 7.0, epsilon: 0.050, frames ran: 415208, critic loss: 0.093, actor_loss: -0.604\n",
      "epoch: 22, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 417760, critic loss: 0.120, actor_loss: -0.605\n",
      "epoch: 23, actor: 1, score: 5.0, epsilon: 0.050, frames ran: 418944, critic loss: 0.097, actor_loss: -0.607\n",
      "epoch: 24, actor: 0, score: 19.0, epsilon: 0.050, frames ran: 423588, critic loss: 0.098, actor_loss: -0.609\n",
      "epoch: 25, actor: 1, score: 4.0, epsilon: 0.050, frames ran: 424872, critic loss: 0.124, actor_loss: -0.609\n",
      "epoch: 26, actor: 0, score: 17.0, epsilon: 0.050, frames ran: 427468, critic loss: 0.099, actor_loss: -0.605\n",
      "epoch: 27, actor: 1, score: 11.0, epsilon: 0.050, frames ran: 429584, critic loss: 0.096, actor_loss: -0.610\n",
      "epoch: 28, actor: 0, score: 5.0, epsilon: 0.050, frames ran: 430804, critic loss: 0.101, actor_loss: -0.607\n",
      "epoch: 29, actor: 1, score: 15.0, epsilon: 0.050, frames ran: 434104, critic loss: 0.095, actor_loss: -0.611\n",
      "epoch: 30, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 436084, critic loss: 0.103, actor_loss: -0.608\n",
      "  its a playoff!\n",
      "  Actor 0 score: 5.0, Actor 1 score: 5.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 31, actor: 1, score: 4.0, epsilon: 0.050, frames ran: 440312, critic loss: 0.106, actor_loss: -0.610\n",
      "epoch: 32, actor: 0, score: 5.0, epsilon: 0.050, frames ran: 441928, critic loss: 0.096, actor_loss: -0.611\n",
      "epoch: 33, actor: 1, score: 20.0, epsilon: 0.050, frames ran: 445508, critic loss: 0.090, actor_loss: -0.610\n",
      "epoch: 34, actor: 0, score: 22.0, epsilon: 0.050, frames ran: 448584, critic loss: 0.101, actor_loss: -0.613\n",
      "epoch: 35, actor: 1, score: 25.0, epsilon: 0.050, frames ran: 452172, critic loss: 0.096, actor_loss: -0.613\n",
      "epoch: 36, actor: 0, score: 18.0, epsilon: 0.050, frames ran: 455000, critic loss: 0.099, actor_loss: -0.611\n",
      "epoch: 37, actor: 1, score: 19.0, epsilon: 0.050, frames ran: 458608, critic loss: 0.114, actor_loss: -0.612\n",
      "epoch: 38, actor: 0, score: 8.0, epsilon: 0.050, frames ran: 460520, critic loss: 0.107, actor_loss: -0.611\n",
      "epoch: 39, actor: 1, score: 15.0, epsilon: 0.050, frames ran: 462580, critic loss: 0.105, actor_loss: -0.612\n",
      "epoch: 40, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 464476, critic loss: 0.106, actor_loss: -0.614\n",
      "  its a playoff!\n",
      "  Actor 0 score: 13.0, Actor 1 score: 24.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 41, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 473388, critic loss: 0.107, actor_loss: -0.616\n",
      "epoch: 42, actor: 0, score: 10.0, epsilon: 0.050, frames ran: 475264, critic loss: 0.121, actor_loss: -0.617\n",
      "epoch: 43, actor: 1, score: 16.0, epsilon: 0.050, frames ran: 479084, critic loss: 0.109, actor_loss: -0.621\n",
      "epoch: 44, actor: 0, score: 14.0, epsilon: 0.050, frames ran: 481116, critic loss: 0.112, actor_loss: -0.616\n",
      "epoch: 45, actor: 1, score: 6.0, epsilon: 0.050, frames ran: 482688, critic loss: 0.111, actor_loss: -0.619\n",
      "epoch: 46, actor: 0, score: 6.0, epsilon: 0.050, frames ran: 483796, critic loss: 0.097, actor_loss: -0.617\n",
      "epoch: 47, actor: 1, score: 10.0, epsilon: 0.050, frames ran: 485740, critic loss: 0.098, actor_loss: -0.620\n",
      "epoch: 48, actor: 0, score: 21.0, epsilon: 0.050, frames ran: 488252, critic loss: 0.102, actor_loss: -0.621\n",
      "epoch: 49, actor: 1, score: 16.0, epsilon: 0.050, frames ran: 490372, critic loss: 0.122, actor_loss: -0.621\n",
      "epoch: 50, actor: 0, score: 17.0, epsilon: 0.050, frames ran: 493212, critic loss: 0.098, actor_loss: -0.623\n",
      "  its a playoff!\n",
      "  Actor 0 score: 12.0, Actor 1 score: 15.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 51, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 501168, critic loss: 0.110, actor_loss: -0.622\n",
      "epoch: 52, actor: 0, score: 17.0, epsilon: 0.050, frames ran: 503916, critic loss: 0.129, actor_loss: -0.621\n",
      "epoch: 53, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 506048, critic loss: 0.117, actor_loss: -0.624\n",
      "epoch: 54, actor: 0, score: 9.0, epsilon: 0.050, frames ran: 508136, critic loss: 0.105, actor_loss: -0.628\n",
      "epoch: 55, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 510084, critic loss: 0.097, actor_loss: -0.622\n",
      "epoch: 56, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 514340, critic loss: 0.124, actor_loss: -0.623\n",
      "epoch: 57, actor: 1, score: 24.0, epsilon: 0.050, frames ran: 517984, critic loss: 0.116, actor_loss: -0.629\n",
      "epoch: 58, actor: 0, score: 13.0, epsilon: 0.050, frames ran: 521820, critic loss: 0.116, actor_loss: -0.630\n",
      "epoch: 59, actor: 1, score: 11.0, epsilon: 0.050, frames ran: 524248, critic loss: 0.106, actor_loss: -0.625\n",
      "epoch: 60, actor: 0, score: 11.0, epsilon: 0.050, frames ran: 526224, critic loss: 0.105, actor_loss: -0.633\n",
      "  its a playoff!\n",
      "  Actor 0 score: 7.0, Actor 1 score: 16.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n",
      "epoch: 61, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 535016, critic loss: 0.112, actor_loss: -0.638\n",
      "epoch: 62, actor: 0, score: 16.0, epsilon: 0.050, frames ran: 537444, critic loss: 0.137, actor_loss: -0.638\n",
      "epoch: 63, actor: 1, score: 19.0, epsilon: 0.050, frames ran: 540160, critic loss: 0.119, actor_loss: -0.635\n",
      "epoch: 64, actor: 0, score: 10.0, epsilon: 0.050, frames ran: 542204, critic loss: 0.122, actor_loss: -0.637\n",
      "epoch: 65, actor: 1, score: 5.0, epsilon: 0.050, frames ran: 543628, critic loss: 0.116, actor_loss: -0.635\n",
      "epoch: 66, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 548660, critic loss: 0.127, actor_loss: -0.640\n",
      "epoch: 67, actor: 1, score: 4.0, epsilon: 0.050, frames ran: 549808, critic loss: 0.118, actor_loss: -0.643\n",
      "epoch: 68, actor: 0, score: 11.0, epsilon: 0.050, frames ran: 551372, critic loss: 0.122, actor_loss: -0.644\n",
      "epoch: 69, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 553348, critic loss: 0.107, actor_loss: -0.643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70, actor: 0, score: 17.0, epsilon: 0.050, frames ran: 556380, critic loss: 0.117, actor_loss: -0.647\n",
      "  its a playoff!\n",
      "  Actor 0 score: 9.0, Actor 1 score: 6.0\n",
      "Actor 0 won.  Overwriting Actor 1.\n",
      "epoch: 71, actor: 1, score: 11.0, epsilon: 0.050, frames ran: 561664, critic loss: 0.111, actor_loss: -0.646\n",
      "epoch: 72, actor: 0, score: 6.0, epsilon: 0.050, frames ran: 562780, critic loss: 0.109, actor_loss: -0.649\n",
      "epoch: 73, actor: 1, score: 24.0, epsilon: 0.050, frames ran: 565740, critic loss: 0.116, actor_loss: -0.650\n",
      "epoch: 74, actor: 0, score: 18.0, epsilon: 0.050, frames ran: 568300, critic loss: 0.121, actor_loss: -0.651\n",
      "epoch: 75, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 570420, critic loss: 0.121, actor_loss: -0.652\n",
      "epoch: 76, actor: 0, score: 4.0, epsilon: 0.050, frames ran: 571492, critic loss: 0.124, actor_loss: -0.648\n",
      "epoch: 77, actor: 1, score: 11.0, epsilon: 0.050, frames ran: 573364, critic loss: 0.109, actor_loss: -0.652\n",
      "epoch: 78, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 578100, critic loss: 0.113, actor_loss: -0.653\n",
      "epoch: 79, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 580000, critic loss: 0.123, actor_loss: -0.656\n",
      "epoch: 80, actor: 0, score: 11.0, epsilon: 0.050, frames ran: 582096, critic loss: 0.107, actor_loss: -0.657\n",
      "  its a playoff!\n",
      "  Actor 0 score: 11.0, Actor 1 score: 10.0\n",
      "Actor 0 won.  Overwriting Actor 1.\n",
      "epoch: 81, actor: 1, score: 6.0, epsilon: 0.050, frames ran: 586824, critic loss: 0.114, actor_loss: -0.661\n",
      "epoch: 82, actor: 0, score: 6.0, epsilon: 0.050, frames ran: 587988, critic loss: 0.112, actor_loss: -0.657\n",
      "epoch: 83, actor: 1, score: 17.0, epsilon: 0.050, frames ran: 590208, critic loss: 0.122, actor_loss: -0.663\n",
      "epoch: 84, actor: 0, score: 7.0, epsilon: 0.050, frames ran: 591896, critic loss: 0.114, actor_loss: -0.661\n",
      "epoch: 85, actor: 1, score: 13.0, epsilon: 0.050, frames ran: 595996, critic loss: 0.110, actor_loss: -0.667\n",
      "epoch: 86, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 598008, critic loss: 0.124, actor_loss: -0.668\n",
      "epoch: 87, actor: 1, score: 12.0, epsilon: 0.050, frames ran: 599944, critic loss: 0.111, actor_loss: -0.664\n",
      "epoch: 88, actor: 0, score: 14.0, epsilon: 0.050, frames ran: 601928, critic loss: 0.106, actor_loss: -0.668\n",
      "epoch: 89, actor: 1, score: 4.0, epsilon: 0.050, frames ran: 602976, critic loss: 0.133, actor_loss: -0.670\n",
      "epoch: 90, actor: 0, score: 11.0, epsilon: 0.050, frames ran: 604916, critic loss: 0.124, actor_loss: -0.670\n",
      "  its a playoff!\n",
      "  Actor 0 score: 16.0, Actor 1 score: 14.0\n",
      "Actor 0 won.  Overwriting Actor 1.\n",
      "epoch: 91, actor: 1, score: 18.0, epsilon: 0.050, frames ran: 612208, critic loss: 0.115, actor_loss: -0.672\n",
      "epoch: 92, actor: 0, score: 20.0, epsilon: 0.050, frames ran: 615436, critic loss: 0.114, actor_loss: -0.673\n",
      "epoch: 93, actor: 1, score: 15.0, epsilon: 0.050, frames ran: 618068, critic loss: 0.125, actor_loss: -0.671\n",
      "epoch: 94, actor: 0, score: 12.0, epsilon: 0.050, frames ran: 619980, critic loss: 0.104, actor_loss: -0.676\n",
      "epoch: 95, actor: 1, score: 14.0, epsilon: 0.050, frames ran: 621968, critic loss: 0.115, actor_loss: -0.674\n",
      "epoch: 96, actor: 0, score: 19.0, epsilon: 0.050, frames ran: 625824, critic loss: 0.111, actor_loss: -0.672\n",
      "epoch: 97, actor: 1, score: 16.0, epsilon: 0.050, frames ran: 627984, critic loss: 0.108, actor_loss: -0.675\n",
      "epoch: 98, actor: 0, score: 4.0, epsilon: 0.050, frames ran: 629156, critic loss: 0.109, actor_loss: -0.674\n",
      "epoch: 99, actor: 1, score: 17.0, epsilon: 0.050, frames ran: 632112, critic loss: 0.102, actor_loss: -0.680\n",
      "epoch: 100, actor: 0, score: 4.0, epsilon: 0.050, frames ran: 633168, critic loss: 0.106, actor_loss: -0.679\n",
      "  its a playoff!\n",
      "  Actor 0 score: 14.0, Actor 1 score: 24.0\n",
      "  Actor 1 won.  Overwriting Actor 0.\n"
     ]
    }
   ],
   "source": [
    "learning_rate_actor = .00005 #.000005\n",
    "learning_rate_critic = .001 #.000005\n",
    "actor_optimizer_0 =  optim.Adam(actor_model_local_0.parameters(), lr=learning_rate_actor)\n",
    "critic_optimizer_0 = optim.Adam(critic_model_local_0.parameters(), lr=learning_rate_critic)\n",
    "actor_optimizer_1 =  optim.Adam(actor_model_local_1.parameters(), lr=learning_rate_actor)\n",
    "critic_optimizer_1 = optim.Adam(critic_model_local_1.parameters(), lr=learning_rate_critic)\n",
    "\n",
    "epochs = 101 #100\n",
    "gamma = .99\n",
    "lambda_frames = 5\n",
    "TAU = .001 #1e-3  \n",
    "\n",
    "epsilon_max = 0.30\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay_frames = 50000\n",
    "trainings_per_epoch = 50\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_critic_loss = 0\n",
    "    total_actor_loss = 0\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        actor_model_local = actor_model_local_0\n",
    "        actor_model_target = actor_model_target_0\n",
    "        critic_model_local = critic_model_local_0\n",
    "        critic_model_target = critic_model_target_0\n",
    "        actor_optimizer =  actor_optimizer_0\n",
    "        critic_optimizer = critic_optimizer_0\n",
    "        actor_name = 0\n",
    "    else:\n",
    "        actor_model_local = actor_model_local_1\n",
    "        actor_model_target = actor_model_target_1\n",
    "        critic_model_local = critic_model_local_1\n",
    "        critic_model_target = critic_model_target_1\n",
    "        actor_optimizer =  actor_optimizer_1\n",
    "        critic_optimizer = critic_optimizer_1\n",
    "        actor_name = 1\n",
    "    \n",
    "    #play a game. game info is saved to the AtariEnv object\n",
    "    atari_env_train.reset()     \n",
    "    epsilon = epsilon_max - (epsilon_max-epsilon_min)*(atari_env_train.global_step_counter/epsilon_decay_frames)\n",
    "    epsilon = max(epsilon_min, epsilon)\n",
    "    actor_model_local.eval()\n",
    "    play_game(atari_env_train, actor_model_local, epsilon)  \n",
    "    actor_model_local.train()\n",
    "    \n",
    "    game_score = atari_env_train.current_score\n",
    "        \n",
    "    for training_iter in range(trainings_per_epoch):\n",
    "        \n",
    "        # TODO - instead of palying a whole game, move forward one step and train a batch.\n",
    "        \n",
    "        if atari_env_train.global_step_counter < 10000:\n",
    "            continue\n",
    "        \n",
    "        frame_batch, next_frame_batch, actions_batch, next_actions_batch, reward_batch, frame_number_batch, lambda_reward_batch = get_train_batch(atari_env_train, batch_size, lambda_frames, gamma)\n",
    "\n",
    "        # print(frame_batch.shape)\n",
    "        # print(next_frame_batch.shape)\n",
    "        # print(actions_batch)\n",
    "        # print(reward_batch)\n",
    "        # print(frame_number_batch)\n",
    "\n",
    "        # convert to tensors for input into the models.\n",
    "        reward_batch_reshaped = np.reshape(reward_batch, (batch_size, 1))   #unsqueeze?\n",
    "        reward_batch_tensor = torch.from_numpy(reward_batch_reshaped).float().cuda()\n",
    "        \n",
    "        img_tensor = torch.from_numpy(frame_batch).float().cuda()\n",
    "        img_tensor_next = torch.from_numpy(next_frame_batch).float().cuda()\n",
    "        \n",
    "        actions_batch_reshaped = np.reshape(actions_batch, (batch_size, 1))\n",
    "        actions_batch_reshaped = action_batch_to_one_hot(action_count, actions_batch_reshaped)\n",
    "        actions_batch_tensor = torch.from_numpy(actions_batch_reshaped).float().cuda()\n",
    "        #print(\"actions_batch_tensor: {}\".format(actions_batch_tensor))\n",
    "        \n",
    "        #next_actions_batch_reshaped = np.reshape(next_actions_batch, (batch_size, 1))\n",
    "        #next_actions_batch_reshaped = action_batch_to_one_hot(action_count, next_actions_batch_reshaped)\n",
    "        #next_actions_batch_tensor = torch.from_numpy(next_actions_batch_reshaped).float().cuda()\n",
    "        #print(\"next_actions_batch_tensor: {}\".format(next_actions_batch_tensor))\n",
    "        \n",
    "        lambda_reward_batch_reshaped = np.reshape(lambda_reward_batch, (batch_size, 1))\n",
    "        lambda_reward_batch_tensor = torch.from_numpy(lambda_reward_batch_reshaped).float().cuda()\n",
    "        \n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        ##### instead of using historical next actions, use the target actor model for next actions\n",
    "        target_actions_next = actor_model_target(img_tensor_next)\n",
    "        #Q_targets_next = critic_model_target(img_tensor_next, next_actions_batch_tensor) #note \"next\" values are end of lambda frames\n",
    "        Q_targets_next = critic_model_target(img_tensor_next, target_actions_next) #note \"next\" values are end of lambda frames\n",
    "        \n",
    "        #Q_targets = reward_batch_tensor + (gamma * Q_targets_next)\n",
    "        Q_targets = lambda_reward_batch_tensor + (gamma**lambda_frames * Q_targets_next)\n",
    "            \n",
    "        # Compute critic loss\n",
    "        Q_expected = critic_model_local(img_tensor, actions_batch_tensor)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "        # Minimize the critic loss\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critic_model_local.parameters(), 1)\n",
    "        critic_optimizer.step()\n",
    "        total_critic_loss += critic_loss.item()\n",
    "            \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = actor_model_local(img_tensor)\n",
    "        actor_loss = -critic_model_target(img_tensor, actions_pred).mean()  #### should this be critic target?????\n",
    "            \n",
    "        # Minimize the actor loss\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        total_actor_loss += actor_loss.item()\n",
    "            \n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        #use very small Tau and update with every step\n",
    "        soft_update(critic_model_local, critic_model_target, TAU) \n",
    "        soft_update(actor_model_local, actor_model_target, TAU)\n",
    "    \n",
    "    print(\"epoch: {}, actor: {}, score: {}, epsilon: {:.3f}, frames ran: {}, critic loss: {:.3f}, actor_loss: {:.3f}\".format(epoch, actor_name, atari_env_train.current_score, epsilon, atari_env_train.global_step_counter, total_critic_loss/trainings_per_epoch, total_actor_loss/trainings_per_epoch))\n",
    "\n",
    "    #now , play a game with either network.  See which one wins\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"  its a playoff!\")\n",
    "        atari_env_train.reset()   \n",
    "        actor_model_local_0.eval()\n",
    "        play_game(atari_env_train, actor_model_local_0, 0.0)  \n",
    "        actor_model_local_0.train()\n",
    "        actor_0_score = atari_env_train.current_score\n",
    "    \n",
    "        atari_env_train.reset()   \n",
    "        actor_model_local_1.eval()\n",
    "        play_game(atari_env_train, actor_model_local_1, 0.0)\n",
    "        actor_model_local_1.train()\n",
    "        actor_1_score = atari_env_train.current_score\n",
    "        \n",
    "        print(\"  Actor 0 score: {}, Actor 1 score: {}\".format(actor_0_score, actor_1_score))\n",
    "        if (actor_0_score > actor_1_score):\n",
    "            print(\"Actor 0 won.  Overwriting Actor 1.\")\n",
    "            soft_update(critic_model_local_0,  critic_model_local_1,  1.0)\n",
    "            soft_update(actor_model_local_0,   actor_model_local_1,   1.0)\n",
    "            soft_update(critic_model_target_0, critic_model_target_1, 1.0)\n",
    "            soft_update(actor_model_target_0,  actor_model_target_1,  1.0)\n",
    "        else:\n",
    "            print(\"  Actor 1 won.  Overwriting Actor 0.\")\n",
    "            soft_update(critic_model_local_1,  critic_model_local_0,  1.0)\n",
    "            soft_update(actor_model_local_1,   actor_model_local_0,   1.0)\n",
    "            soft_update(critic_model_target_1, critic_model_target_0, 1.0)\n",
    "            soft_update(actor_model_target_1,  actor_model_target_0,  1.0)\n",
    "    \n",
    "atari_env_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_model_local_0.state_dict(), \"{}actor_model_local_0.pt\".format(environment_name))\n",
    "torch.save(actor_model_target_0.state_dict(), \"{}actor_model_target_0.pt\".format(environment_name))\n",
    "torch.save(critic_model_local_0.state_dict(), \"{}critic_model_local_0.pt\".format(environment_name))\n",
    "torch.save(critic_model_target_0.state_dict(), \"{}critic_model_target_0.pt\".format(environment_name))\n",
    "\n",
    "torch.save(actor_model_local_1.state_dict(), \"{}actor_model_local_1.pt\".format(environment_name))\n",
    "torch.save(actor_model_target_1.state_dict(), \"{}actor_model_target_1.pt\".format(environment_name))\n",
    "torch.save(critic_model_local_1.state_dict(), \"{}critic_model_local_1.pt\".format(environment_name))\n",
    "torch.save(critic_model_target_1.state_dict(), \"{}critic_model_target_1.pt\".format(environment_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 22.0\n"
     ]
    }
   ],
   "source": [
    "# actor_model_local.load_state_dict(torch.load(\"demonattack_actor_local.pt\"))\n",
    "# actor_model_target.load_state_dict(torch.load(\"demonattack_actor_target.pt\"))\n",
    "\n",
    "# critic_model_local.load_state_dict(torch.load(\"demonattack_critic_local.pt\"))\n",
    "# critic_model_target.load_state_dict(torch.load(\"demonattack_critic_target.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "#play a game using the model\n",
    "#play_environment_name = 'DemonAttackNoFrameskip-v4'\n",
    "#play_environment_name = 'SpaceInvadersNoFrameskip-v4'\n",
    "play_environment_name = environment_name\n",
    "atari_env_play = AtariEnv(play_environment_name)\n",
    "play_model = actor_model_local_1\n",
    "\n",
    "for i in range(1):\n",
    "    atari_env_play.reset()\n",
    "\n",
    "    play_model.eval()\n",
    "    play_game(atari_env_play, play_model, 0.0, max_frames=4000, debug=False)\n",
    "    print(\"score: {}\".format(atari_env_play.current_score))\n",
    "    play_model.train()\n",
    "\n",
    "atari_env_play.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 4.0\n",
      "reward: 0.0\n",
      "frame:  7020.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAGMCAYAAACcdLRRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAP80lEQVR4nO3dX6g091kH8OdJ3xo1r4lo/NOENKUtelFD7IUKinohKI34YkFQQdsY1KtemMpLFUVaacXmzR8suaiExipVDIoXq74XrRcFGyqFYpOYgGha27dNTKvW2qRak/bnxZkT9yy7e3b37M4+M/v5wMI5e/bMMzu7+51nfjM7k621AKjiqn3PAMA0oQSUIpSAUoQSUIpQAkoRSkApQgkoRSixscx8fWZeycxnM/O1+56faZn51sx8vpu3a1Z4/NXdY5/PzLf3MY/MJ5SYKzM/mJm/eMrD7o6IN7XWzrfW/r6P+VrTQ928PRcRkUfemZn/3t3uysyMiGitfbm1dj4i/nivc0yc2/cMcDbdhypba19ddt+O3BwRjy+Yr3OttRd2XH9dvxwRPxkRt0ZEi4gPRMTHI+Ld+5wpTtIp7VFm3pSZf5GZn+vW3Pd39781M9839bhXZGbLzHPd7x/MzHdk5sMR8aWIeOWC+67LzPdk5tOZ+ZnMfHtmvqSbxu2Z+aHMvDszP5+Zn8jM13V/e0dE/GBE3N9t0tw/M99XZ+azEfGSiHgkM5/s7v+XzHxLZj4aEc9l5rnM/LXMfDIzv5iZT2Tm66emc3tmPpyZ92Xmf2bmxzPz+7v7r2TmZzPzjTN1787MT2XmM5n57sz8ujUW+Rsj4p7W2qdba5+JiHsi4vY1/p8eCKU96cLhryLikxHxioi4MSL+dI1J/Hwcrfm/oZvGvPv+MCJeiIhXR8RrI+JHI2J6k+z7IuIfI+L6iLgrIt6Tmdla+42I+Nv4/02zN00XntrUiYi4tbX2qqk//2xE/HhEfGPXKT0ZRwF3XUS8LSLel5kvm5mHRyPimyPiT7pl8D3dPP9cHAXjca13RsR3RMR3d3+/MSJ+a9UFFhGviYhHpn5/pLuPQoTS/nxvRNwQERdba8+11v6ntfahNf7/va21x1trL7TWnp+9LyK+KSJeFxG/0k3/sxFxX0T8zNQ0Ptlae6C19pU4CrCXRcS3nfF5vau1dqW19t8REa21P2utPdVa+2pr7aGI+Kc4eu7HPtFa+4NuHh6KiJsi4re74Ht/RPxvRLy62yT9pYi4s7X2H621L0bE78w8n9Ocj4gvTP3+hYg4fzyuRA3GlPbnpjgKhU3HXa6cct/NEfHSiHh66jN31cxj/vX4h9bal7rHnY+zOTFfmfmGiHhzHHWDx9O/fuohz0z9fBxks/edj4hviYivj4iPTj2fjKNNyFU9GxHXTv1+bUQ825wqoxShtD9XIuLlCwaEn4ujD+Cxb5/z//M+SNP3XYmIL0fE9RsG36Yf1Bf/LzNvjogHIuJHIuLDrbWvZObH4ihM1vVvcRRQr+nGgzbxeBwNcn+k+/3WWDBQz/7YfNufj0TE0xHxu5l5TWZ+bWb+QPe3j0XED2XmyzPzuoj49XUn3lp7OiLeHxH3ZOa1mXlVZr4qM394xUk8ExGvXLfujGviKKQ+FxGRmb8QEd+1yYS6PYkPRMR9mfmt3fRuzMwfW2MyfxQRb+7+74aI+NWIeO8m88PuCKU96cZQfiKOBmw/FRGfjoif7v72gTgaX3k0Ij4aRwPim3hDRHxNRDwREZ+PiD+Po3GjVfxeRPxUt2fuXZsUb609EUd7uD4cRyF3S0Q8vMm0Om+JiH+OiL/LzP+KiL+JiO9c4/9/PyL+MiIei4h/iIi/7u6jkLQ5zRhl5m/GUYf5fETceHwA5ZLHXx1HwfnSiLirtfa23c8l8wgloBSbb0ApQgkoRSgBpSw9TikzDTgBW9daW3ismk4JKEUoAaUIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoRSgBpQgloBShBJQilIBShBJQilACStn4um+XLl3a5nxERMTFixe3Ps2hGPPyfOyxx7Y+zVtuuWXr0xyKsS9PnRJQytKrmVQ682TfncSYO5ex67uTGHvnsgvOPAkMxsad0rJOYtMO5JA7iTEvz2WdxKYdyNg7iWXGsDx1SsBgjGJMqVInMYTOZez67iTG0Ln0bVmnZPOtiDEvTx/a7RrD8rT5BgyGzbcN/m9TVTqXsRvKZlilzqVvOiVgMIwpFTHm5amT2K4xLE+dEjAYxpS2bAidy9gNZWyoUufSt94PCdjUIX9ox7w8fTdsu8awPG2+AYMxis23TTkkYJycJaA+nRIwGIPplIDx0CkBgyGUgFKEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoRSgBpQgloBShBJQilIBShFJETCaTXmuppx6LCSWgltbawltEtEO5TSaTNplMzvwY9dar1/frN9R6Y7styx2dElCLTunkbd4acPa+Xa9xx1Zv2Txsszs7hHpjuS3LnXPBCRcuXIiIk4Pfx/ept1m92YFh9VjG5htQiivkdlbZzbvNNaB66h0yV8gFBkMozbhw4cKJNdz0z7s4cG663mztsdSbV/u43raNvd4hEEpAKfa+zZhdu+16bTc9/T7WrOoNu94h0CkBpQgloBSbbzOW7cbd9UCpeuqhUwKK0SnF6Wu0ba7xVpmWeuodMp0SUIqvmQC98zUTYDCEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoxfmUZiw7F84uLi6onnqc5NQlnXVOzLWNN5t626t1CPXGxqlLgMEQSjNmr3K66D71Vq+3yjyoxzGhBJRiTCk2O9H7pmvCTU8qr95h1hsrY0rAYDgkoHO8JjteE06v2Wbv28YldA6p3myXMF1vW5cjGnu9QyKUZsz7YO6y9T6EerMfTPVYxuYbUIpOKU6u7Va5NvxZ1oTLNqMOsd5Zu4qx1ztEOiWgFKEElCKUgFKEElCKge4F+h6gHHO9MT+3fdQbO50SUIrvvgG98903YDCEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoRSgBpQgloBRnnlygr4s1HkK9vi/WOPZ6Y6dTAkoRSp3JZHLqdd9XeYx669XblrHXOyROhxsnr2S67I00/ZhNW/TZK6seer2zXpV37PXGyulwgcEw0D1j3hpwl2u5Q6jX50Dw2OsdAp0SUIpQAkoRSkApxpRmzNtbsss9KIdQb3a66rGMUIr13zxnebNt8r9jrnfWD+7Y6x0im29ALa21hbeIaIdym0wmbTKZnPkx6q1Xr+/Xb6j1xnZbljs6JaAWndLJ27w14DY7iEOst8o8qHdYN50SMBi+kAv0zhdygcEQSkApQgkoRSgBpQgloBShBJQilIBShBJQilACShFKQClCCShFKAGlCCWgFOfonrHKZa3VU29f9Q6BU5d0lr25Zm3jzabe9modQr2xceoSYDB0SnFyrXe8Zpu+dte8teKma8BF152fvn7Y2OrNXgdNPXRKwGDolDpVx1zGXm+oYzzGlM5GpwQMhk5pxth3Kas37HpjsaxTcpzSAtNvqHU2fdRbXks9TmPzDajFFXJXv6rpNq98qt78q8uqdxg3V8gFBsOY0gLzDqhU7+y11OM0OiWgFmNKJ2/zxgq2OdZyiPVWmQf1Duu2LHccpwT0zhHdwGAIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoRSgBpQgloBShBJQilIBShBJQilACShFKQClOhztjrNcNm73e/ViN9fU7JDoloBRnnuyscwHBbawB+643W3dsa/FVl+e2nnff9cZm2ZknhVIsv7LHojffpm+2ZVe+2MWmwKIQmkwmvX1gjt9jmQvfhxvb1/Lsq95YOR0uMBgGukduUec3bw1vbU4FOiWgFJ3SjHUGoIdQb52rt/Z9ld5dGNvrd4h0SkAp9r7NmLfmO+4adjH20le9fY4l7XLv26zTlue2n2vf9cZi2d43m28L9P1m2lW9VQ4JmH3M0D9MY3ntDpXNN6AUndLIrXJIwGn/A33SKQGl6JQOzLwuaJ9fQYFZOiWgFJ1SLD8AbtsHx/VZa1ND6pL6Xp5DeP2GznFKQO+cJQAYDKEElCKUgFKEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpSAUoQSUIpQAkpx6pIF+r4G2hiuuVbFOte6G2K9sdMpAaU4n1JnnRN0bfM6bH3VG7tVl+e2lmXf9cbG+ZSAwdApxfzxnFXvO0utPuqN3bzluc4yrl5vrHRKwGDY+zZj3ljBLk8I33e9sfP6jUBrbeEtItqh3CaTSZtMJmd+TNV6Y7+tujyHWm9st2W5Y/MNKEUoAaUIJaAUA92d2d24u96F23e9sZtenn0sy77rHRKdElCLvW/zb8v2ruxir8qyadqLs93XbtvLs+96Y7jZ+wYMhlACSjHQvcA+Bk4Nem+HnRbDplMCSnGWAKB3zhIADIZQAkoRSkApQgkoRSgBpQgloBShBJQilIBShBJQilACShFKQClCCShFKAGlCCWgFKEElCKUgFKEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpTYidZaLLumICwilIBShBJQilACShFKQClCCShFKAGlCCWgFKEElCKUgFKEElCKUAJKEUpAKUIJKEUoAaUIJaCUc/ueAcYpM/c9CwyUTgkoRSgBpQgloBShBJRioHsHLl++PPf+2267rec52b7Lly+/+Dymn+dYntsiY3h+Q6FTAkoRSkApQgkoJZddMDAzXU3wDOaNUQx1bGLZ+NEYxpaWjSdNG+rzq6a1tvDoWp0SUIpQAkpxSMAOrLopMFRjf37sl04JKEUoAaUIJaAUobQlly9fNtYCWyCUgFKEElCKUNoym3BwNkIJKEUoAaUIJaAUXzM5o3XHkI4fP5RvmxsjO2lor98Q6ZSAUoQSUIrNNzZmE4Zd0CkBpQgloBShBJRiTImNTe8enz10YCjjTQ55qEenBJQilIBSbL5t2bLNljFsKgxls2wTY3/thkKnBJQilIBShBJQSrbWFv8xc/EfATbUWstFf9MpAaUIJaAUoQSUIpSAUoQSUIpQAkoRSkApQgkoRSgBpQgloBShBJQilIBShBJQilACShFKQClCCShFKLE3ly5d2vcsUJBQAkpxOlx6N69Dunjx4h7mhH1xOlxgMIQSUIpQAkoxpkRvVtnbZmzpMCwbUxJK9M5ANwa6gcEQSkApQgkoxZgSO7fJ10mMMY2bMSVgMHRK9MYhARxzSAB7ZfONWTbfgMEQSkApQgkoZedjSnfeeedZJwGMzL333mtMCRiGc7uY6NC6oxtuuOHFn5966qk9zgljd8cdd7z484MPPrjHOalraSgNLVyA4bP5BpQilIBShBJQilACShFKQClCCShlJ8cpDY1jk+iLY5NOp1MCShFKQClCCShFKAGlCCWgFKEElCKUgFKEElCKUAJKEUpAKUIJKEUoAaUIJaAUoQSUIpSAUpZeIRegbzoloBShBJQilIBShBJQilACShFKQCn/BwIh3jCwuQRRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get some information from  the batch to make sure it looks good.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#frame_batch, next_frame_batch, actions_batch, reward_batch, frame_number_batch = get_train_batch(atari_env_train, batch_size)\n",
    "frame_batch, next_frame_batch, actions_batch, next_actions_batch, reward_batch, frame_number_batch, lambda_reward_batch = get_train_batch(atari_env_train, batch_size, lambda_frames, gamma)\n",
    "\n",
    "print(\"action: {}\".format(actions_batch[0]))\n",
    "print(\"reward: {}\".format(reward_batch[0]))\n",
    "print(\"frame:  {}\".format(frame_number_batch[0]))\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"current frame [0]\")\n",
    "plt.imshow(frame_batch[0][0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(11, 7))\n",
    "# plt.subplot(121)\n",
    "# plt.title(\"current frame [2]\")\n",
    "# plt.imshow(frame_batch[0][2], cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"next frame [0]\")\n",
    "plt.imshow(next_frame_batch[0][0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(11, 7))\n",
    "# plt.subplot(121)\n",
    "# plt.title(\"next frame [2]\")\n",
    "# plt.imshow(next_frame_batch[0][2], cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "-5.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "-5.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    reward = atari_env_train.frame_buffer[-i].getReward()\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
