{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym_utils_q_learning import AtariEnv\n",
    "from gym_utils_q_learning import AtariFrame\n",
    "from AtariACModels import AtariActorModel, AtariCriticModel\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#environment_name = \"SpaceInvaders-v4\"\n",
    "environment_name  = \"SpaceInvadersNoFrameskip-v4\"\n",
    "\n",
    "# environment_name = \"Pong-v4\"\n",
    "# typical_bad_game_frame_count = 1100\n",
    "# reward_frame_shift = -1\n",
    "\n",
    "action_count = gym.make(environment_name).action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a quick game to test out gym_utils\n",
    "\n",
    "if False:\n",
    "    atari_env = AtariEnv(environment_name)\n",
    "    model = AtariActorModel(action_count).cuda()\n",
    "    action = atari_env.env.action_space.sample()\n",
    "\n",
    "    for i in range(4000):\n",
    "        atari_frame = atari_env.step(action)\n",
    "        if atari_frame is None:\n",
    "            break\n",
    "        processed_frames = atari_frame.get_next_processed_frames()\n",
    "        #print(\"processed_frames.shape: {}\".format(processed_frames.shape))\n",
    "        processed_frames = np.reshape(processed_frames, (1,)+processed_frames.shape)\n",
    "        #print(\"processed_frames.shape: {}\".format(processed_frames.shape))\n",
    "        img_tensor = torch.from_numpy(processed_frames).float().cuda()\n",
    "\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        print(\"model_actions: {}\".format(action_array))\n",
    "        action = np.argmax(action_array)\n",
    "\n",
    "    atari_env.close()\n",
    "    \n",
    "    print(\"len(atari_env.frame_buffer): {}\".format(len(atari_env.frame_buffer)))\n",
    "    atariFrame = atari_env.frame_buffer[107]\n",
    "    processed_frames = atariFrame.get_processed_frames()\n",
    "    print(\"processed_frames.shape: {}\".format(processed_frames.shape))\n",
    "    print(\"sample_frame.frame_index: {}\".format(atariFrame.frame_index))\n",
    "    print(\"sample_frame.reward_list: {}\".format(atariFrame.reward_list))\n",
    "    atariFrame.show_processed_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game. feed each frame into the model and see what we get\n",
    "def play_game(atari_env, model, epsilon, max_frames=5000):\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    frame_counter = 0\n",
    "    action_count = atari_env.env.action_space.n\n",
    "    choices = np.arange(action_count)\n",
    "    \n",
    "    while not done:\n",
    "        save_step = frame_counter >= 60\n",
    "        atari_frame = atari_env.step(current_action, save_step=save_step)\n",
    "\n",
    "        if atari_frame is None:  #processed_frames == None\n",
    "            done = True\n",
    "            continue\n",
    "            \n",
    "        processed_frames = atari_frame.get_processed_frames()\n",
    "\n",
    "        processed_frames_batch = np.reshape(processed_frames, (1,)+processed_frames.shape)\n",
    "        img_tensor = torch.from_numpy(processed_frames_batch).float().cuda()\n",
    "        output = model(img_tensor)\n",
    "        action_array = output.detach().cpu().numpy()[0]\n",
    "        # print(\"play_game action_array: {}\".format(action_array))\n",
    "    \n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < epsilon:\n",
    "            current_action = atari_env.env.action_space.sample()\n",
    "        else:\n",
    "            #current_action = np.argmax(action_array)\n",
    "            current_action = np.random.choice(choices, p=action_array)\n",
    "\n",
    "        frame_counter += 1\n",
    "        if frame_counter > max_frames:\n",
    "            break\n",
    "\n",
    "def game_step(atari_env, model, epsilon, max_frames=5000):\n",
    "    current_action = 0\n",
    "    done = False\n",
    "    \n",
    "    frame = atari_env.frame_buffer[-1]\n",
    "    processed_frames = frame.get_next_processed_frames()\n",
    "\n",
    "    processed_frames_batch = np.reshape(processed_frames, (1,)+processed_frames.shape)\n",
    "    img_tensor = torch.from_numpy(processed_frames_batch).float().cuda()\n",
    "    output = model(img_tensor)\n",
    "    action_array = output.detach().cpu().numpy()[0]\n",
    "\n",
    "    rand = random.uniform(0, 1)\n",
    "    if rand < epsilon:\n",
    "        current_action = atari_env.env.action_space.sample()\n",
    "    else:\n",
    "        current_action = np.argmax(action_array)\n",
    "\n",
    "    atari_frame = atari_env.step(current_action, save_step=save_step)\n",
    "\n",
    "    if atari_frame is None:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "            \n",
    "def get_train_batch(atari_env, batch_size):\n",
    "    rand_arr = np.arange(len(atari_env.frame_buffer)-1)\n",
    "    np.random.shuffle(rand_arr)\n",
    "   \n",
    "    index_counter = 0\n",
    "    batch_index_counter = 0\n",
    "    \n",
    "    frame_batch        = np.zeros((batch_size, 4, 210, 160))\n",
    "    next_frame_batch   = np.zeros((batch_size, 4, 210, 160))\n",
    "    reward_batch       = np.zeros(batch_size)\n",
    "    actions_batch      = np.zeros(batch_size)\n",
    "    next_actions_batch = np.zeros(batch_size)\n",
    "    frame_number_batch = np.zeros(batch_size)\n",
    "    \n",
    "    for batch_index in range(batch_size):\n",
    "        frame_number = rand_arr[batch_index]\n",
    "        atari_frame = atari_env.frame_buffer[frame_number]\n",
    "        next_atari_frame = atari_env.frame_buffer[frame_number+1]\n",
    "        \n",
    "        frame_batch[batch_index]      = atari_frame.get_processed_frames()\n",
    "        next_frame_batch[batch_index] = atari_frame.get_next_processed_frames()\n",
    "        reward_batch[batch_index]     = atari_frame.getReward()\n",
    "        actions_batch[batch_index]    = atari_frame.action_taken\n",
    "        next_actions_batch[batch_index] = next_atari_frame.action_taken\n",
    "        frame_number_batch[batch_index] = frame_number\n",
    "\n",
    "    return frame_batch, next_frame_batch, actions_batch, next_actions_batch, reward_batch, frame_number_batch\n",
    "    \n",
    "def action_batch_to_one_hot(action_count, action_batch):\n",
    "#     print(action_batch.shape)\n",
    "#     print(action_batch)\n",
    "    return_value = np.zeros((action_batch.shape[0], action_count))\n",
    "    for i in range(action_batch.shape[0]):\n",
    "        this_row = np.zeros(action_count)\n",
    "#         print(\"action_batch[i][0]: {}\".format(action_batch[i][0]))\n",
    "        this_row[int(action_batch[i][0])] = 1.0\n",
    "        return_value[i] = this_row\n",
    "    return return_value\n",
    "    \n",
    "    \n",
    "\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model_local  = AtariActorModel(action_count).cuda()\n",
    "actor_model_target = AtariActorModel(action_count).cuda()\n",
    "\n",
    "critic_model_local  = AtariCriticModel(action_count).cuda()\n",
    "critic_model_target = AtariCriticModel(action_count).cuda()\n",
    "\n",
    "# atari_model_local.load_state_dict(torch.load(\"space_invaders.pt\"));\n",
    "# atari_model_target_1.load_state_dict(torch.load(\"space_invaders.pt\"));\n",
    "# atari_model_target_2.load_state_dict(torch.load(\"space_invaders.pt\"));\n",
    "\n",
    "atari_env_train = AtariEnv(environment_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, score: 24.0, epsilon: 0.150, frames ran: 464056, critic loss: 0.093633, actor_loss: -0.640677\n",
      "epoch: 1, score: 4.0, epsilon: 0.150, frames ran: 465400, critic loss: 0.115362, actor_loss: -0.637655\n",
      "epoch: 2, score: 8.0, epsilon: 0.150, frames ran: 466972, critic loss: 0.096921, actor_loss: -0.639078\n",
      "epoch: 3, score: 11.0, epsilon: 0.150, frames ran: 468960, critic loss: 0.117639, actor_loss: -0.646034\n",
      "epoch: 4, score: 15.0, epsilon: 0.150, frames ran: 472212, critic loss: 0.099931, actor_loss: -0.645035\n",
      "epoch: 5, score: 10.0, epsilon: 0.150, frames ran: 473976, critic loss: 0.130410, actor_loss: -0.641016\n",
      "epoch: 6, score: 5.0, epsilon: 0.150, frames ran: 475196, critic loss: 0.111096, actor_loss: -0.637024\n",
      "epoch: 7, score: 18.0, epsilon: 0.150, frames ran: 478556, critic loss: 0.095147, actor_loss: -0.639879\n",
      "epoch: 8, score: 10.0, epsilon: 0.150, frames ran: 480492, critic loss: 0.097078, actor_loss: -0.640775\n",
      "epoch: 9, score: 6.0, epsilon: 0.150, frames ran: 482224, critic loss: 0.092594, actor_loss: -0.641655\n",
      "epoch: 10, score: 6.0, epsilon: 0.150, frames ran: 483456, critic loss: 0.106191, actor_loss: -0.645000\n",
      "epoch: 11, score: 7.0, epsilon: 0.150, frames ran: 485420, critic loss: 0.091494, actor_loss: -0.651614\n",
      "epoch: 12, score: 7.0, epsilon: 0.150, frames ran: 486548, critic loss: 0.096185, actor_loss: -0.651474\n",
      "epoch: 13, score: 15.0, epsilon: 0.150, frames ran: 488680, critic loss: 0.128570, actor_loss: -0.650950\n",
      "epoch: 14, score: 12.0, epsilon: 0.150, frames ran: 490808, critic loss: 0.099065, actor_loss: -0.653765\n",
      "epoch: 15, score: 21.0, epsilon: 0.150, frames ran: 494172, critic loss: 0.111225, actor_loss: -0.646819\n",
      "epoch: 16, score: 7.0, epsilon: 0.150, frames ran: 495712, critic loss: 0.095371, actor_loss: -0.651661\n",
      "epoch: 17, score: 17.0, epsilon: 0.150, frames ran: 498888, critic loss: 0.101494, actor_loss: -0.649500\n",
      "epoch: 18, score: 19.0, epsilon: 0.150, frames ran: 502920, critic loss: 0.096233, actor_loss: -0.647157\n",
      "epoch: 19, score: 26.0, epsilon: 0.150, frames ran: 506140, critic loss: 0.091971, actor_loss: -0.651234\n",
      "epoch: 20, score: 7.0, epsilon: 0.150, frames ran: 507268, critic loss: 0.109331, actor_loss: -0.650212\n",
      "epoch: 21, score: 4.0, epsilon: 0.150, frames ran: 508400, critic loss: 0.110726, actor_loss: -0.657567\n",
      "epoch: 22, score: 6.0, epsilon: 0.150, frames ran: 509520, critic loss: 0.106882, actor_loss: -0.653170\n",
      "epoch: 23, score: 5.0, epsilon: 0.150, frames ran: 510892, critic loss: 0.099100, actor_loss: -0.662438\n",
      "epoch: 24, score: 16.0, epsilon: 0.150, frames ran: 513276, critic loss: 0.094298, actor_loss: -0.657584\n",
      "epoch: 25, score: 7.0, epsilon: 0.150, frames ran: 514848, critic loss: 0.093347, actor_loss: -0.662765\n",
      "epoch: 26, score: 21.0, epsilon: 0.150, frames ran: 518168, critic loss: 0.089483, actor_loss: -0.667681\n",
      "epoch: 27, score: 7.0, epsilon: 0.150, frames ran: 519288, critic loss: 0.116545, actor_loss: -0.661618\n",
      "epoch: 28, score: 5.0, epsilon: 0.150, frames ran: 520472, critic loss: 0.084783, actor_loss: -0.670252\n",
      "epoch: 29, score: 25.0, epsilon: 0.150, frames ran: 523576, critic loss: 0.100961, actor_loss: -0.665303\n",
      "epoch: 30, score: 20.0, epsilon: 0.150, frames ran: 527584, critic loss: 0.106748, actor_loss: -0.668989\n",
      "epoch: 31, score: 15.0, epsilon: 0.150, frames ran: 530824, critic loss: 0.095063, actor_loss: -0.671074\n",
      "epoch: 32, score: 16.0, epsilon: 0.150, frames ran: 533588, critic loss: 0.102711, actor_loss: -0.671994\n",
      "epoch: 33, score: 24.0, epsilon: 0.150, frames ran: 536820, critic loss: 0.093760, actor_loss: -0.672450\n",
      "epoch: 34, score: 9.0, epsilon: 0.150, frames ran: 539632, critic loss: 0.084850, actor_loss: -0.671875\n",
      "epoch: 35, score: 20.0, epsilon: 0.150, frames ran: 542532, critic loss: 0.085240, actor_loss: -0.672264\n",
      "epoch: 36, score: 17.0, epsilon: 0.150, frames ran: 545612, critic loss: 0.090638, actor_loss: -0.675140\n",
      "epoch: 37, score: 9.0, epsilon: 0.150, frames ran: 547188, critic loss: 0.085784, actor_loss: -0.677337\n",
      "epoch: 38, score: 14.0, epsilon: 0.150, frames ran: 550148, critic loss: 0.087639, actor_loss: -0.687802\n",
      "epoch: 39, score: 20.0, epsilon: 0.150, frames ran: 553504, critic loss: 0.087433, actor_loss: -0.686410\n",
      "epoch: 40, score: 6.0, epsilon: 0.150, frames ran: 555280, critic loss: 0.088260, actor_loss: -0.698011\n",
      "epoch: 41, score: 28.0, epsilon: 0.150, frames ran: 559588, critic loss: 0.081971, actor_loss: -0.696529\n",
      "epoch: 42, score: 16.0, epsilon: 0.150, frames ran: 561516, critic loss: 0.102345, actor_loss: -0.700796\n",
      "epoch: 43, score: 5.0, epsilon: 0.150, frames ran: 562788, critic loss: 0.110373, actor_loss: -0.697957\n",
      "epoch: 44, score: 8.0, epsilon: 0.150, frames ran: 564812, critic loss: 0.096637, actor_loss: -0.701992\n",
      "epoch: 45, score: 12.0, epsilon: 0.150, frames ran: 566872, critic loss: 0.084465, actor_loss: -0.705169\n",
      "epoch: 46, score: 22.0, epsilon: 0.150, frames ran: 570396, critic loss: 0.080546, actor_loss: -0.709600\n",
      "epoch: 47, score: 8.0, epsilon: 0.150, frames ran: 572276, critic loss: 0.073895, actor_loss: -0.723683\n",
      "epoch: 48, score: 28.0, epsilon: 0.150, frames ran: 575460, critic loss: 0.089367, actor_loss: -0.727373\n",
      "epoch: 49, score: 7.0, epsilon: 0.150, frames ran: 576648, critic loss: 0.072588, actor_loss: -0.731412\n",
      "epoch: 50, score: 9.0, epsilon: 0.150, frames ran: 578612, critic loss: 0.087469, actor_loss: -0.733938\n",
      "epoch: 51, score: 8.0, epsilon: 0.150, frames ran: 580408, critic loss: 0.088277, actor_loss: -0.730473\n",
      "epoch: 52, score: 17.0, epsilon: 0.150, frames ran: 582952, critic loss: 0.086335, actor_loss: -0.736841\n",
      "epoch: 53, score: 10.0, epsilon: 0.150, frames ran: 584812, critic loss: 0.087458, actor_loss: -0.748374\n",
      "epoch: 54, score: 6.0, epsilon: 0.150, frames ran: 585892, critic loss: 0.076344, actor_loss: -0.750390\n",
      "epoch: 55, score: 21.0, epsilon: 0.150, frames ran: 589204, critic loss: 0.082413, actor_loss: -0.755249\n",
      "epoch: 56, score: 8.0, epsilon: 0.150, frames ran: 590996, critic loss: 0.090053, actor_loss: -0.756049\n",
      "epoch: 57, score: 9.0, epsilon: 0.150, frames ran: 592988, critic loss: 0.079441, actor_loss: -0.758170\n",
      "epoch: 58, score: 7.0, epsilon: 0.150, frames ran: 594168, critic loss: 0.077406, actor_loss: -0.765285\n",
      "epoch: 59, score: 15.0, epsilon: 0.150, frames ran: 596192, critic loss: 0.086760, actor_loss: -0.769806\n",
      "epoch: 60, score: 7.0, epsilon: 0.150, frames ran: 597400, critic loss: 0.100694, actor_loss: -0.766930\n",
      "epoch: 61, score: 16.0, epsilon: 0.150, frames ran: 599892, critic loss: 0.099553, actor_loss: -0.775743\n",
      "epoch: 62, score: 9.0, epsilon: 0.150, frames ran: 602008, critic loss: 0.071223, actor_loss: -0.778917\n",
      "epoch: 63, score: 23.0, epsilon: 0.150, frames ran: 605288, critic loss: 0.082604, actor_loss: -0.778530\n",
      "epoch: 64, score: 5.0, epsilon: 0.150, frames ran: 606356, critic loss: 0.089775, actor_loss: -0.783717\n",
      "epoch: 65, score: 16.0, epsilon: 0.150, frames ran: 608856, critic loss: 0.068402, actor_loss: -0.794268\n",
      "epoch: 66, score: 13.0, epsilon: 0.150, frames ran: 611804, critic loss: 0.061587, actor_loss: -0.798270\n",
      "epoch: 67, score: 16.0, epsilon: 0.150, frames ran: 614992, critic loss: 0.076521, actor_loss: -0.796472\n",
      "epoch: 68, score: 5.0, epsilon: 0.150, frames ran: 616120, critic loss: 0.080583, actor_loss: -0.801359\n",
      "epoch: 69, score: 7.0, epsilon: 0.150, frames ran: 617792, critic loss: 0.078578, actor_loss: -0.807291\n",
      "epoch: 70, score: 5.0, epsilon: 0.150, frames ran: 618876, critic loss: 0.092839, actor_loss: -0.803750\n",
      "epoch: 71, score: 15.0, epsilon: 0.150, frames ran: 621332, critic loss: 0.059989, actor_loss: -0.815970\n",
      "epoch: 72, score: 6.0, epsilon: 0.150, frames ran: 622448, critic loss: 0.084389, actor_loss: -0.818093\n",
      "epoch: 73, score: 6.0, epsilon: 0.150, frames ran: 623516, critic loss: 0.066371, actor_loss: -0.819037\n",
      "epoch: 74, score: 9.0, epsilon: 0.150, frames ran: 624876, critic loss: 0.073437, actor_loss: -0.826112\n",
      "epoch: 75, score: 24.0, epsilon: 0.150, frames ran: 628488, critic loss: 0.074846, actor_loss: -0.822354\n",
      "epoch: 76, score: 6.0, epsilon: 0.150, frames ran: 629616, critic loss: 0.061468, actor_loss: -0.829961\n",
      "epoch: 77, score: 18.0, epsilon: 0.150, frames ran: 632396, critic loss: 0.094159, actor_loss: -0.835694\n",
      "epoch: 78, score: 12.0, epsilon: 0.150, frames ran: 634336, critic loss: 0.077170, actor_loss: -0.841933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79, score: 7.0, epsilon: 0.150, frames ran: 635432, critic loss: 0.077119, actor_loss: -0.837880\n",
      "epoch: 80, score: 17.0, epsilon: 0.150, frames ran: 637760, critic loss: 0.063064, actor_loss: -0.846788\n",
      "epoch: 81, score: 12.0, epsilon: 0.150, frames ran: 639672, critic loss: 0.080593, actor_loss: -0.844028\n",
      "epoch: 82, score: 8.0, epsilon: 0.150, frames ran: 641256, critic loss: 0.082849, actor_loss: -0.851453\n",
      "epoch: 83, score: 15.0, epsilon: 0.150, frames ran: 643508, critic loss: 0.075975, actor_loss: -0.852576\n",
      "epoch: 84, score: 6.0, epsilon: 0.150, frames ran: 644568, critic loss: 0.084709, actor_loss: -0.861877\n",
      "epoch: 85, score: 7.0, epsilon: 0.150, frames ran: 645636, critic loss: 0.081526, actor_loss: -0.866398\n",
      "epoch: 86, score: 13.0, epsilon: 0.150, frames ran: 648432, critic loss: 0.081310, actor_loss: -0.865486\n",
      "epoch: 87, score: 7.0, epsilon: 0.150, frames ran: 649980, critic loss: 0.067473, actor_loss: -0.871347\n",
      "epoch: 88, score: 4.0, epsilon: 0.150, frames ran: 651088, critic loss: 0.099149, actor_loss: -0.872128\n",
      "epoch: 89, score: 9.0, epsilon: 0.150, frames ran: 652820, critic loss: 0.075006, actor_loss: -0.871983\n",
      "epoch: 90, score: 7.0, epsilon: 0.150, frames ran: 653892, critic loss: 0.080508, actor_loss: -0.882505\n",
      "epoch: 91, score: 4.0, epsilon: 0.150, frames ran: 655108, critic loss: 0.073148, actor_loss: -0.880894\n",
      "epoch: 92, score: 15.0, epsilon: 0.150, frames ran: 657900, critic loss: 0.086142, actor_loss: -0.888960\n",
      "epoch: 93, score: 10.0, epsilon: 0.150, frames ran: 659892, critic loss: 0.082213, actor_loss: -0.888792\n",
      "epoch: 94, score: 19.0, epsilon: 0.150, frames ran: 662708, critic loss: 0.090064, actor_loss: -0.892990\n",
      "epoch: 95, score: 15.0, epsilon: 0.150, frames ran: 664736, critic loss: 0.111298, actor_loss: -0.892204\n",
      "epoch: 96, score: 9.0, epsilon: 0.150, frames ran: 666408, critic loss: 0.082339, actor_loss: -0.900659\n",
      "epoch: 97, score: 8.0, epsilon: 0.150, frames ran: 668080, critic loss: 0.095324, actor_loss: -0.899517\n",
      "epoch: 98, score: 4.0, epsilon: 0.150, frames ran: 669160, critic loss: 0.096860, actor_loss: -0.902045\n",
      "epoch: 99, score: 6.0, epsilon: 0.150, frames ran: 670208, critic loss: 0.085010, actor_loss: -0.904185\n"
     ]
    }
   ],
   "source": [
    "learning_rate_actor = .0005 #.000005\n",
    "learning_rate_critic = .005 #.000005\n",
    "actor_optimizer = optim.Adam(actor_model_local.parameters(), lr=learning_rate_actor)\n",
    "critic_optimizer = optim.Adam(critic_model_local.parameters(), lr=learning_rate_critic)\n",
    "\n",
    "epochs = 100\n",
    "gamma = .99\n",
    "TAU = .005 #1e-3  \n",
    "\n",
    "epsilon_max = 0.95\n",
    "epsilon_min = 0.15\n",
    "epsilon_decay_frames = 50000\n",
    "trainings_per_epoch = 100\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_critic_loss = 0\n",
    "    total_actor_loss = 0\n",
    "    \n",
    "    #play a game. game info is saved to the AtariEnv object\n",
    "    atari_env_train.reset()     \n",
    "    epsilon = epsilon_max - (epsilon_max-epsilon_min)*(atari_env_train.global_step_counter/epsilon_decay_frames)\n",
    "    epsilon = max(epsilon_min, epsilon)\n",
    "    actor_model_local.eval()\n",
    "    play_game(atari_env_train, actor_model_local, epsilon)  \n",
    "    actor_model_local.train()\n",
    "    \n",
    "    for training_iter in range(trainings_per_epoch):\n",
    "        \n",
    "        # TODO - instead of palying a whole game, move forward one step and train a batch.\n",
    "        \n",
    "        if atari_env_train.global_step_counter < 8000:\n",
    "            continue\n",
    "        \n",
    "        frame_batch, next_frame_batch, actions_batch, next_actions_batch, reward_batch, frame_number_batch = get_train_batch(atari_env_train, batch_size)\n",
    "\n",
    "        # print(frame_batch.shape)\n",
    "        # print(next_frame_batch.shape)\n",
    "        # print(actions_batch)\n",
    "        # print(reward_batch)\n",
    "        # print(frame_number_batch)\n",
    "\n",
    "        # convert to tensors for input into the models.\n",
    "        reward_batch_reshaped = np.reshape(reward_batch, (batch_size, 1))   #unsqueeze?\n",
    "        reward_batch_tensor = torch.from_numpy(reward_batch_reshaped).float().cuda()\n",
    "        img_tensor = torch.from_numpy(frame_batch).float().cuda()\n",
    "        img_tensor_next = torch.from_numpy(next_frame_batch).float().cuda()\n",
    "        actions_batch_reshaped = np.reshape(actions_batch, (batch_size, 1))\n",
    "        actions_batch_reshaped = action_batch_to_one_hot(action_count, actions_batch_reshaped)\n",
    "        actions_batch_tensor = torch.from_numpy(actions_batch_reshaped).float().cuda()\n",
    "        #print(\"actions_batch_tensor: {}\".format(actions_batch_tensor))\n",
    "        next_actions_batch_reshaped = np.reshape(next_actions_batch, (batch_size, 1))\n",
    "        next_actions_batch_reshaped = action_batch_to_one_hot(action_count, next_actions_batch_reshaped)\n",
    "        next_actions_batch_tensor = torch.from_numpy(next_actions_batch_reshaped).float().cuda()\n",
    "        #print(\"next_actions_batch_tensor: {}\".format(next_actions_batch_tensor))\n",
    "        \n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets_next = critic_model_target(img_tensor_next, next_actions_batch_tensor)\n",
    "        Q_targets = reward_batch_tensor + (gamma * Q_targets_next)\n",
    "            \n",
    "        # Compute critic loss\n",
    "        Q_expected = critic_model_local(img_tensor, actions_batch_tensor)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "        # Minimize the critic loss\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(critic_model_local.parameters(), 1)\n",
    "        critic_optimizer.step()\n",
    "        total_critic_loss += critic_loss.item()\n",
    "            \n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = actor_model_local(img_tensor)\n",
    "        actor_loss = -critic_model_local(img_tensor, actions_pred).mean()\n",
    "            \n",
    "        # Minimize the actor loss\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        total_actor_loss += actor_loss.item()\n",
    "            \n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        #use very small Tau and update with every step\n",
    "        soft_update(critic_model_local, critic_model_target, TAU)\n",
    "        soft_update(actor_model_local, actor_model_target, TAU)\n",
    "\n",
    "    print(\"epoch: {}, score: {}, epsilon: {:.3f}, frames ran: {}, critic loss: {:.6f}, actor_loss: {:.6f}\".format(epoch, atari_env_train.current_score, epsilon, atari_env_train.global_step_counter, total_critic_loss/trainings_per_epoch, total_actor_loss/trainings_per_epoch))\n",
    "    \n",
    "atari_env_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor_model_local.state_dict(), \"space_invaders_actor_local.pt\")\n",
    "torch.save(actor_model_target.state_dict(), \"space_invaders_actor_target.pt\")\n",
    "torch.save(critic_model_local.state_dict(), \"space_invaders_critic_local.pt\")\n",
    "torch.save(critic_model_target.state_dict(), \"space_invaders_critic_target.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8c0971ef2355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0matari_env_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAtariEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#play_game(atari_env_play, atari_model_local, 0.05, max_frames=2000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matari_env_play\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_model_local\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"score: {}\".format(atari_env_play.current_score))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0matari_env_play\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-de4ad3a8aa64>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(atari_env, model, epsilon, max_frames)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprocessed_frames_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprocessed_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_frames_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0maction_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(\"play_game action_array: {}\".format(action_array))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Magnetic_HDD/github/atari-reinforcement-learning-pytorch/AtariACModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_array)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# fc layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# x = self.dropout(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1691\u001b[0m             \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     return torch.batch_norm(\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512])"
     ]
    }
   ],
   "source": [
    "#play a game using the model\n",
    "atari_env_play = AtariEnv(environment_name)\n",
    "atari_env_play.reset()\n",
    "#play_game(atari_env_play, atari_model_local, 0.05, max_frames=2000)\n",
    "play_game(atari_env_play, actor_model_local, 0.1)\n",
    "#print(\"score: {}\".format(atari_env_play.current_score))\n",
    "atari_env_play.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0.0\n",
      "reward: 0.0\n",
      "frame:  862.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get some information from  the batch to make sure it looks good.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frame_batch, next_frame_batch, actions_batch, reward_batch, frame_number_batch = get_train_batch(atari_env_train, batch_size)\n",
    "\n",
    "print(\"action: {}\".format(actions_batch[0]))\n",
    "print(\"reward: {}\".format(reward_batch[0]))\n",
    "print(\"frame:  {}\".format(frame_number_batch[0]))\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"current frame [0]\")\n",
    "plt.imshow(frame_batch[0][0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(11, 7))\n",
    "# plt.subplot(121)\n",
    "# plt.title(\"current frame [2]\")\n",
    "# plt.imshow(frame_batch[0][2], cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.subplot(121)\n",
    "plt.title(\"next frame [0]\")\n",
    "plt.imshow(next_frame_batch[0][0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(11, 7))\n",
    "# plt.subplot(121)\n",
    "# plt.title(\"next frame [2]\")\n",
    "# plt.imshow(next_frame_batch[0][2], cmap=\"gray\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
